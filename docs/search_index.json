[["index.html", "Python and R Welcome", " Python and R Clay Ford, Jacob Goldstein-Greenwood, Oyinkansola Adenekan, Samantha Lomuscio 2022-03-24 Welcome This book provides parallel examples in Python and R to help users of one platform more easily learn how the other platform works when it comes to data analysis. This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. "],["basics.html", "Chapter 1 Basics 1.1 Math 1.2 Missing values 1.3 Assignment 1.4 Printing a value 1.5 Packages 1.6 Logic 1.7 Generating a sequence of values 1.8 Calculating means and medians 1.9 Writing your own functions", " Chapter 1 Basics This chapter covers the very basics of Python and R. 1.1 Math Mathematical operators are the same except for exponents, integer division, and remainder division (modulo). Python Python uses ** for exponentiation, // for integer division, and % for remainder division. &gt; 3**2 9 &gt; 5 // 2 2 &gt; 5 % 2 1 In Python, the + operator can also be used to combine strings. See this TBD section. R Python uses ^ for exponentiation, %/% for integer division, and %% for remainder division. &gt; 3^2 [1] 9 &gt; 5 %/% 2 [1] 2 &gt; 5 %% 2 [1] 1 1.2 Missing values Python and R represent missing values differently, and the distinction is worth keeping in mind, as missing values will crop up throughout this booksome code examples intake or output data that are entirely or partially missing. In Python, a standard indicator for a missing value in a data set is NaN.* In R, missing values are generally indicated by NA. NaN does appear in R as well, but R reserves NaN to indicate values that are not technically absent but that are not defined and/or that cant be represented with numbers; e.g., Inf/Inf. 1.3 Assignment Python uses = for assignment, while R can use either = or &lt;- for assignment. The latter assignment arrow is preferred in most R style guides to distinguish between assignment and setting the value of a function argument. According to Rs documentation, The operator &lt;- can be used anywhere, whereas the operator = is only allowed at the top level (e.g., in the complete expression typed at the command prompt) or as one of the subexpressions in a braced list of expressions. See ?assignOps. Python &gt; x = 12 R &gt; x &lt;- 12 1.4 Printing a value To see the value of an object created via assignment, you can simply enter the object at the console and hit enter for both Python and R, though it is common in Python to explicitly use the print() function. Python &gt; x 12 R &gt; x [1] 12 1.5 Packages User-created functions can be bundled and distributed as packages. Packages need to be installed only once. Thereafter theyre imported (Python) or loaded (R) in each new session when needed. Packages with large user bases are often updated to add functionality and fix bugs. The updates are not automatically installed. Staying apprised of library/package updates can be challenging. Some suggestions are following developers on Twitter, signing up for newsletters, or periodically checking to see what updates are available. Packages often depend on other packages. These are known as dependencies. Sometimes packages are updated to accommodate changes to other packages they depend on. Python When you download Python, you gain access to The Python Standard Library. This library includes several datatypes and functions for storing data, performing mathematical operations, and beyond. Commonly used datatypes include list and range. As you can see below, you do not need to import data types from the Standard Python Library. &gt; my_list = [] + for idx in range(5): + my_list.append(idx) + print(my_list) [0, 1, 2, 3, 4] Libraries contain modules, groups of functions. To use functions from modules in the Python Standard Library, users must import the appropriate module. Examples include math and itertools, which both include several functions for a range of operations. &gt; import math + one = 1 + two = 2 + print(math.pow(two, one)) 2.0 Users can also download 100s of libraries outside of the Standard Python Library. Python libraries are also called packages. Popular libraries include numpy, used for operations on arrays/vectors and pandas, used for data analysis. The following code is an example of importing a Python library, NumPy, into a Python script. &gt; import numpy as np + + my_array = np.array([1,2,3]) + print(my_array) [1 2 3] To use Python libraries outside the Python Standard Library, they must be installed in the Python environment. Anaconda is the most popular Python library manager. Anaconda allows you to use and create Python virtual environments and install libraries to these environments. A Python virtual environment is a collection of libraries isolated from other virtual environments. These environments allow users to seamlessly organize programming projects. You can download Anaconda from the following link: https://www.anaconda.com/products/individual. When you download Anaconda, you have access to the Anaconda Navigator, a graphical user interface, and the Anaconda Prompt, a command prompt. Anaconda comes with an automatic environment called base. The following screenshot illustrates how to install a library to an environment using the Anaconda GUI. Using the drop down menu, navigate to Not installed. Then, select the desired library from the list. The search bar can be used to search for libraries. Finally, click the green Apply button to install the package. The following screenshot illustrates how to install a library to the base environment using the Anaconda Command Prompt. Sometimes the commands to download libraries are not as simple as shown in the above example. The Anaconda website provides commands for how to download popular Python libraries. R The main repository for R packages is the Comprehensive R Archive Network (CRAN). Another repository is Bioconductor, which provides tools for working with genomic data. Many packages are also distributed on GitHub. To install packages from CRAN use the install.packages() function. In RStudio, you can also go to ToolsInstall Packages for a dialog that will auto-complete package names as you type. &gt; # install the vcd package, a package for Visualizing Categorical Data &gt; install.packages(&quot;vcd&quot;) &gt; &gt; # load the package &gt; library(vcd) &gt; &gt; # see which packages on your computer have updates available &gt; old.packages() &gt; &gt; # download and install available package updates; &gt; # set ask = TRUE to verify installation of each package &gt; update.packages(ask = FALSE) To install R packages from GitHub use the install_github() function from the devtools package. You need to include the username of the repo owner followed by a forward slash and the name of the package. Typing two colons between a package and a function in the package allows you to use that function without loading the package. Thats how we use install_github() below. &gt; install.packages(&quot;devtools&quot;) &gt; devtools::install_github(&quot;username/packagename&quot;) Occasionally when installing package updates you will be asked, Do you want to install from sources the package which needs compilation? R packages on CRAN are compiled for Mac and Windows operating systems. That can take a day or two after a package has been submitted to CRAN. If you try to install a package that has not been compiled then youll get asked the question above. If you click Yes, R will try to compile the package on your computer. This will only work if you have the required build tools on your computer. For Windows this means having Rtools installed. Mac users should already have the necessary build tools. Unless you absolutely need the latest version of a package, its probably fine to click No. 1.6 Logic Python and R share the same relational operators for making comparisons: == (equals) != (not equal to) &lt; (less than) &lt;= (less than or equal to) &gt; (greater than) &gt;= (greater than or equal to) Likewise they share the same operators for logical AND and OR: &amp; (AND) | (OR) However R also has &amp;&amp; and || operators for programming control-flow. Python and R have different operators for negation. Python uses not. R uses !. Python These Python operators can be used to compare arrays to single values or other arrays. This operation returns an array containing true and false values. &gt; import numpy as np + + # Comparison of array to single value + x1 = np.array([1,5,9,12,11,6]) + x1 &lt; 8 + + # Comparison of array to another array array([ True, True, False, False, False, True]) &gt; x2 = np.array([2,4,6,14,15,7]) + x1 &gt; x2 array([False, True, True, False, False, False]) We can make multiple comparisons with the AND (and) and OR (or) operators. An important thing to note is that the and operator is inclusive, meaning that all statements must be true to return True. The or operator is exclusive, meaning that at least one of the statements joined by or must be true to return True. &gt; x=5 + y= 4 + + x &gt; 6 and y &lt; 10 False &gt; x &gt; 6 or y &lt; 10 True True and False operators have numeric values of 1 and 0, respectively. We can sum and average these values. &gt; # Sum of values greater than 10 in array x2 + np.sum(x2 &gt; 10) + + # Portion of values greater than 10 in array x2 2 &gt; np.mean(x2 &gt; 10) 0.3333333333333333 R Rs relational operators allow comparisons between a vector and a single value, or comparisons between two vectors. The result is a vector of TRUE/FALSE values. &gt; # vector compared with value &gt; x1 &lt;- c(1, 5, 9, 12, 11, 6) &gt; x1 &lt; 8 [1] TRUE TRUE FALSE FALSE FALSE TRUE &gt; &gt; # vector compared with vector &gt; x2 &lt;- c(2, 4, 6, 14, 15, 7) &gt; x1 &gt; x2 [1] FALSE TRUE TRUE FALSE FALSE FALSE Comparisons with NA (missing value) results in NA. &gt; x1 &lt;- c(1, 5, 9, NA, 11, 6) &gt; x1 &lt; 8 [1] TRUE TRUE FALSE NA FALSE TRUE Multiple comparisons can be made with AND (&amp;) and OR (|) operators. &gt; x2 &gt; 3 &amp; x2 &lt; 10 [1] FALSE TRUE TRUE FALSE FALSE TRUE &gt; x2 &lt; 3 | x2 &gt; 10 [1] TRUE FALSE FALSE TRUE TRUE FALSE TRUE/FALSE values in R have numeric values of 1/0. This allows us to sum and average them. (Note: an average of 0 and 1 values is the proportion of 1s.) &gt; # sum of values greater than 10 &gt; sum(x2 &gt; 10) [1] 2 &gt; &gt; # proportion of values greater than 10 &gt; mean(x2 &gt; 10) [1] 0.3333333 Use the ! operator for negation. This allows to check for something that is NOT TRUE. &gt; # which value are NOT less than 6 &gt; !x2 &lt; 6 [1] FALSE FALSE TRUE TRUE TRUE TRUE See the ?Comparison and ?Logic help pages for more information. 1.7 Generating a sequence of values In Python, one option for generating a sequence of values is arange() from NumPy. In R, a common approach is to use seq(). The sequences can be incremented by indicating a step argument in arange() or a by argument in seq(). Be aware that the end of the start/stop interval in arange() is open, but both sides of the from/to interval in seq() are closed. Python &gt; import numpy as np + x = np.arange(start = 1, stop = 11, step = 2) + x array([1, 3, 5, 7, 9]) R &gt; x &lt;- seq(from = 1, to = 11, by = 2) &gt; x [1] 1 3 5 7 9 11 1.8 Calculating means and medians The NumPy Python library has functions for calculating means and medians, and base R has functions for doing the same. Python Mean, using function from NumPy library &gt; import numpy as np + x = [90, 105, 110] + x_avg = np.mean(x) + print(x_avg) 101.66666666666667 Median, using function from NumPy library &gt; x = [98, 102, 20, 22, 304] + x_med = np.median(x) + print(x_med) 98.0 R Mean, using function from base R &gt; x &lt;- c(90, 105, 110) &gt; x_avg &lt;- mean(x) &gt; x_avg [1] 101.6667 Median, using function from base R &gt; x &lt;- c(98, 102, 20, 22, 304) &gt; x_med &lt;- median(x) &gt; x_med [1] 98 1.9 Writing your own functions Python and R allow and encourage users to create their own functions. Functions can be created, named, and stored in memory and used throughout a session. Or they can be created on-the-fly anonymously and used once. Python Functions in Python are defined by using the def keyword followed by the name we choose for our function with its arguments inside parentheses. We must include a return() statement after the body of our function to indicate the end of the function. The return statement takes an optional argument in its parentheses that will be the output of the function. Here we create a function to calculate the standard error of a mean (SEM) and call it SEM. &gt; def SEM(x): + import numpy as np # import statement included inside the function to ensure it&#39;s always imported + s = x.std(ddof=1) # find standard deviation of the data, specify delta degrees of freedom as 1 (makes denominator n-1 not n) + n = x.shape[0] # extract the length of the input array + sem = s / np.sqrt(n) # calculate the SEM + return(sem) # return the calculated SEM value Now lets try our function out on some test data. &gt; d = np.array([3,4,4,7,9,6,2,5,7]) + SEM(d) 0.7412035591181296 Oftentimes functions have built-in error-checking that returns messages describing the error. Here we show a simple error-check to ensure that the argument passed to our function is a number. &gt; def SEM(x): + import numpy as np + + if np.issubdtype(x.dtype,np.number)==False: + raise ValueError(&quot;Data must be numeric&quot;) + + s = x.std(ddof=1) + n = x.shape[0] + sem = s / np.sqrt(n) + return(sem) Python functions can return more than one result. It will output the results into a tuple. A tuple is a data structure very similar to a list, but it is immutable - we cannot change the order of the entries. Here we make our function return both the mean and the SEM of our data. &gt; def SEM(x): + import numpy as np + + if np.issubdtype(x.dtype,np.number)==False: + raise ValueError(&quot;Data must be numeric&quot;) + + s = x.std(ddof=1) + n = x.shape[0] + sem = s / np.sqrt(n) + + m = np.mean(x) + return(sem,m) R Functions in R can be created and named using function(). Add arguments inside the parentheses. Longer functions with multiple lines can be wrapped in curly braces {}. Below we create a function to calculate the standard error of a mean (SEM) and name it sem. It takes one argument: x, a vector of numbers. Both the function name and argument name(s) can be whatever we like, as long as they follow Rs naming conventions. &gt; sem &lt;- function(x){ + s &lt;- sd(x) + n &lt;- length(x) + s/sqrt(n) + } Now we can try it out on some test data. &gt; d &lt;- c(3,4,4,7,9,6,2,5,7) &gt; sem(d) [1] 0.7412036 Functions that will be used on different data and/or by different users often need built-in error-checking to return informative error messages. This simple example checks if the data are not numeric and returns a special error message. &gt; sem &lt;- function(x){ + if(!is.numeric(x)) stop(&quot;x must be numeric&quot;) + s &lt;- sd(x) + n &lt;- length(x) + s/sqrt(n) + } &gt; sem(c(1, 4, 6, &quot;a&quot;)) Error in sem(c(1, 4, 6, &quot;a&quot;)): x must be numeric R functions can also return more than one result. Below we return a list that holds the mean and SEM, but we could also return a vector, a data frame, or other data structure. Notice we also add an additional argument, ..., known as the three dots argument. This allows us to pass arguments for sd and mean directly through our own function. Below we pass through na.rm = TRUE to drop missing values. &gt; sem &lt;- function(x, ...){ + if(!is.numeric(x)) stop(&quot;x must be numeric&quot;) + s &lt;- sd(x, ...) + n &lt;- length(x) + se &lt;- s/sqrt(n) + mean &lt;- mean(x, ...) + list(mean = mean, SEM = se) + } &gt; &gt; d &lt;- c(1, 4, 6, 8, NA, 4, 4, 8, 6) &gt; sem(d, na.rm = TRUE) $mean [1] 5.125 $SEM [1] 0.7855339 Functions can also be created on-the-fly as anonymous functions. This simply means the functions are not saved as objects in memory. These are often used with Rs family of apply functions. As before, the functions can be created with function(). We can also use the backslash \\ as a shorthand for function(). We demonstrate both below with a data frame. &gt; # generate some example data &gt; d &lt;- data.frame(x1 = c(3, 5, 7, 1, 5, 4), + x2 = c(6, 9, 8, 9, 2, 5), + x3 = c(1, 9, 9, 7, 8, 4)) &gt; d x1 x2 x3 1 3 6 1 2 5 9 9 3 7 8 9 4 1 9 7 5 5 2 8 6 4 5 4 Now find the standard error of the mean for the three columns using an anonymous function with lapply(). The l means the result will be a list. We apply the function to each column of the data frame. &gt; lapply(d, function(x)sd(x)/sqrt(length(x))) $x1 [1] 0.8333333 $x2 [1] 1.118034 $x3 [1] 1.308094 We can also use the backslash as a shorthand for function(). &gt; lapply(d, \\(x)sd(x)/sqrt(length(x))) $x1 [1] 0.8333333 $x2 [1] 1.118034 $x3 [1] 1.308094 "],["data-structures.html", "Chapter 2 Data Structures 2.1 One-dimensional data 2.2 Two-dimensional data 2.3 Three-dimensional and higher data 2.4 General data structures", " Chapter 2 Data Structures This chapter compares and contrasts data structures in Python and R. 2.1 One-dimensional data A one-dimensional data structure can be visualized as a column in a spreadsheet or as a list of values. Python There are many ways to organize one-dimensional data in Python. Three of the most common one-dimensional data structures are lists, numpy arrays, and pandas Series. All three are ordered and mutable, and can contain data of different types. Lists in Python do not need to be explicitly declared; they are indicated by the use of square brackets. &gt; l = [1,2,3,&#39;hello&#39;] Values in lists can be accessed by using square brackets. Python indexing begins at 0, so to extract the first element, we would use the index 0. Python also allows for negative indexing; using an index of -1 will return the last value in the list. Indexing a range in Python is not inclusive of the last index. &gt; # extract first element + l[0] + + #extract last element 1 &gt; l[-1] + + # extract 2nd and 3rd elements &#39;hello&#39; &gt; l[1:3] [2, 3] Numpy arrays, on the other hand, need to be declared using the numpy.array() function, and the numpy package needs to be imported. &gt; import numpy as np + + arr = np.array([1,2,3,&#39;hello&#39;]) + print(arr) [&#39;1&#39; &#39;2&#39; &#39;3&#39; &#39;hello&#39;] Accessing data in a numpy array is the same as indexing a list. &gt; # extract first element + arr[0] + + # extract last element &#39;1&#39; &gt; arr[-1] + + # extract 2nd and 3rd elements &#39;hello&#39; &gt; arr[1:3] array([&#39;2&#39;, &#39;3&#39;], dtype=&#39;&lt;U11&#39;) Pandas Series also need to be declared using the pandas.Series() function. Like numpy, the pandas package must be imported as well. The pandas package is built on numpy, so we can input data into a pandas Series using a numpy array. We can extract data from the Series by using the index similar to indexing a list and numpy array. &gt; import pandas as pd + import numpy as np + + data = np.array([1,2,3,&quot;hello&quot;]) + ser1 = pd.Series(data) + print(ser1) + + # extract first element 0 1 1 2 2 3 3 hello dtype: object &gt; ser1[0] + + # extract 2nd and 3rd elements &#39;1&#39; &gt; ser1[1:3] 1 2 2 3 dtype: object To extract the last element of a pandas Series using -1, we need to use the iloc function. &gt; ser1.iloc[-1] &#39;hello&#39; We can relabel the indices of the Series to whatever we like using the index attribute within the Series function. &gt; import pandas as pd + import numpy as np + + ser2 = pd.Series(data, index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;]) + print(ser2) a 1 b 2 c 3 d hello dtype: object We can then use our own specified indices to select and index our data. Indexing with our labels can be done in two ways. One similar to indexing arrays and lists with square brackets using the .loc function, and the other follows this form: Series.label_name. &gt; + # extract element in row b + ser2.loc[&quot;b&quot;] + + # extract elements from row b to the end &#39;2&#39; &gt; ser2.loc[&quot;b&quot;:] + + # extract element in row &quot;d&quot; b 2 c 3 d hello dtype: object &gt; ser2.d + + # extract element in row &quot;b&quot; &#39;hello&#39; &gt; ser2.b &#39;2&#39; One thing to note is that mathematical operations cannot be carried out on lists, but they can be carried out on numpy arrays and pandas Series. In general, lists are better for short data sets that you will not be operating on mathematically. Numpy arrays and pandas Series are better for long data sets, and for data sets that will be operated on mathematically. R In R a one-dimensional data structure is called a vector. We can create a vector using the c() function. A vector in R can only contain one type of data (all numbers, all strings, etc). The columns of data frames are vectors. If multiple types of data are put into a vector, the data will be coerced according to the hierarchy logical &lt; integer &lt; double &lt; complex &lt; character. This means if you mix, say, integers and character data, all the data will be coerced to character. &gt; x1 &lt;- c(23, 43, 55) &gt; x1 [1] 23 43 55 &gt; &gt; # all values coerced to character &gt; x2 &lt;- c(23, 43, &#39;hi&#39;) &gt; x2 [1] &quot;23&quot; &quot;43&quot; &quot;hi&quot; Values in a vector can be accessed by position using indexing brackets. R indexes elements of a vector starting at 1. Index values are inclusive. For example, 2:3 selects the second and third elements. &gt; # extract the 2nd value &gt; x1[2] [1] 43 &gt; &gt; # extract the 2nd and 3rd value &gt; x1[2:3] [1] 43 55 2.2 Two-dimensional data Two-dimensional data are rectangular in nature, consisting of rows and columns. These can be the type of data you might find in a spreadsheet with a mix of data types in columns; they can also be matrices as you might encounter in matrix algebra. Python In Python, two common two-dimensional data structures include the numpy array and the pandas DataFrame. A two-dimensional numpy array is made in a similar way to the one-dimensional array using the numpy.array function. &gt; import numpy as np + + arr2d = np.array([[1,2,3,&quot;hello&quot;],[4,5,6,&quot;world&quot;]]) + print(arr2d) [[&#39;1&#39; &#39;2&#39; &#39;3&#39; &#39;hello&#39;] [&#39;4&#39; &#39;5&#39; &#39;6&#39; &#39;world&#39;]] Selecting data for a two-dimensional numpy array follows the same form as indexing a one-dimensional array. &gt; import numpy as np + + # extract first element + arr2d[0,0] + + # extract last element &#39;1&#39; &gt; arr2d[-1, -1] + + # extract 2nd and 3rd columns &#39;world&#39; &gt; arr2d[:,1:3] array([[&#39;2&#39;, &#39;3&#39;], [&#39;5&#39;, &#39;6&#39;]], dtype=&#39;&lt;U11&#39;) A pandas DataFrame is made using the pandas.DataFrame function in a similar way to the pandas Series. &gt; import pandas as pd + import numpy as np + + data = np.array([[1,2,3,&quot;hello&quot;],[4,5,6,&quot;world&quot;]]) + df = pd.DataFrame(data) + print(df) 0 1 2 3 0 1 2 3 hello 1 4 5 6 world Selecting data from a DataFrame is similar to that of the Series. &gt; # extract first element + df.loc[0,0] + + # extract column 1 &#39;1&#39; &gt; df.loc[0] + + # extract row 1 0 1 1 2 2 3 3 hello Name: 0, dtype: object &gt; df.loc[0,0] &#39;1&#39; Like the pandas Series, we can change the indices and the column names of the DataFrame and can use those to select and index our data. We change the indices again using the index attribute in the pandas.DataFrame function: &gt; import pandas as pd + import numpy as np + + data = np.array([[1,2,3,&quot;hello&quot;],[4,5,6,&quot;world&quot;]]) + df = pd.DataFrame(data, index=[&quot;a&quot;,&quot;b&quot;]) + print(df) 0 1 2 3 a 1 2 3 hello b 4 5 6 world We can change the column names using the columns attribute in the pandas.DataFrame function: &gt; import pandas as pd + import numpy as np + + data = np.array([[1,2,3,&quot;hello&quot;],[4,5,6,&quot;world&quot;]]) + df = pd.DataFrame(data, index=[&quot;a&quot;,&quot;b&quot;], columns=[&quot;column 1&quot;,&quot;column 2&quot;, &quot;column 3&quot;, &quot;column 4&quot;]) + print(df) column 1 column 2 column 3 column 4 a 1 2 3 hello b 4 5 6 world One thing to note is that numpy arrays can actually have N dimensions, whereas pandas DataFrames can only have two. Numpy arrays will be the better choice for data with more than two dimensions. R Two-dimensional data structures in R include the matrix and data frame. A matrix can contain only one data type. A data frame can contain multiple vectors, each of which can consist of different data types. Create a matrix with the matrix() function. Create a data frame with the data.frame() function. Most imported data comes into R as a data frame. &gt; # matrix; populated down by column by default &gt; m &lt;- matrix(data = c(1,3,5,7), nrow = 2, ncol = 2) &gt; m [,1] [,2] [1,] 1 5 [2,] 3 7 &gt; &gt; # data frame &gt; d &lt;- data.frame(name = c(&quot;Rob&quot;, &quot;Cindy&quot;), + age = c(35, 37)) &gt; d name age 1 Rob 35 2 Cindy 37 Values in a matrix and data frame can be accessed by position using indexing brackets. The first number(s) refers to rows; the second number(s) refers to columns. Leaving row or column numbers empty selects all rows or columns. &gt; # extract value in row 1, column 2 &gt; m[1,2] [1] 5 &gt; &gt; # extract values in row 2 &gt; d[2,] name age 2 Cindy 37 2.3 Three-dimensional and higher data Three-dimensional and higher data can be visualized as multiple rectangular structures stratified by extra variables. These are sometimes referred to as arrays. Analysts usually prefer two-dimensional data frames to arrays. Data frames can accommodate multidimensional data by including the additional dimensions as variables. Python To create a three-dimensional and higher data structure in Python, we again use a numpy array. We can think of the three-dimensional array as a stack of two-dimensional arrays. We construct this in the same way as the one- and two-dimensional arrays. &gt; import numpy as np + + arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]) + arr3d array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) We can also construct a three-dimensional numpy array using the reshape function on an existing array. The argument of reshape is where you input your desired dimensions - strata, rows, columns. Here, the arange function is used to create a numpy array containing the numbers 1 through 12 (to recreate the same array shown above). &gt; arr3d_2 = np.arange(1,13).reshape(2,2,3) + arr3d_2 array([[[ 1, 2, 3], [ 4, 5, 6]], [[ 7, 8, 9], [10, 11, 12]]]) Indexing the three-dimensional array follows the same format as the two-dimensional arrays. Since we can think of the three-dimensional array as a stack of two-dimensional arrays, we can extract each stacked two-dimensional array. Here we extract the first of the stacked two-dimensional arrays: &gt; # extract first strata (first &quot;stacked&quot; 2-D array) + arr3d[0] array([[1, 2, 3], [4, 5, 6]]) We can also extract entire rows and columns, and individual array elements: &gt; # extract 1st row of 2nd strata (second &quot;stacked&quot; 2-D array) + arr3d[1, 0] + + # extract 1st column of 2nd strata array([7, 8, 9]) &gt; arr3d[1, :, 0] + + # extract the number 6 (1st strata, 2nd row, 3rd column) array([ 7, 10]) &gt; arr3d[0, 1, 2] 6 The three-dimensional arrays can be converted to two-dimensional arrays again using the reshape function: &gt; arr3d_2d = arr3d.reshape(4,3) + arr3d_2d array([[ 1, 2, 3], [ 4, 5, 6], [ 7, 8, 9], [10, 11, 12]]) R The array() function in R can create three-dimensional and higher data structures. Arrays are like vectors and matrices in that they can only contain one data type. In fact matrices and arrays are sometimes described as vectors with instructions on how to layout the data. We can specify the dimension number and size using the dim argument. Below we specify 2 rows, 3 columns, and 2 strata using a vector: c(2,3,2). This creates a three-dimensional data structure. The data in the example are simply the numbers 1 through 12. &gt; a1 &lt;- array(data = 1:12, dim = c(2,3,2)) &gt; a1 , , 1 [,1] [,2] [,3] [1,] 1 3 5 [2,] 2 4 6 , , 2 [,1] [,2] [,3] [1,] 7 9 11 [2,] 8 10 12 Values in arrays can be accessed by position using indexing brackets. &gt; # extract value in row 1, column 2, strata 1 &gt; a1[1,2,1] [1] 3 &gt; &gt; # extract column 2 in both strata &gt; # result is returned as matrix &gt; a1[,2,] [,1] [,2] [1,] 3 9 [2,] 4 10 The dimensions can be named using the dimnames() function. Notice the names must be a list. &gt; dimnames(a1) &lt;- list(&quot;X&quot; = c(&quot;x1&quot;, &quot;x2&quot;), + &quot;Y&quot; = c(&quot;y1&quot;, &quot;y2&quot;, &quot;y3&quot;), + &quot;Z&quot; = c(&quot;z1&quot;, &quot;z2&quot;)) &gt; a1 , , Z = z1 Y X y1 y2 y3 x1 1 3 5 x2 2 4 6 , , Z = z2 Y X y1 y2 y3 x1 7 9 11 x2 8 10 12 The as.data.frame.table() function can collapse an array into a two-dimensional structure that may be easier to use with standard statistical and graphical routines. The responseName argument allows you to provide a suitable column name for the values in the array. &gt; as.data.frame.table(a1, responseName = &quot;value&quot;) X Y Z value 1 x1 y1 z1 1 2 x2 y1 z1 2 3 x1 y2 z1 3 4 x2 y2 z1 4 5 x1 y3 z1 5 6 x2 y3 z1 6 7 x1 y1 z2 7 8 x2 y1 z2 8 9 x1 y2 z2 9 10 x2 y2 z2 10 11 x1 y3 z2 11 12 x2 y3 z2 12 2.4 General data structures Both R and Python provide general catch-all data structures that can contain any number, shape, and type of data. Python The most general data structures in Python include the list and the tuple. Both lists and tuples are ordered collections of objects called elements. The elements can be other lists/tuples, arrays, integers, objects, etc. Lists are mutable objects; elements can be reordered or deleted and new elements can be added after the list has been created. Tuples, on the other hand, are immutable; once a tuple is created it cannot be changed. Lists are created using square brackets. Here we create a list and add an element to the list after it is created using the append function. &gt; lst = [1, 2, &#39;a&#39;, &#39;b&#39;, [3, 4, 5]] + lst [1, 2, &#39;a&#39;, &#39;b&#39;, [3, 4, 5]] &gt; lst.append(&#39;c&#39;) + lst [1, 2, &#39;a&#39;, &#39;b&#39;, [3, 4, 5], &#39;c&#39;] Tuples are created using parenthesis. Here we create a tuple. &gt; tuple = (1, 2, &#39;a&#39;,&#39;b&#39;, [3, 4, 5]) + tuple (1, 2, &#39;a&#39;, &#39;b&#39;, [3, 4, 5]) Lets try to use the append function to explore the immutability of the tuple. We expect to get an error. &gt; tuple.append(&#39;c&#39;) Error in py_call_impl(callable, dots$args, dots$keywords): AttributeError: &#39;tuple&#39; object has no attribute &#39;append&#39; Detailed traceback: File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; We can refer to specific list/tuple elements by using square brackets. In the square brackets we put the index number of the element. The element in the first position is at index 0. &gt; # Extract the first element of the list and the tuple + lst[0] 1 &gt; tuple[0] + + # Extract the last element of each 1 &gt; lst[-1] &#39;c&#39; &gt; tuple[-1] [3, 4, 5] R The most general data structure in R is the list. A list is an ordered collection of objects, which are referred to as the components. The components can be vectors, matrices, arrays, data frames, and other lists. The components are always numbered but can also have names. The results of statistical functions are often returned as lists. We can create lists with the list() function. The list below contains three components: a vector named x, a matrix named y, and a data frame named z. Notice the m and d objects were created in the two-dimensional data section earlier in this chapter. &gt; l &lt;- list(x = c(1,2,3), + y = m, + z = d) &gt; l $x [1] 1 2 3 $y [,1] [,2] [1,] 1 5 [2,] 3 7 $z name age 1 Rob 35 2 Cindy 37 We can refer to list components by their order number or name (if present). To use order number, use indexing brackets. Single brackets returns a list. Double brackets return the component itself. &gt; # second element returned as list &gt; l[2] $y [,1] [,2] [1,] 1 5 [2,] 3 7 &gt; &gt; # second element returned as itself (matrix) &gt; l[[2]] [,1] [,2] [1,] 1 5 [2,] 3 7 Use the $ operator to refer to components by name. This returns the component itself. &gt; l$y [,1] [,2] [1,] 1 5 [2,] 3 7 Finally it is worth noting that a data frame is a special case of a list consisting of components with the same length. The is.list() function returns TRUE if an object is a list and FALSE otherwise. &gt; # object d is data frame &gt; d name age 1 Rob 35 2 Cindy 37 &gt; str(d) &#39;data.frame&#39;: 2 obs. of 2 variables: $ name: chr &quot;Rob&quot; &quot;Cindy&quot; $ age : num 35 37 &gt; &gt; # but a data frame is a list &gt; is.list(d) [1] TRUE "],["import-export-and-save-data.html", "Chapter 3 Import, Export, and Save Data 3.1 CSV 3.2 XLS/XLSX (Excel) 3.3 JSON 3.4 XML 3.5 Exporting/Writing/Saving data and variables", " Chapter 3 Import, Export, and Save Data This chapter reviews importing external data into Python and R, including CSV, Excel, and other structured data files. There is often more than one way to import data into Python and R. Each example below highlights one way per file type. The data set we use for demonstration is the New York State Math Test Results by Grade from 2006 - 2011, downloaded from data.gov on September 30, 2021. The final section presents approaches to exporting and saving data. 3.1 CSV Comma separated value (CSV) files are text files with fields separated by commas. They are useful for rectangular data, where rows represent observations and columns represent variables or features. Python The pandas function read_csv() is a common approach to importing CSV files into Python. &gt; import pandas as pd + d = pd.read_csv(&#39;data/ny_math_test.csv&#39;) + d.loc[0:2, [&quot;Grade&quot;, &quot;Year&quot;, &quot;Mean Scale Score&quot;]] Grade Year Mean Scale Score 0 3 2006 700 1 4 2006 699 2 5 2006 691 R There are many ways to import a csv file. A common way is to use the base R function read.csv(). &gt; d &lt;- read.csv(&quot;data/ny_math_test.csv&quot;) &gt; d[1:3, c(&quot;Grade&quot;, &quot;Year&quot;, &quot;Mean.Scale.Score&quot;)] Grade Year Mean.Scale.Score 1 3 2006 700 2 4 2006 699 3 5 2006 691 Notice the spaces in the column names have been replaced with periods. Two packages that provide alternatives to read.csv() are readr and data.table. The readr function read_csv() returns a tibble. The data.table function fread() returns a data.table. 3.2 XLS/XLSX (Excel) Excel files are native to Microsoft Excel. Prior to 2007, Excel files had an extension of XLS. With the launch of Excel 2007, the extension was changed to XLSX. Excel files can have multiple sheets of data. This needs to be accounted for when importing into Python and R. Python The pandas function read_excel() is a common approach to importing Excel files into Python. The sheet_name argument allows you to specify which sheet you want to import. You can specify sheet by its (zero-indexed) ordering or by its name. Since this Excel file only has one sheet we do not need to use the argument. In addition, specifying sheet_name=None will read in all sheets and return a dict data structure where the key is the sheet name and the value is a DataFrame. &gt; import pandas as pd &gt; d = pd.read_excel(&#39;data/ny_math_test.xlsx&#39;) &gt; d.loc[0:2, [&quot;Grade&quot;, &quot;Year&quot;, &quot;Mean Scale Score&quot;]] &gt; R readxl is a well-documented and actively maintained package for importing Excel files into R. The workhorse function is read_excel(). The sheet argument allows you to specify which sheet you want to import. You can specify sheet by its ordering or by its name. Since this Excel file only has one sheet we do not need to use the argument. &gt; library(readxl) &gt; d_xls &lt;- read_excel(&quot;data/ny_math_test.xlsx&quot;) &gt; d_xls[1:3, c(&quot;Grade&quot;, &quot;Year&quot;, &quot;Mean Scale Score&quot;)] # A tibble: 3 x 3 Grade Year `Mean Scale Score` &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 2006 700 2 4 2006 699 3 5 2006 691 The result is a tibble, a tidyverse data frame. Its worth noting we can use the range argument to specify a range of cells to import. For example, if the top left corner of the data was B5 and the bottom right corner of the data was J54, we could enter range=\"B5:J54\" to just import that section of data. 3.3 JSON JSON (JavaScript Object Notation) is a flexible format for storing data. JSON files are text and can be viewed in any text editor. Because of their flexibility JSON files can be quite complex in the way they store data. Therefore there is no one-size-fits-all method for importing JSON files into Python or R. Python Below is one approach to importing our ny_math_test.json example file. We first import Pythons built-in json package and use its loads() function to read in the lines of the json file. The file is accessed using the open function and its associated read method. Next we use the pandas function json_normalize() to convert the data structure of the json data into a DataFrame. Finally we add column names to the DataFrame. &gt; import json + # load data using Python JSON module + with open(&#39;data/ny_math_test.json&#39;,&#39;r&#39;) as f: + data = json.loads(f.read()) + + import pandas as pd + d_json = pd.json_normalize(data, record_path =[&#39;data&#39;]) + + # add column names + names = list() + for i in range(23): + names.append(data[&#39;meta&#39;][&#39;view&#39;][&#39;columns&#39;][i][&#39;name&#39;]) + d_json.columns = names + + d_json.loc[0:2, [&quot;Grade&quot;, &quot;Year&quot;, &quot;Mean Scale Score&quot;]] Grade Year Mean Scale Score 0 3 2006 700 1 4 2006 699 2 5 2006 691 Again, this is just one approach that assumes we want a DataFrame. R jsonlite is one of several R packages available for importing JSON files into R. The read_json() function takes a JSON file and returns a list or data frame depending on the structure of the data file and its arguments. We set simplifyVector = TRUE so the data is simplified into a matrix. &gt; library(jsonlite) &gt; d_json &lt;- read_json(&#39;data/ny_math_test.json&#39;, simplifyVector = TRUE) The d_json object is a list with two elements: meta and data. The data element is a matrix that contains the data of interest. The meta element contains the column names for the data (among much else). Notice we had to drill down in the list to find the column names. We assign column names to the matrix using the colnames() function and then convert the matrix to a data frame using the as.data.frame() function. &gt; colnames(d_json$data) &lt;- d_json$meta$view$columns$fieldName &gt; d_json &lt;- as.data.frame(d_json$data) &gt; d_json[1:3,c(&quot;grade&quot;, &quot;year&quot;, &quot;mean_scale_score&quot;)] grade year mean_scale_score 1 3 2006 700 2 4 2006 699 3 5 2006 691 3.4 XML XML (eXtensible Markup Language) is a markup language that was designed to store data. XML files are text and can be viewed in any text editor or a web browser. Because of their flexibility, XML files can be quite complex in the way they store data. Therefore there is no one-size-fits-all approach for importing XML files into Python or R. Python The pandas library provides the read_xml function for importing XML files. The ny_math_test.xml file identifies records with nodes named row. The 168 rows are nested in one node also called row. Therefore we use the xpath argument to specify that we want to elect all row elements that are descendant of the single row element. &gt; import pandas as pd + d_xml = pd.read_xml(&#39;data/ny_math_test.xml&#39;, xpath=&quot;row//row&quot;) + + d_xml.loc[0:2, [&quot;grade&quot;, &quot;year&quot;, &quot;mean_scale_score&quot;]] grade year mean_scale_score 0 3 2006 700 1 4 2006 699 2 5 2006 691 R xml2 is a relatively small but powerful package for importing and working with XML files. The read_xml() function imports an XML file and returns a list of pointers to XML nodes. There are a number of ways to proceed once you import an XML file, such as using the xml_find_all() function to find nodes that match an xpath expression. Below we take a simple approach and convert the XML nodes into a list using the as_list() function that is part of the xml2 package. Once we have the XML nodes in a list, we can use the bind_rows() function in the dplyr package to create a data frame. Notice we have to drill down into the list to select the element that contains the data. After this we need to do one more thing: unlist each the columns into vectors. We do this by applying the unlist function to each column of d. We save the result by assigning to d[], which overwrites each element (or column) of d with the unlisted result. &gt; library(xml2) &gt; d_xml &lt;- read_xml(&#39;data/ny_math_test.xml&#39;) &gt; d_list &lt;- as_list(d_xml) &gt; d &lt;- dplyr::bind_rows(d_list$response$row) &gt; d[] &lt;- lapply(d, unlist) &gt; d[1:3,c(&quot;grade&quot;, &quot;year&quot;, &quot;mean_scale_score&quot;)] # A tibble: 3 x 3 grade year mean_scale_score &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 3 2006 700 2 4 2006 699 3 5 2006 691 The result is a tibble, a tidyverse data frame. We would most likely want to proceed to converting certain columns to numeric. 3.5 Exporting/Writing/Saving data and variables There are several ways to export/write/save files from Python and R. The following examples highlight some of these ways. Python The pandas function to_csv() saves a pandas DataFrame as a csv file. &gt; # pass a file name to the function + d.to_csv(&quot;data.csv&quot;) The Python package pickle allows you to write (save) any object from the Python environment and read (load) any object you have written into the Python environment. The following code writes to a pickle file. The first line opens the file object being written to. In the open function, file_name specifies the file path of the file object. Then, wb stands for write binary, which means the file is being written in binary form (1s and 0s). After the as keyword, file_, is the user selected name of the file object. The second line uses the pickle.dump() function. This function requires two arguments: the object being written and the name of the file object. &gt; import pickle + + # define the file name + file_name = &#39;data.pickle&#39; + + # write the variable to the file system + with open(file_name, &#39;wb&#39;) as file_: + pickle.dump(d, file_) The following code reads to a pickle file. The first line opens the file object being read from. In the open function, data.pickle specifies the file path of the file object. Then, rb stands for read binary, which means the file is being read in binary form (1s and 0s). After the as keyword, my_file, is the user selected name of the file object. The second line uses the pickle.load() function. This function requires one argument: the name of the file object. &gt; + # read the specified file from the file system and load into variable + with open(&#39;data.pickle&#39;, &#39;rb&#39;) as my_file: + d = pickle.load(my_file) R To export a matrix or data frame to a CSV file, use the write.csv() function. To export to a file with a different field separator, such as a tab, use write.table(). The minimal arguments for write.csv() are the object and the file name. To export a data frame named dat to a file named dat.csv to your current working directory: &gt; write.csv(dat, file = &quot;dat.csv&quot;) By default a column for row names or numbers is included in the exported csv file. To turn that off, set row.names = FALSE, like so: &gt; write.csv(dat, file = &quot;dat.csv&quot;, row.names = FALSE) To append a matrix or data frame to an existing csv file, set append = TRUE. See also sink(), cat(), and writeLines() for sending text and output to a file. To save and load R objects for future use in R, there are two options: Save and load a single object using saveRDS() and readRDS(). Save multiple objects using save() and load(). Save and load a single object The minimal arguments for saveRDS() are the object and a file name with an .rds extension. For example, to save a single data frame named dat to your current working directory as dat.rds: &gt; saveRDS(dat, file = &quot;dat.rds&quot;) To load the rds file into R from your current working directory, use the readRDS() function. Notice we must assign the result of readRDS() to an object. The object name need not match the file name. &gt; d &lt;- readRDS(&quot;dat.rds&quot;) The advantage of saving and loading native R objects is the preservation of characteristics such as factors, attributes, classes, etc. Any object can be saved, including model objects, functions, vectors, lists, etc. Save multiple objects The minimal arguments for save() are the objects to save and a file name with a .rda extension. Objects can also be specified as a character vector to the list argument. For example, to save a data frame named dat, a model object named m, and a plot object called p, to your current working directory as work.rda: &gt; save(dat, m, p, file = &quot;work.rda&quot;) Or with objects specified as a character vector: &gt; save(list = c(&quot;dat&quot;, &quot;m&quot;, &quot;p&quot;), file = &quot;work.rda&quot;) To load the rda file from your current working directory, use the load() function. Notice we do not assign the result to an object name. The result of the load() function is to load the objects into your global environment. &gt; load(&quot;work.rda&quot;) Upon successful execution of the load() function, the dat, m, and p objects will be loaded into your global environment. Any objects already in your global environment with the same name will be overwritten without warning. You can also save everything in your global environment into a rda file using the save.image() function. It works just like the save() function except you do not specify which objects to save. You simply provide a file name. If you do not specify a file name, a default name of .Rdata is used. To load the file use the load() function. Again, all objects will be loaded into the gloabal environment, overwriting any existing objects with the same name. "],["data-manipulation.html", "Chapter 4 Data Manipulation 4.1 Names of variables and their types 4.2 Select variables 4.3 Filter/Subset variables 4.4 Rename variables 4.5 Create, replace and remove variables 4.6 Create strings from numbers 4.7 Create numbers from strings 4.8 Combine strings 4.9 Finding and replacing patterns within strings 4.10 Change case 4.11 Drop duplicate rows 4.12 Format dates 4.13 Randomly sample rows", " Chapter 4 Data Manipulation This chapter looks at various strategies for filtering, selecting, modifying and deriving variables in data. Unless otherwise stated, examples are for DataFrames (Python) and data frames (R) and use the mtcars data frame that is included with R. &gt; # Python + import pandas + mtcars = pandas.read_csv(&#39;data/mtcars.csv&#39;) &gt; # R &gt; data(mtcars) &gt; # drop row names to match Python version of data &gt; rownames(mtcars) &lt;- NULL 4.1 Names of variables and their types View and inspect the names of variables and their type (numeric, string, logical, etc.) This is useful to ensure that variables have the expected type. Python The .info() function in pandas lists information on the DataFrame. Setting the argument verbose to True prints the name of the columns, their length excluding NULL values, and their data type (dtype) in a table. The function lists the unique data types in the DataFrame, and it prints how much memory the DataFrame takes up. &gt; mtcars.info(verbose=True) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32 entries, 0 to 31 Data columns (total 11 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 mpg 32 non-null float64 1 cyl 32 non-null int64 2 disp 32 non-null float64 3 hp 32 non-null int64 4 drat 32 non-null float64 5 wt 32 non-null float64 6 qsec 32 non-null float64 7 vs 32 non-null int64 8 am 32 non-null int64 9 gear 32 non-null int64 10 carb 32 non-null int64 dtypes: float64(5), int64(6) memory usage: 2.9 KB Setting verbose to False excludes the table describing each column. &gt; mtcars.info(verbose=False) &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32 entries, 0 to 31 Columns: 11 entries, mpg to carb dtypes: float64(5), int64(6) memory usage: 2.9 KB If a DataFrame has 100 or fewer columns, the verbose argument defaults to True. R The str() function in R lists the names of the variables, their type, the first few values, and the dimensions of the data frame. &gt; str(mtcars) &#39;data.frame&#39;: 32 obs. of 11 variables: $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... $ disp: num 160 160 108 258 360 ... $ hp : num 110 110 93 110 175 105 245 62 95 123 ... $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... $ wt : num 2.62 2.88 2.32 3.21 3.44 ... $ qsec: num 16.5 17 18.6 19.4 17 ... $ vs : num 0 0 1 1 0 1 0 1 1 1 ... $ am : num 1 1 1 0 0 0 0 0 0 0 ... $ gear: num 4 4 4 3 3 3 3 4 4 4 ... $ carb: num 4 4 1 1 2 1 4 2 2 4 ... To see just the names of the data frame, use the names() function. &gt; names(mtcars) [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; To see just the dimensions of the data frame, use the dim() function. It returns the number of rows and columns, respectively. &gt; dim(mtcars) [1] 32 11 4.2 Select variables How to select specific columns of data frames. Python The period operator . provides access to a column in a DataFrame as a vector. This returns pandas Series. A pandas series can do everything a numpy array can do. &gt; mtcars.mpg 0 21.0 1 21.0 2 22.8 3 21.4 4 18.7 5 18.1 6 14.3 7 24.4 8 22.8 9 19.2 10 17.8 11 16.4 12 17.3 13 15.2 14 10.4 15 10.4 16 14.7 17 32.4 18 30.4 19 33.9 20 21.5 21 15.5 22 15.2 23 13.3 24 19.2 25 27.3 26 26.0 27 30.4 28 15.8 29 19.7 30 15.0 31 21.4 Name: mpg, dtype: float64 Indexing also provides access to columns as a pandas Series. Single and double quotations both work. &gt; mtcars[&#39;mpg&#39;] 0 21.0 1 21.0 2 22.8 3 21.4 4 18.7 5 18.1 6 14.3 7 24.4 8 22.8 9 19.2 10 17.8 11 16.4 12 17.3 13 15.2 14 10.4 15 10.4 16 14.7 17 32.4 18 30.4 19 33.9 20 21.5 21 15.5 22 15.2 23 13.3 24 19.2 25 27.3 26 26.0 27 30.4 28 15.8 29 19.7 30 15.0 31 21.4 Name: mpg, dtype: float64 Operations on numpy arrays are faster than operations on pandas Series. But using pandas series should be fine, in terms of performance, in many cases. This is important for large data sets on which many operations are performed. The .values function returns a numpy array. &gt; mtcars[&#39;mpg&#39;].values array([21. , 21. , 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5, 15.2, 13.3, 19.2, 27.3, 26. , 30.4, 15.8, 19.7, 15. , 21.4]) Double indexing returns a pandas DataFrame, instead of a numpy array or pandas series. &gt; mtcars[[&#39;mpg&#39;]] mpg 0 21.0 1 21.0 2 22.8 3 21.4 4 18.7 5 18.1 6 14.3 7 24.4 8 22.8 9 19.2 10 17.8 11 16.4 12 17.3 13 15.2 14 10.4 15 10.4 16 14.7 17 32.4 18 30.4 19 33.9 20 21.5 21 15.5 22 15.2 23 13.3 24 19.2 25 27.3 26 26.0 27 30.4 28 15.8 29 19.7 30 15.0 31 21.4 The head() and tail() functions return the first 5 or last 5 values. Use the n argument to change the number of values. This function works on numpy arrays, pandas series and pandas DataFrames. &gt; # first 6 values + mtcars.mpg.head() 0 21.0 1 21.0 2 22.8 3 21.4 4 18.7 Name: mpg, dtype: float64 &gt; # last row of DataFrame + mtcars.tail(n=1) mpg cyl disp hp drat wt qsec vs am gear carb 31 21.4 4 121.0 109 4.11 2.78 18.6 1 1 4 2 R The dollar sign operator, $, provides access to a column in a data frame as a vector. &gt; mtcars$mpg [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 [31] 15.0 21.4 Double-indexing brackets also provide access to columns as a vector. &gt; mtcars[[&quot;mpg&quot;]] [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3 15.2 10.4 [16] 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3 26.0 30.4 15.8 19.7 [31] 15.0 21.4 Single-indexing brackets work as well, but they return a data frame instead of a vector (if used with a data frame). &gt; mtcars[&quot;mpg&quot;] mpg 1 21.0 2 21.0 3 22.8 4 21.4 5 18.7 6 18.1 7 14.3 8 24.4 9 22.8 10 19.2 11 17.8 12 16.4 13 17.3 14 15.2 15 10.4 16 10.4 17 14.7 18 32.4 19 30.4 20 33.9 21 21.5 22 15.5 23 15.2 24 13.3 25 19.2 26 27.3 27 26.0 28 30.4 29 15.8 30 19.7 31 15.0 32 21.4 Single-indexing brackets also allow selection of rows when used with a comma. The syntax is rows, columns &gt; # first three rows &gt; mtcars[1:3, &quot;mpg&quot;] [1] 21.0 21.0 22.8 Finally single-indexing brackets allow us to select multiple columns. Request columns either by name or position using a vector. &gt; mtcars[c(&quot;mpg&quot;, &quot;cyl&quot;)] mpg cyl 1 21.0 6 2 21.0 6 3 22.8 4 4 21.4 6 5 18.7 8 6 18.1 6 7 14.3 8 8 24.4 4 9 22.8 4 10 19.2 6 11 17.8 6 12 16.4 8 13 17.3 8 14 15.2 8 15 10.4 8 16 10.4 8 17 14.7 8 18 32.4 4 19 30.4 4 20 33.9 4 21 21.5 4 22 15.5 8 23 15.2 8 24 13.3 8 25 19.2 8 26 27.3 4 27 26.0 4 28 30.4 4 29 15.8 8 30 19.7 6 31 15.0 8 32 21.4 4 &gt; # same as mtcars[1:2] The head() and tail() functions return the first 6 or last 6 values. Use the n argument to change the number of values. They work with vectors or data frames. &gt; # first 6 values &gt; head(mtcars$mpg) [1] 21.0 21.0 22.8 21.4 18.7 18.1 &gt; # last row of data frame &gt; tail(mtcars, n = 1) mpg cyl disp hp drat wt qsec vs am gear carb 32 21.4 4 121 109 4.11 2.78 18.6 1 1 4 2 4.3 Filter/Subset variables How to view rows of a data frame that meet certain conditions. Python We can filter rows of a DataFrame based on a condition to subset. The data type returned depends on the filtration method. The following code returns a DataFrame, not a Series, as there is more than one column selected from the DataFrame. Use a list, square brackets [], to subset more than one column. &gt; mtcars[mtcars[&quot;mpg&quot;] &gt; 30][[&quot;mpg&quot;, &quot;cyl&quot;]] mpg cyl 17 32.4 4 18 30.4 4 19 33.9 4 27 30.4 4 Both pandas Series and NumPy arrays can be used for faster performance or vector operations. Many functions require a vector as input. The following code returns one column, mpg, as a pandas Series. A pandas Series is one column from a pandas DataFrame. &gt; mtcars[mtcars[&quot;mpg&quot;] &gt; 30][&quot;mpg&quot;] 17 32.4 18 30.4 19 33.9 27 30.4 Name: mpg, dtype: float64 The following code also returns a pandas Series, but using the . operator to select for a column, rather than square brackets []. &gt; mtcars[mtcars[&quot;mpg&quot;] &gt; 30].mpg 17 32.4 18 30.4 19 33.9 27 30.4 Name: mpg, dtype: float64 Both of the following lines of code return NumPy arrays using the .values function. df1 is one dimension, for the one column, and df1 is two dimensions, for the two columns. &gt; df1 = mtcars[mtcars[&quot;mpg&quot;] &gt; 30][&quot;mpg&quot;].values + df2 = mtcars[mtcars[&quot;mpg&quot;] &gt; 30][[&quot;mpg&quot;, &quot;cyl&quot;]].values You can also filter with multiple row conditions. &gt; mtcars[mtcars[&quot;mpg&quot;] &gt; 30][mtcars[&quot;hp&quot;] &lt; 66] mpg cyl disp hp drat wt qsec vs am gear carb 18 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 19 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 &lt;string&gt;:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index. R In base R, we can use subsetting brackets or the subset() function to select rows based on some condition. Below we demonstrate both approaches to view only those rows with mpg greater than 30. First we begin with subsetting brackets. The subsetting brackets take three arguments: i: the condition to subset on. j: the columns to show. If none specified, all columns are returned drop: an optional logical argument (TRUE/FALSE) to determine whether or not to coerce the output to the lowest possible dimension. The default is TRUE. We rarely type the first two argument names, i and j, when using subsetting brackets. This example returns only the rows with mpg &gt; 30 and all columns. Notice we need to preface mpg with mtcars$ to tell R where to find the mpg column and that we need to provide a comma after the condition. &gt; mtcars[mtcars$mpg &gt; 30, ] mpg cyl disp hp drat wt qsec vs am gear carb 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 We can select what columns to see in the second argument as a vector. Notice we only need to specify the column names as a character vector. We can also use numbers corresponding to the column number as well as conditional statements. &gt; mtcars[mtcars$mpg &gt; 30, c(&quot;mpg&quot;, &quot;wt&quot;, &quot;gear&quot;)] mpg wt gear 18 32.4 2.200 4 19 30.4 1.615 4 20 33.9 1.835 4 28 30.4 1.513 5 Show first three columns. &gt; mtcars[mtcars$mpg &gt; 30, 1:3] mpg cyl disp 18 32.4 4 78.7 19 30.4 4 75.7 20 33.9 4 71.1 28 30.4 4 95.1 Show columns with names consisting of only two characters. The nchar() function counts the number of characters in a string. The expression nchar(names(mtcars)) == 2 returns a vector of TRUE/FALSE values where TRUE indicates the column name is only two characters in length. &gt; mtcars[mtcars$mpg &gt; 30, nchar(names(mtcars)) == 2] hp wt vs am 18 66 2.200 1 1 19 52 1.615 1 1 20 65 1.835 1 1 28 113 1.513 1 1 Notice when we specify only one column, the brackets return a vector. &gt; mtcars[mtcars$mpg &gt; 30, &quot;mpg&quot;] [1] 32.4 30.4 33.9 30.4 To get a data frame, set the drop argument to FALSE. &gt; mtcars[mtcars$mpg &gt; 30, &quot;mpg&quot;, drop = FALSE] mpg 18 32.4 19 30.4 20 33.9 28 30.4 The subset() function allows us to refer to column names without using the $ extractor function or quoting column names. It also has a drop argument but its default is FALSE. It has four arguments: x: the data frame to subset. subset: the condition to subset on. select: the columns to select. drop: an optional logical argument (TRUE/FALSE) to determine whether or not to coerce the output to the lowest possible dimension. The default is FALSE. We rarely type the first three argument names, x, subset and select, when using subset(). Below we replicate the previous examples using subset(). &gt; # rows where mpg &gt; 30 and all columns &gt; subset(mtcars, mpg &gt; 30) mpg cyl disp hp drat wt qsec vs am gear carb 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 19 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 &gt; # rows where mpg &gt; 30 and the mpg, wt, and gear columns &gt; subset(mtcars, mpg &gt; 30, c(mpg, wt, gear)) mpg wt gear 18 32.4 2.200 4 19 30.4 1.615 4 20 33.9 1.835 4 28 30.4 1.513 5 &gt; # rows where mpg &gt; 30 and the first three columns &gt; subset(mtcars, mpg &gt; 30, 1:3) mpg cyl disp 18 32.4 4 78.7 19 30.4 4 75.7 20 33.9 4 71.1 28 30.4 4 95.1 &gt; # rows where mpg &gt; 30 and columns consisting of two characters &gt; subset(mtcars, mpg &gt; 30, nchar(names(mtcars)) == 2) hp wt vs am 18 66 2.200 1 1 19 52 1.615 1 1 20 65 1.835 1 1 28 113 1.513 1 1 &gt; # rows where mpg &gt; 30 and mpg column, as a vector &gt; subset(mtcars, mpg &gt; 30, mpg, drop = TRUE) [1] 32.4 30.4 33.9 30.4 &gt; # rows where mpg &gt; 30 and mpg column, as a data frame &gt; subset(mtcars, mpg &gt; 30, mpg) mpg 18 32.4 19 30.4 20 33.9 28 30.4 Another difference between subsetting brackets and the subset() function is how they handle missing values. Subsetting brackets return missing values while subset() does not. We demonstrate with a toy data frame. Notice the x column has a missing value. &gt; dframe &lt;- data.frame(x = c(1, NA, 5), + y = c(12, 21, 34)) &gt; dframe x y 1 1 12 2 NA 21 3 5 34 When we condition on x &lt; 3, the subsetting bracket approach returns a row with NA values. &gt; dframe[dframe$x &lt; 3,] x y 1 1 12 NA NA NA The subset() approach ignores the missing value. &gt; subset(dframe, x &lt; 3) x y 1 1 12 To replicate the subset() result with the subsetting brackets, we need to include an additional condition to only show rows where x is NOT missing. We can do that with the is.na() function. The is.na() function returns TRUE if a value is missing and FALSE otherwise. If we preface with !, we get TRUE if a value is NOT missing and FALSE otherwise. &gt; dframe[dframe$x &lt; 3 &amp; !is.na(dframe$x),] x y 1 1 12 See also the filter() function in the dplyr package and the enhanced subsetting brackets in the data.table package. 4.4 Rename variables How to rename variables or column headers. Python Column names can be changed using the function .rename(). Below, we change the column names cyl and wt to cylinder and WT, respectively. &gt; mtcars.rename(columns={&quot;cyl&quot;:&quot;cylinder&quot;, &quot;wt&quot;:&quot;WT&quot;}) mpg cylinder disp hp drat WT qsec vs am gear carb 0 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 6 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 7 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 8 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 9 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 10 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 11 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 12 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 13 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 14 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 15 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 16 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 17 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 18 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 19 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 20 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 21 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 22 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 23 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 24 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 25 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 26 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 27 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 28 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 29 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 30 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 31 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 Alternatively, column names can be changed by replacing the vector of column names with a new vector. Below, we create a vector of columns that replaces drat with axle_ratio using conditional match and indexing and disp with DISP using indexing. &gt; column_names = mtcars.columns.values + + # using conditional match + column_names[column_names == &quot;drat&quot;] = &quot;axle_ratio&quot; + + # using indexing + column_names[2] = &quot;DISP&quot; + + mtcars.columns = column_names + mtcars.columns Index([&#39;mpg&#39;, &#39;cyl&#39;, &#39;DISP&#39;, &#39;hp&#39;, &#39;axle_ratio&#39;, &#39;wt&#39;, &#39;qsec&#39;, &#39;vs&#39;, &#39;am&#39;, &#39;gear&#39;, &#39;carb&#39;], dtype=&#39;object&#39;) R Variable names can be changed by their index (ie, order of columns in the data frame). Below the second column is cyl. We change the name to cylinders. &gt; names(mtcars)[2] [1] &quot;cyl&quot; &gt; names(mtcars)[2] &lt;- &quot;cylinders&quot; &gt; names(mtcars) [1] &quot;mpg&quot; &quot;cylinders&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; [7] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; &quot;carb&quot; Variable names can also be changed by conditional match. Below we find the variable name that matches drat and change to axle_ratio. &gt; names(mtcars)[names(mtcars) == &quot;drat&quot;] [1] &quot;drat&quot; &gt; names(mtcars)[names(mtcars) == &quot;drat&quot;] &lt;- &quot;axle_ratio&quot; &gt; names(mtcars) [1] &quot;mpg&quot; &quot;cylinders&quot; &quot;disp&quot; &quot;hp&quot; &quot;axle_ratio&quot; [6] &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; More than one variable name can be changed using a vector of positions or matches. &gt; names(mtcars)[c(6,8)] &lt;- c(&quot;weight&quot;, &quot;engine&quot;) &gt; &gt; # or &gt; # names(mtcars)[names(mtcars) %in% c(&quot;wt&quot;, &quot;vs&quot;)] &lt;- c(&quot;weight&quot;, &quot;engine&quot;) &gt; &gt; names(mtcars) [1] &quot;mpg&quot; &quot;cylinders&quot; &quot;disp&quot; &quot;hp&quot; &quot;axle_ratio&quot; [6] &quot;weight&quot; &quot;qsec&quot; &quot;engine&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; See also the rename() function in the dplyr. 4.5 Create, replace and remove variables We often need to create variables that are functions of other variables, or replace existing variables with an updated version. Python Adding a new variable using the indexing notation and assigning a result adds a new column. &gt; # add column for Kilometer per liter + mtcars[&#39;kpl&#39;] = mtcars.mpg/2.352 Doing the same with an existing column name updates the values in a column. &gt; # update to liters per 100 Kilometers + mtcars[&#39;kpl&#39;] = 100/mtcars.kpl Alternatively, the . notation can be used to update the values in a column. &gt; # update to liters per 50 Kilometers + mtcars.kpl = 50/mtcars.kpl To remove a column, use the .drop() function. &gt; # drop the kpl variable + mtcars.drop(columns=[&#39;kpl&#39;]) mpg cyl DISP hp axle_ratio wt qsec vs am gear carb 0 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 1 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 2 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 4 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 6 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 7 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 8 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 9 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 10 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 11 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 12 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 13 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 14 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 15 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 16 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 17 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 18 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 19 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 20 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 21 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 22 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 23 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 24 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 25 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 26 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 27 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 28 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 29 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 30 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 31 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 R Adding a new variable name after the dollar sign notation and assigning a result adds a new column. &gt; # add column for Kilometer per liter &gt; mtcars$kpl &lt;- mtcars$mpg/2.352 Doing the same with an existing variable updates the values in a column. &gt; # update to liters per 100 Kilometers &gt; mtcars$kpl &lt;- 100/mtcars$kpl To remove a variable, assign it NULL. &gt; # drop the kpl variable &gt; mtcars$kpl &lt;- NULL See also the mutate() function in the dplyr package. 4.6 Create strings from numbers You may have data that is numeric but that needs to be treated as a string. Python You can change the data type of a column in a DataFrame using the astype function. &gt; mtcars[&#39;am&#39;] = mtcars[&#39;am&#39;].astype(str) + type(mtcars.am[0]) # check the type of the first item in &#39;am&#39; column &lt;class &#39;str&#39;&gt; A potential number-to-string conversion task in Python might be formatting 5-digit American zip codes. Some zip codes begin with 0, but if stored as a numeric value, the 0 is dropped. For example, consider the following pandas DataFrame. Notice the leading 0 is dropped from two of the zip codes. &gt; zc = pandas.read_csv(&#39;data/zc.csv&#39;) + print(zc) state zip 0 VT 5001 1 VA 22901 2 NH 3282 One way to fix this is using the string zfill() method. First we convert the numeric column to string type using the method we just demonstrated. Then we access the zip column using zc.zip and the zfill() method using str.zfill with the width parameter set to 5. This pads the string with 0 on the left to make each value 5 characters wide. &gt; zc[&#39;zip&#39;] = zc[&#39;zip&#39;].astype(str) + zc[&#39;zip&#39;] = zc.zip.str.zfill(5) + print(zc) state zip 0 VT 05001 1 VA 22901 2 NH 03282 If we knew we were importing zip codes using read_csv, we could also use the dtype argument to specify which storage type to use for the zip column. Below we pass a dictionary that maps the str type to the zip column. The result is a properly formatted zip code column. &gt; zc = pandas.read_csv(&#39;data/zc.csv&#39;, dtype = {&#39;zip&#39;: &#39;str&#39;}) + print(zc) state zip 0 VT 05001 1 VA 22901 2 NH 03282 R The as.character() function takes a vector and converts it to string format. &gt; head(mtcars$am) [1] 1 1 1 0 0 0 &gt; head(as.character(mtcars$am)) [1] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; Note we just demonstrated conversion. To save the conversion we need to assign the result to the data frame. &gt; # add new string variable am_ch &gt; mtcars$am_ch &lt;- as.character(mtcars$am) &gt; head(mtcars$am_ch) [1] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; The factor() function can also be used to convert a numeric vector into a categorical variable. The result is not exactly a string, however. A factor is made of integers with character labels. Factors are useful for character data that have a fixed set of levels (eg, grade 1, grade 2, etc) &gt; # convert to factor &gt; head(mtcars$am) [1] 1 1 1 0 0 0 &gt; head(factor(mtcars$am)) [1] 1 1 1 0 0 0 Levels: 0 1 &gt; &gt; # convert to factor with labels &gt; head(factor(mtcars$am, labels = c(&quot;automatic&quot;, &quot;manual&quot;))) [1] manual manual manual automatic automatic automatic Levels: automatic manual Again we just demonstrated factor conversion. To save the conversion we need to assign to the data frame. &gt; # create factor variable am_fac &gt; mtcars$am_fac &lt;- factor(mtcars$am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)) &gt; head(mtcars$am_fac) [1] manual manual manual automatic automatic automatic Levels: automatic manual A common number-to-string conversion task in R is formatting 5-digit American zip codes. Some zip codes begin with 0, but if stored as a numeric value, the 0 is dropped. &gt; zip_codes &lt;- c(03766, 03748, 22901, 03264) &gt; zip_codes [1] 3766 3748 22901 3264 We need to store the zip code as a character value so the 0 is preserved. One way to do this is via the sprintf() function in base R. The first argument is the format string or conversion specification. A conversion specification begins with %. The following 0 and 5 says to format the zip_codes vector as a 5-digit string padded by zeroes on the left. The final i says were working with integer values. &gt; sprintf(&quot;%05i&quot;, zip_codes) [1] &quot;03766&quot; &quot;03748&quot; &quot;22901&quot; &quot;03264&quot; See also the str_pad() function in the stringr package. 4.7 Create numbers from strings String variables that ought to be numbers usually have some character data in the values such as units (eg, 4 cm). To create numbers from strings its important to remove any character data that cannot be converted to a number. Python The astype(float) or astype(int) function will coerce strings to numerical representation. For demonstration, lets say we have the following numpy array. &gt; import numpy as np + weight = np.array([&quot;125 lbs.&quot;, &quot;132 lbs.&quot;, &quot;156 lbs.&quot;]) The astype(float) function throws an error due to the presence of strings. The astype() function is for numpy arrays. &gt; try: + weight.astype(float) + except ValueError: + print(&quot;ValueError: could not convert string to float: &#39;125 lbs.&#39;&quot;) ValueError: could not convert string to float: &#39;125 lbs.&#39; One way to approach this is to first remove the strings from the objects and then use astype(float). Below we use the strip() function to find  lbs. using a list comprehension. &gt; # [] indicates a list in python + # np.array() changes the list back into an array + weight = np.array([w.strip(&quot; lbs.&quot;) for w in weight]) Now we can use the astype() function to change the elements in weight from str to float. &gt; weight.astype(float) array([125., 132., 156.]) R The as.numeric() function will attempt to coerce strings to numeric type if possible. Any non-numeric values are coerced to NA. For demonstration, lets say we have the following vector. &gt; weight &lt;- c(&quot;125 lbs.&quot;, &quot;132 lbs.&quot;, &quot;156 lbs.&quot;) The as.numeric() function returns all NA due to presence of character data. &gt; as.numeric(weight) Warning: NAs introduced by coercion [1] NA NA NA There are many ways to approach this. A common approach is to first remove the characters and then use as.numeric(). Below we use the gsub() function to find lbs. and replace with nothing (find-and-replace procedures are discussed more below). &gt; weightN &lt;- gsub(&quot;lbs.&quot;, &quot;&quot;, weight) &gt; as.numeric(weightN) [1] 125 132 156 The parse_number() function in the readr package can often take care of these situations automatically. &gt; readr::parse_number(weight) [1] 125 132 156 4.8 Combine strings String concatenationturning Jane and Smith into Jane Smithis easily done in both languages. Python The + operator can combine strings in Python. &gt; species = &#39;yellow-bellied sea snake&#39; + tail_shape = &#39;paddle-shaped&#39; + + statement = &#39;The &#39; + species + &#39; has a &#39; + tail_shape + &#39; tail that helps it swim.&#39; + print(statement) The yellow-bellied sea snake has a paddle-shaped tail that helps it swim. R The paste() and paste0() functions combine strings in R. The former concatenates strings and places spaces between them; the latter concatenates sans spaces. &gt; species &lt;- &#39;rainbow boa&#39; &gt; appearance &lt;- &#39;iridescent&#39; &gt; location &lt;- &#39;Central and South America&#39; &gt; &gt; statement1 &lt;- paste(&#39;The&#39;, species, &#39;has an&#39;, appearance, &#39;sheen.&#39;) &gt; statement1 [1] &quot;The rainbow boa has an iridescent sheen.&quot; &gt; &gt; # Note that spaces must be provided explicitly when using paste0() &gt; statement2 &lt;- paste0(&#39;The &#39;, species, &#39; is found in &#39;, location) &gt; statement2 [1] &quot;The rainbow boa is found in Central and South America&quot; 4.9 Finding and replacing patterns within strings This section reviews key functions in Python and R for finding and replacing character patterns. The functions we discuss can search for fixed character patterns (e.g., Meredith Rollins to case-sensitively match that name and that name alone) or regular expression (regex) patterns (e.g., \\w+ to capture all instances of &gt;=1 word character). Note that in R, meta characters, like w (to match word characters) and d (to match digits), are escaped with two backslashes (e.g., \\\\w and \\\\d). In Python, regex patterns are generally headed by r, which allows meta characters in the regex itself to be escaped with just one \\ (e.g., r\"\\w+\"). Regex is an enormous topic, and we dont discuss it at any length here, but you can learn more about regular expressionsand how theyre implemented in different programming languagesat these resources: https://www.regular-expressions.info/; https://regexone.com/ Python The re module provides a set of functions for searching and manipulating strings. The search() function does exactly as its name suggests: It identifies matches for a fixed or regex character pattern in a string. sub() searches for and replaces character patterns (fixed or regex). The count argument in sub() allows a user to specify how many instances of the matched pattern they want to to replace; e.g., use count = 1 to replace just the first instance of a match. &gt; import re + statement = &#39;Pencils with an HB graphite grade are commonly used for writing. An HB pencil is approximately equal to a #2 pencil.&#39; + + # Search for &quot;HB&quot; using fixed and regex patterns + search_result1 = re.search(pattern = &quot;HB&quot;, string = statement) + print(search_result1) &lt;re.Match object; span=(16, 18), match=&#39;HB&#39;&gt; &gt; search_result2 = re.search(pattern = r&quot;[H,B]{2}&quot;, string = statement) + print(search_result2) + + # Replace all instances of &quot;HB&quot; &lt;re.Match object; span=(16, 18), match=&#39;HB&#39;&gt; &gt; all_replaced = re.sub(pattern = &#39;HB&#39;, repl = &#39;HB (hard black)&#39;, string = statement) + print(all_replaced) + + # Replace just the first instance of HB Pencils with an HB (hard black) graphite grade are commonly used for writing. An HB (hard black) pencil is approximately equal to a #2 pencil. &gt; one_replaced = re.sub(pattern = &#39;HB&#39;, repl = &#39;HB (hard black)&#39;, string = statement, count = 1) + print(one_replaced) + + # Search and replace using a regex pattern instead of a fixed string Pencils with an HB (hard black) graphite grade are commonly used for writing. An HB pencil is approximately equal to a #2 pencil. &gt; regex_replaced = re.sub(pattern = r&#39;(?&lt;=\\.)\\s{1}&#39;, repl = &#39;\\n&#39;, string = statement) + print(regex_replaced) Pencils with an HB graphite grade are commonly used for writing. An HB pencil is approximately equal to a #2 pencil. R The standard-issue string-search function is grep(); it returns the index of the elements in a set of one or more strings for which a pattern match was found. (grepl() acts similarly but returns a vector of TRUE/FALSE indicating whether a match was found in each string passed to the function.) The functions sub() and gsub() can be used to find and replace instances of a pattern: The former replaces just the first instance; the latter replaces all instances. The search pattern can be provided as a raw character string or as a regular expression. &gt; statements &lt;- c(&#39;Great Pencil Co. primarily sells pencils of the following grades: HB; B; and 3B.&#39;, + &#39;Great Pencil Co. has its headquarters in Maine, and Great Pencil Co. has supplied the Northeast for decades.&#39;) &gt; &gt; # Search for pattern and return indexex of elements for which match is found &gt; grep(pattern = &#39;pencil&#39;, x = statements) # When searched for case sensitively, &quot;pencil&quot; is only found in the first string [1] 1 &gt; grep(pattern = &#39;(?i)pencil&#39;, x = statements) # When searched for case insensitively, &quot;P/pencil&quot; is found in both strings [1] 1 2 &gt; &gt; # Replace the first instance of a pattern (Co. --&gt; Company) &gt; revised &lt;- sub(pattern = &#39;Co.&#39;, replacement = &#39;Company&#39;, x = statements) &gt; revised [1] &quot;Great Pencil Company primarily sells pencils of the following grades: HB; B; and 3B.&quot; [2] &quot;Great Pencil Company has its headquarters in Maine, and Great Pencil Co. has supplied the Northeast for decades.&quot; &gt; &gt; # Replace all instances of a pattern (; --&gt; ,) &gt; revised2 &lt;- gsub(pattern = &#39;;&#39;, replacement = &#39;,&#39;, x = revised) &gt; revised2 [1] &quot;Great Pencil Company primarily sells pencils of the following grades: HB, B, and 3B.&quot; [2] &quot;Great Pencil Company has its headquarters in Maine, and Great Pencil Co. has supplied the Northeast for decades.&quot; &gt; &gt; # Find and replace a pattern using regex (3B --&gt; 2B) &gt; final &lt;- sub(pattern = &#39;\\\\d{1}&#39;, replacement = &#39;2&#39;, x = revised2) &gt; final [1] &quot;Great Pencil Company primarily sells pencils of the following grades: HB, B, and 2B.&quot; [2] &quot;Great Pencil Company has its headquarters in Maine, and Great Pencil Co. has supplied the Northeast for decades.&quot; Those functions can be used to trim excess (or all) white space in character strings. &gt; spaced_string &lt;- c(&#39;This string started out with too many spaces.&#39;) &gt; # Replace all instances of &gt;=2 spaces with single spaces &gt; gsub(pattern = &#39;\\\\s{2,}&#39;, replacement = &#39; &#39;, x = spaced_string) [1] &quot;This string started out with too many spaces.&quot; &gt; # Remove all white space &gt; collapse_these &lt;- c(&#39;9:00 - 10:15&#39;, &#39;10:15 - 11:30&#39;, &#39;11:30 - 12:00&#39;) &gt; gsub(pattern = &#39;\\\\s&#39;, replacement = &#39;&#39;, x = collapse_these) [1] &quot;9:00-10:15&quot; &quot;10:15-11:30&quot; &quot;11:30-12:00&quot; The package stringi also provides an array of string-search and string-manipulation functions, including stri_detect(), stri_replace(), and stri_extract(), all of which easily handle fixed and regex search patterns. For example: &gt; library(stringi) &gt; user_dat &lt;- data.frame(name = c(&#39;Shire, Jane E&#39;, &#39;Winchester, Marcus L&#39;, &#39;Fox, Sal&#39;), id_number = c(&#39;aaa101&#39;, &#39;aaa102&#39;, &#39;aaa103&#39;)) &gt; user_dat name id_number 1 Shire, Jane E aaa101 2 Winchester, Marcus L aaa102 3 Fox, Sal aaa103 &gt; # Say we want to use regex patterns and the stringi package to eliminate the &#39;aaa&#39; patterns from &gt; # the user IDs and then add middle initials---for those users who have them---to the data frame &gt; user_dat$id_number &lt;- stri_replace(user_dat$id_number, regex = &#39;\\\\w{3}(?=\\\\d+)&#39;, replacement = &#39;&#39;) &gt; user_dat$middle_initial &lt;- stri_extract(user_dat$name, regex = &#39;\\\\b\\\\w{1}\\\\b&#39;) &gt; user_dat name id_number middle_initial 1 Shire, Jane E 101 E 2 Winchester, Marcus L 102 L 3 Fox, Sal 103 &lt;NA&gt; 4.10 Change case How to change the case of strings. The most common case transformations are lower case, upper case, and title case. Python The lower(), upper(), and title() functions convert case to lower, upper, and title, respectively. We can use a list comprehension to apply these functions to each string in a list. &gt; col_names = [col.upper() for col in mtcars.columns] + mtcars.columns = col_names R The tolower() and toupper() functions convert case to lower and upper, respectively. &gt; names(mtcars) &lt;- toupper(names(mtcars)) &gt; names(mtcars) [1] &quot;MPG&quot; &quot;CYLINDERS&quot; &quot;DISP&quot; &quot;HP&quot; &quot;AXLE_RATIO&quot; [6] &quot;WEIGHT&quot; &quot;QSEC&quot; &quot;ENGINE&quot; &quot;AM&quot; &quot;GEAR&quot; [11] &quot;CARB&quot; &quot;AM_CH&quot; &quot;AM_FAC&quot; &gt; names(mtcars) &lt;- tolower(names(mtcars)) &gt; names(mtcars) [1] &quot;mpg&quot; &quot;cylinders&quot; &quot;disp&quot; &quot;hp&quot; &quot;axle_ratio&quot; [6] &quot;weight&quot; &quot;qsec&quot; &quot;engine&quot; &quot;am&quot; &quot;gear&quot; [11] &quot;carb&quot; &quot;am_ch&quot; &quot;am_fac&quot; The stringr package provides a convenient title case conversion function, str_to_title(), which capitalizes the first letter of each string. &gt; stringr::str_to_title(names(mtcars)) [1] &quot;Mpg&quot; &quot;Cylinders&quot; &quot;Disp&quot; &quot;Hp&quot; &quot;Axle_ratio&quot; [6] &quot;Weight&quot; &quot;Qsec&quot; &quot;Engine&quot; &quot;Am&quot; &quot;Gear&quot; [11] &quot;Carb&quot; &quot;Am_ch&quot; &quot;Am_fac&quot; 4.11 Drop duplicate rows How to find and drop duplicate elements. Python The duplicated() function determines which rows of a DataFrame are duplicates of previous rows. First, we create a DataFrame with a duplicate row by using the pandas concat() function. concat() combines DataFrames by rows or columns, row by default. &gt; # create DataFrame with duplicate rows + import pandas as pd + mtcars2 = pd.concat([mtcars.iloc[0:3,0:6], mtcars.iloc[0:1,0:6]]) The duplicated() function returns a logical vector. TRUE indicates a row is a duplicate of a previous row. &gt; # create DataFrame with duplicate rows + mtcars2.duplicated() 0 False 1 False 2 False 0 True dtype: bool R The duplicated() function determines which elements of a vector or data frame are duplicates of elements with smaller subscripts. (from ?duplicated) &gt; # create data frame with duplicate rows &gt; mtcars2 &lt;- rbind(mtcars[1:3,1:6], mtcars[1,1:6]) &gt; # last row is duplicate of first &gt; mtcars2 mpg cylinders disp hp axle_ratio weight 1 21.0 6 160 110 3.90 2.620 2 21.0 6 160 110 3.90 2.875 3 22.8 4 108 93 3.85 2.320 4 21.0 6 160 110 3.90 2.620 The duplicated() function returns a logical vector. TRUE indicates a row is a duplicate of a previous row. &gt; # last row is duplicate &gt; duplicated(mtcars2) [1] FALSE FALSE FALSE TRUE The TRUE/FALSE vector can be used to extract or drop duplicate rows. Since TRUE in indexing brackets will keep a row, we can use ! to negate the logicals and keep those that are NOT TRUE &gt; # drop the duplicate and update the data frame &gt; mtcars3 &lt;- mtcars2[!duplicated(mtcars2),] &gt; mtcars3 mpg cylinders disp hp axle_ratio weight 1 21.0 6 160 110 3.90 2.620 2 21.0 6 160 110 3.90 2.875 3 22.8 4 108 93 3.85 2.320 &gt; # extract and investigate the duplicate row &gt; mtcars2[duplicated(mtcars2),] mpg cylinders disp hp axle_ratio weight 4 21 6 160 110 3.9 2.62 The anyDuplicated() function returns the row number of duplicate rows. &gt; anyDuplicated(mtcars2) [1] 4 4.12 Format dates With formatted dates we can calculate elapsed time, extract components of a date, properly order names of months, and more. Python The Python module datetime can be used to create various date and time objects. Here we will discuss 4 of the main classes within datetime that are most useful. The first class we will go over is the date() class. This creates a date object whose only attributes are year, month, day. Here we create a date object using the date class. The attributes are specified as integers in the argument of date() in this order date(year, month, day). &gt; import datetime as dt + + x = dt.date(2001, 4, 12) + print(x) 2001-04-12 To get todays date, we can use the date.today() function: &gt; today = dt.date.today() + print(today) 2022-03-24 Note that the output of both x and today are only year-month-day because they are date objects. We can extract each of these attributes (year, month, day) from the date object as follows: &gt; today.year 2022 &gt; today.month 3 &gt; today.day 24 Next we will discuss the time() class. This class creates time objects containing information about only a time. The attributes that go into the time() class are hours, minutes, seconds in that order. Like the date class, these attributes must be inputted as integers. &gt; y = dt.time(11, 34, 56) + print(y) 11:34:56 If you want a time object containing only hours and minutes, only seconds, etc. you can specify the attributes by name when creating the time object. &gt; only_hrs = dt.time(hour = 10) + only_mins = dt.time(minute = 55) + + print(only_hrs) 10:00:00 &gt; print(only_mins) 00:55:00 Again similar to the date class, we can extract hour, minute, and second attributes from the time objects: &gt; y.hour 11 &gt; y.minute 34 &gt; y.second 56 &gt; y.microsecond 0 Now we will talk about the datetime() class that creates a datetime object containing information about both date and time. The attributes must be inputted as integers and are year, month, day, hour, minute, second, in that order. Like the date and time classes, we can speficy specific attributes in the argument using the attrubute names as well. If we dont specify any time components, the datetime object defaults to time 00:00:00. &gt; # Input attributes in order + z = dt.datetime(1981, 4, 12, 11, 34, 56) + print(z) + + # Input attributes using attribute names (any order) 1981-04-12 11:34:56 &gt; z2= dt.datetime(year = 2021, day = 6, month = 12, hour = 6) + print(z2) + + # No time attributes 2021-12-06 06:00:00 &gt; z3 = dt.datetime(1981, 4, 12) + print(z3) 1981-04-12 00:00:00 Again, we can extract attributes in exactly the same way as the date and time classes. &gt; z.year 1981 &gt; z.day 12 &gt; z.hour 11 The final class we will discuss is the timedelta class. This class is used to store date/time differences between date objects. The default settings for a timedelta object are as follows: timedelta(weeks=0, days=0, hours=0,minutes=0, seconds=0, milliseconds=0, microseconds=0) Here is an example of howto add and subtract datesa and times using these objects. &gt; # Create a datetime object for the current time that we will increment + d1 = dt.datetime.now() + print(d1) + + # Add 550 days to our datetime object 2022-03-24 01:15:23.025741 &gt; d2 = d1 + dt.timedelta(days = 550) + print(d2) + + # Subtract 5 hours from our datetime object 2023-09-25 01:15:23.025741 &gt; d3 = d1 - dt.timedelta(hours = 5) + print(d3) 2022-03-23 20:15:23.025741 Finally, we will discuss how to convert strings to datetime objects and vice versa. The attribute strftime() converts datetime objects to strings. In the argument of strftime() can specify the format you would like. &gt; d1 datetime.datetime(2022, 3, 24, 1, 15, 23, 25741) &gt; d1.strftime(&quot;%A %m %Y&quot;) &#39;Thursday 03 2022&#39; &gt; d1.strftime(&quot;%a %m %y&quot;) &#39;Thu 03 22&#39; The attribute strptime() converts strings into datetime objects. In the argument of strptime() you must specify the string and then the format of the string. &gt; d4 = &quot;27/10/98 11:03:9.033&quot; + + d1.strptime(d4, &quot;%d/%m/%y %H:%M:%S.%f&quot;) datetime.datetime(1998, 10, 27, 11, 3, 9, 33000) R Dates in R can be stored as a Date class or a Date-Time class. Dates are stored as the number of days since January 1, 1970. Date-Times are stored as the number of seconds since January 1, 1970. With dates stored in this manner we can calculate elapsed time in units such as days, weeks, hours, minutes, and so forth. Below are the dates of the first five NASA Columbia Space Shuttle flights entered as a character vector. &gt; date &lt;- c(&quot;12 April 1981&quot;, + &quot;12 November 1981&quot;, + &quot;22 March 1982&quot;, + &quot;27 June 1982&quot;, + &quot;11 November 1982&quot;) R does not immediately recognize these as a Date class. To format as a Date class, we can either use the base R as.Date() function or one of the convenience functions in the lubridate package. The as.Date() function requires a specified POSIX conversion specification as documented in ?strptime. Below the conversion code %d %B %Y says Date is entered as two digit day of month (%d), full month name (%B), and year with century (%Y). &gt; date1 &lt;- as.Date(date, format = &quot;%d %B %Y&quot;) &gt; date1 [1] &quot;1981-04-12&quot; &quot;1981-11-12&quot; &quot;1982-03-22&quot; &quot;1982-06-27&quot; &quot;1982-11-11&quot; The dates now print in year-month-date format, however they are stored internally as number of days since January 1, 1970. This can be seen by using as.numeric() on the date1 vector. &gt; as.numeric(date1) [1] 4119 4333 4463 4560 4697 The lubridate package provides a series of functions that are permutations of the letters m, d, and y to represent the order of date components. To format the original date vector, we use the dmy() function since the date components are ordered as day, month and year. Notice we must load the lubridate package to use this function. &gt; library(lubridate) &gt; date2 &lt;- dmy(date) &gt; date2 [1] &quot;1981-04-12&quot; &quot;1981-11-12&quot; &quot;1982-03-22&quot; &quot;1982-06-27&quot; &quot;1982-11-11&quot; When dates are formatted we can easily extract information such as day of week or month. For example to extract the day of week of the launches as an ordered factor, we can use the lubridate function wday() with label=TRUE and abbr = FALSE. &gt; wday(date2, label = TRUE, abbr = FALSE) [1] Sunday Thursday Monday Sunday Thursday 7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday To calculate elapsed time between launches in days we can use the base R diff() function. &gt; diff(date2) Time differences in days [1] 214 130 97 137 To store a date as a Date-Time class we need to include a time component. Below are the first five Columbia launch dates with times. UTC refers to Universal Coordinated Time. &gt; datetime &lt;- c(&quot;12 April 1981 12:00:04 UTC&quot;, + &quot;12 November 1981 15:10:00 UTC&quot;, + &quot;22 March 1982 16:00:00 UTC&quot;, + &quot;27 June 1982 15:00:00 UTC&quot;, + &quot;11 November 1982 12:19:00 UTC&quot;) To format as a Date-Time class we can use either the base R as.POSIXct() function or one of the convenience functions in the lubridate package. To use as.POSIXct() we need to include additional POSIX conversion specifications for the hour, minute and second of launch. The %H:%M:%S specification refers to hours, minutes and seconds. The tz argument specifies the time zone of the times. &gt; datetime1 &lt;- as.POSIXct(datetime, + format = &quot;%d %B %Y %H:%M:%S&quot;, + tz = &quot;UTC&quot;) &gt; datetime1 [1] &quot;1981-04-12 12:00:04 UTC&quot; &quot;1981-11-12 15:10:00 UTC&quot; [3] &quot;1982-03-22 16:00:00 UTC&quot; &quot;1982-06-27 15:00:00 UTC&quot; [5] &quot;1982-11-11 12:19:00 UTC&quot; When we use as.numeric() on the datetime1 vector we see it is stored as number of seconds since January 1, 1970. &gt; as.numeric(datetime1) [1] 355924804 374425800 385660800 394038000 405865140 Using lubridate we can append _hms() to any of the mdy functions to format dates with time components as a Date-Time class. Notice the default time zone in lubridate is UTC. &gt; datetime2 &lt;- dmy_hms(datetime) &gt; datetime2 [1] &quot;1981-04-12 12:00:04 UTC&quot; &quot;1981-11-12 15:10:00 UTC&quot; [3] &quot;1982-03-22 16:00:00 UTC&quot; &quot;1982-06-27 15:00:00 UTC&quot; [5] &quot;1982-11-11 12:19:00 UTC&quot; To calculate elapsed time between launches in hours, we can use the lubridate function time_length() with the unit set to hours. Below we use diff() and then pipe to time_length(). &gt; diff(datetime2) |&gt; time_length(unit = &quot;hours&quot;) [1] 5139.166 3120.833 2327.000 3285.317 For more information on working with dates and times in R, see the vignette accompanying the lubridate package. 4.13 Randomly sample rows How to take a random sample of rows from a data frame. The sample is usually either a fixed size or a proportion. Python The pandas package provide a function for taking a sample of fixed size or a proportion. To sample with replacement, set replace = TRUE. Additionally, the random sample will change every time the code is run. To always generate the same random sample, set random_state to any positive integer. To create a sample with a fixed number of rows, use the n argument. &gt; # sample 5 rows from mtcars + mtcars.sample(n=5, replace=True) MPG CYL DISP HP AXLE_RATIO ... VS AM GEAR CARB KPL 15 10.4 8 460.0 215 3.00 ... 0 0 3 4 2.210884 31 21.4 4 121.0 109 4.11 ... 1 1 4 2 4.549320 18 30.4 4 75.7 52 4.93 ... 1 1 4 2 6.462585 26 26.0 4 120.3 91 4.43 ... 0 1 5 2 5.527211 11 16.4 8 275.8 180 3.07 ... 0 0 3 3 3.486395 [5 rows x 12 columns] To create a sample of a proportion, use the frac argument. &gt; # sample 20% of rows from mtcars + mtcars.sample(frac = 0.20, random_state=1) MPG CYL DISP HP AXLE_RATIO ... VS AM GEAR CARB KPL 27 30.4 4 95.1 113 3.77 ... 1 1 5 2 6.462585 3 21.4 6 258.0 110 3.08 ... 1 0 3 1 4.549320 22 15.2 8 304.0 150 3.15 ... 0 0 3 2 3.231293 18 30.4 4 75.7 52 4.93 ... 1 1 4 2 6.462585 23 13.3 8 350.0 245 3.73 ... 0 0 3 4 2.827381 17 32.4 4 78.7 66 4.08 ... 1 1 4 1 6.887755 [6 rows x 12 columns] The numpy function random.choice() in combination with the loc() function can be used to sample from a DataFrame. The random.choice() function creates a random sample according to the given parameters. The loc() function is used to access rows and columns by index. &gt; # import the numpy package + import numpy as np + + # create a random sample of size 5 with replacement + random_sample = np.random.choice(len(mtcars), (5,), replace=True) + + # use random_sample to sample from mtcars + mtcars.loc[random_sample,] MPG CYL DISP HP AXLE_RATIO ... VS AM GEAR CARB KPL 26 26.0 4 120.3 91 4.43 ... 0 1 5 2 5.527211 8 22.8 4 140.8 95 3.92 ... 1 0 4 2 4.846939 21 15.5 8 318.0 150 2.76 ... 0 0 3 2 3.295068 15 10.4 8 460.0 215 3.00 ... 0 0 3 4 2.210884 21 15.5 8 318.0 150 2.76 ... 0 0 3 2 3.295068 [5 rows x 12 columns] The random sample will change every time the code is run. To always generate the same random sample, use the random.seed() function with any positive integer. &gt; # setting seed to always get same random sample + np.random.seed(123) + + # create a random sample of size 5 with replacement + sample = np.random.choice(len(mtcars), (5,), replace=True) + mtcars.loc[sample,] MPG CYL DISP HP AXLE_RATIO WT QSEC VS AM GEAR CARB KPL 30 15.0 8 301.0 335 3.54 3.57 14.60 0 1 5 8 3.188776 13 15.2 8 275.8 180 3.07 3.78 18.00 0 0 3 3 3.231293 30 15.0 8 301.0 335 3.54 3.57 14.60 0 1 5 8 3.188776 2 22.8 4 108.0 93 3.85 2.32 18.61 1 1 4 1 4.846939 28 15.8 8 351.0 264 4.22 3.17 14.50 0 1 5 4 3.358844 R There are many ways to sample rows from a data frame in R. The dplyr package provides a convenience function, slice_sample(), for taking either a fixed sample size or a proportion. &gt; # sample 5 rows from mtcars &gt; dplyr::slice_sample(mtcars, n = 5) mpg cylinders disp hp axle_ratio weight qsec engine am gear carb am_ch 1 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 1 2 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 0 3 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 0 4 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 0 5 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 1 am_fac 1 manual 2 automatic 3 automatic 4 automatic 5 manual &gt; &gt; # sample 20% of rows from mtcars &gt; dplyr::slice_sample(mtcars, prop = 0.20) mpg cylinders disp hp axle_ratio weight qsec engine am gear carb am_ch 1 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 0 2 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 0 3 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 1 4 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 0 5 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 1 6 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 1 am_fac 1 automatic 2 automatic 3 manual 4 automatic 5 manual 6 manual To sample with replacement, set replace = TRUE. The base R functions sample() and runif() can be combined to sample fixed sizes or approximate proportions. &gt; # sample 5 rows from mtcars &gt; # get random row numbers &gt; i &lt;- sample(nrow(mtcars), size = 5) &gt; # use i to select rows &gt; mtcars[i,] mpg cylinders disp hp axle_ratio weight qsec engine am gear carb am_ch 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 1 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 0 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 1 20 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 1 13 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 0 am_fac 3 manual 8 automatic 18 manual 20 manual 13 automatic &gt; # sample about 20% of rows from mtcars &gt; # generate random values on range of [0,1] &gt; i &lt;- runif(nrow(mtcars)) &gt; # use i &lt; 0.20 logical vector to &gt; # select rows that correspond to TRUE &gt; mtcars[i &lt; 0.20,] mpg cylinders disp hp axle_ratio weight qsec engine am gear carb am_ch 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 1 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 0 8 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 0 15 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 0 18 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 1 21 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 0 22 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 0 28 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 1 am_fac 3 manual 4 automatic 8 automatic 15 automatic 18 manual 21 automatic 22 automatic 28 manual The random sample will change every time the code is run. To always generate the same random sample, use the set.seed() function with any positive integer. &gt; # always get the same random sample &gt; set.seed(123) &gt; i &lt;- runif(nrow(mtcars)) &gt; mtcars[i &lt; 0.20,] mpg cylinders disp hp axle_ratio weight qsec engine am gear carb am_ch 6 18.1 6 225.0 105 2.76 3.46 20.22 1 0 3 1 0 15 10.4 8 472.0 205 2.93 5.25 17.98 0 0 3 4 0 18 32.4 4 78.7 66 4.08 2.20 19.47 1 1 4 1 1 30 19.7 6 145.0 175 3.62 2.77 15.50 0 1 5 6 1 am_fac 6 automatic 15 automatic 18 manual 30 manual "],["combine-reshape-and-merge.html", "Chapter 5 Combine, Reshape and Merge 5.1 Combine rows 5.2 Combine columns 5.3 Reshaping data 5.4 Merge/Join", " Chapter 5 Combine, Reshape and Merge This chapter looks at various strategies for combining, reshaping, and merging data. 5.1 Combine rows Combining rows may be thought of as stacking rectangular data structures. Python The pandas function concat function binds rows. It takes a list of pandas DataFrame objects. The second argument axis specifies a row bind when 0 and a column bind when 1. The default value is 0. The column names of the DataFrames should match, otherwise the DataFrame fills with NaNs. You can bind rows with different column types. &gt; import pandas as pd + + d1 = pd.DataFrame({&#39;x&#39;:[4,5,6], &#39;y&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]}) + d2 = pd.DataFrame({&#39;x&#39;:[3,2,1], &#39;y&#39;:[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]}) + + # create list of DataFrame objects + frames = [d1, d2] + combined_df = pd.concat(frames) + + combined_df x y 0 4 a 1 5 b 2 6 c 0 3 d 1 2 e 2 1 f The following code is an example of when column names do not match, resulting in NaNs in the DataFrame. &gt; + # DataFrame with different column names + d1 = pd.DataFrame({&#39;x&#39;:[4,5,6], &#39;z&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]}) + d2 = pd.DataFrame({&#39;x&#39;:[3,2,1], &#39;y&#39;:[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]}) + + # create list of DataFrame objects + frames = [d1, d2] + combined_df = pd.concat(frames) + + combined_df x z y 0 4 a NaN 1 5 b NaN 2 6 c NaN 0 3 NaN d 1 2 NaN e 2 1 NaN f R The rbind() function binds rows. It takes two or more objects. To row bind data frames, the column names must match, otherwise an error is returned. If columns being stacked have differing variable types, the values will be coerced according to logical &lt; integer &lt; double &lt; complex &lt; character. (E.g., if you stack a set of rows with type logical in column J on a set of rows with type character in column J, the output will have column J as type character.) &gt; d1 &lt;- data.frame(x = 4:6, y = letters[1:3]) &gt; d2 &lt;- data.frame(x = 3:1, y = letters[4:6]) &gt; rbind(d1, d2) x y 1 4 a 2 5 b 3 6 c 4 3 d 5 2 e 6 1 f See also the bind_rows() function in the dplyr package. 5.2 Combine columns Combining columns may be thought of as setting rectangular data structures next to each other. Python The concat function also binds columns. It takes two or more objects. The second argument axis specifies a row bind when 0 and a column bind when 1. The default value is 0. To column bind data frames, the number of rows must match; otherwise, the function throws an error. &gt; + d1 = pd.DataFrame({&#39;x&#39;:[4,5,6], &#39;y&#39;:[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;]}) + d2 = pd.DataFrame({&#39;z&#39;:[3,2,1], &#39;a&#39;:[&#39;d&#39;,&#39;e&#39;,&#39;f&#39;]}) + + # create list of DataFrame objects + frames = [d1, d2] + combined_df = pd.concat(frames, axis=1) + + combined_df x y z a 0 4 a 3 d 1 5 b 2 e 2 6 c 1 f R The cbind() function binds columns. It takes two or more objects. To column bind data frames, the number of rows must match; otherwise, the object with fewer rows will have rows recycled (if possible) or an error will be returned. &gt; d1 &lt;- data.frame(x = 10:13, y = letters[1:4]) &gt; d2 &lt;- data.frame(x = c(23,34,45,44)) &gt; cbind(d1, d2) x y x 1 10 a 23 2 11 b 34 3 12 c 45 4 13 d 44 &gt; # example of recycled rows (d1 is repeated twice) &gt; d1 &lt;- data.frame(x = 10:13, y = letters[1:4]) &gt; d2 &lt;- data.frame(x = c(23,34,45,44,99,99,99,99)) &gt; cbind(d1, d2) x y x 1 10 a 23 2 11 b 34 3 12 c 45 4 13 d 44 5 10 a 99 6 11 b 99 7 12 c 99 8 13 d 99 See also the bind_cols() function in the dplyr package. 5.3 Reshaping data The next two sections discuss how to reshape data from wide to long and from long to wide. Wide data are structured such that multiple values associated with a given unit (e.g., a person, a cell culture, etc.) are placed in the same row: name time_1_score time_2_score 1 larry 3 0 2 moe 6 3 3 curly 2 1 Long data, conversely, are structured such that all values are contained in one column, with another column identifying what value is given in any particular row (time 1, time 2, etc.): id time score 1 larry 1 3 2 larry 2 0 3 moe 1 6 4 moe 2 3 5 curly 1 2 6 curly 2 1 Shifting between these two data formats is often necessary for implementing certain statistical techniques or representing data with particular visualizations. 5.3.1 Wide to long Python To reshape a DataFrame from wide to long, we can use the pandas melt() function. The following is an example of a wide DataFrame. &gt; import numpy as np + import pandas as pd + + data = {&quot;sex&quot;: np.random.choice([&#39;i&#39;, &#39;f&#39;, &#39;m&#39;], 3), + &quot;wk1&quot;: np.random.choice(range(20), 3), + &quot;wk2&quot;: np.random.choice(range(20), 3), + &quot;wk3&quot;: np.random.choice(range(20), 3)} + + df_wide = pd.DataFrame(data) The following code uses the pandas melt() function to reshape the DataFrame from wide to long. &gt; pd.melt(df_wide, + id_vars = [&quot;sex&quot;], + value_vars = [&quot;wk1&quot;, &quot;wk2&quot;, &quot;wk3&quot;], + value_name = &quot;observations&quot;) sex variable observations 0 m wk1 19 1 m wk1 10 2 f wk1 1 3 m wk2 0 4 m wk2 17 5 f wk2 15 6 m wk3 9 7 m wk3 0 8 f wk3 14 R In base R, the reshape() function can take data from wide to long or long to wide. The tidyverse also provides reshaping functions: pivot_longer() and pivot_wider(). The tidyverse functions have a degree of intuitiveness and usability that may make them the go-to reshaping tools for many R users. We give examples below using both base R and tidyverse. Say we begin with a wide data frame, df_wide, that looks like this: id sex wk1 wk2 wk3 1 1 m 16 7 15 2 2 m 12 19 10 3 3 f 8 15 7 To lengthen a data frame using reshape(), a user provides arguments specifying the columns that identify values origins (person, cell culture, etc.), the columns containing values to be lengthened, and the desired names for new columns in long data: &gt; df_long &lt;- reshape(df_wide, + direction = &#39;long&#39;, + idvar = c(&#39;id&#39;, &#39;sex&#39;), # column(s) that uniquely identifies/y each row + varying = c(&#39;wk1&#39;, &#39;wk2&#39;, &#39;wk3&#39;), # variables that contain the values to be lengthened + v.names = &#39;val&#39;, # desired name of column in long data that will contain values + timevar = &#39;week&#39;) # desired name of column in long data that will identify each value&#39;s context &gt; df_long id sex week val 1.m.1 1 m 1 16 2.m.1 2 m 1 12 3.f.1 3 f 1 8 1.m.2 1 m 2 7 2.m.2 2 m 2 19 3.f.2 3 f 2 15 1.m.3 1 m 3 15 2.m.3 2 m 3 10 3.f.3 3 f 3 7 The tidyverse function for taking data from wide to long is pivot_longer(). To lengthen df_wide using pivot_longer(), a user would write: &gt; library(tidyverse) &gt; df_long_PL &lt;- pivot_longer(df_wide, + cols = -c(&#39;id&#39;, &#39;sex&#39;), # columns that contain the values to be lengthened (can use -c() to negate variables) + names_to = &#39;week&#39;, # desired name of column in long data that will identify each value&#39;s context + values_to = &#39;val&#39;) # desired name of column in long data that will contain values &gt; df_long_PL # A tibble: 9 x 4 id sex week val &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; 1 1 m wk1 16 2 1 m wk2 7 3 1 m wk3 15 4 2 m wk1 12 5 2 m wk2 19 6 2 m wk3 10 7 3 f wk1 8 8 3 f wk2 15 9 3 f wk3 7 pivot_longer() is particularly useful (a) when dealing with wide data that contain multiple sets of repeated measures in each row that need to be lengthened separately (e.g., two monthly height measurements and two monthly weight measurements for each person) and (b) when column names and/or column values in the long data need to be extracted from column names of the wide data using regular expressions. For example, say we begin with a wide data frame, animals_wide, in which every row contains two values for each of two different measures: animal lives_in_water jan_playfulness feb_playfulness jan_excitement 1 dolphin TRUE 6.0 5.5 7.0 2 porcupine FALSE 3.5 4.5 3.5 3 capybara FALSE 4.0 5.0 4.0 feb_excitement 1 7.0 2 3.5 3 4.0 pivot_longer() can be used to convert this data frame to a long format where there is one column for each of the measures, playfulness and excitement: &gt; animals_long_1 &lt;- pivot_longer(animals_wide, + cols = -c(&#39;animal&#39;, &#39;lives_in_water&#39;), + names_to = c(&#39;month&#39;, &#39;.value&#39;), # &quot;.value&quot; is placeholder for strings that will be extracted from wide column names + names_pattern = &#39;(.+)_(.+)&#39;) # specify structure of wide column names with regex from which long column names will be extracted &gt; animals_long_1 # A tibble: 6 x 5 animal lives_in_water month playfulness excitement &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 dolphin TRUE jan 6 7 2 dolphin TRUE feb 5.5 7 3 porcupine FALSE jan 3.5 3.5 4 porcupine FALSE feb 4.5 3.5 5 capybara FALSE jan 4 4 6 capybara FALSE feb 5 4 Alternatively, pivot_longer() can be used to convert this data frame to a long format where there is one column containing all the playfulness and excitement values: &gt; animals_long_2 &lt;- pivot_longer(animals_wide, + cols = -c(&#39;animal&#39;, &#39;lives_in_water&#39;), + names_to = c(&#39;month&#39;, &#39;measure&#39;), + names_pattern = &#39;(.+)_(.+)&#39;, + values_to = &#39;val&#39;) &gt; animals_long_2 # A tibble: 12 x 5 animal lives_in_water month measure val &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 dolphin TRUE jan playfulness 6 2 dolphin TRUE feb playfulness 5.5 3 dolphin TRUE jan excitement 7 4 dolphin TRUE feb excitement 7 5 porcupine FALSE jan playfulness 3.5 6 porcupine FALSE feb playfulness 4.5 7 porcupine FALSE jan excitement 3.5 8 porcupine FALSE feb excitement 3.5 9 capybara FALSE jan playfulness 4 10 capybara FALSE feb playfulness 5 11 capybara FALSE jan excitement 4 12 capybara FALSE feb excitement 4 5.3.2 Long to wide Python To reshape a DataFrame from long to wide, we can use the pandas pivot() and pivot_table() functions. The following is an example of a long DataFrame. &gt; import numpy as np + import pandas as pd + + data = {&quot;sex&quot;: np.random.choice([&#39;i&#39;, &#39;f&#39;, &#39;m&#39;], 9), + &quot;week&quot;: np.random.choice(range(3), 9), + &quot;observations&quot;: np.random.choice(range(20), 9)} + + df_long = pd.DataFrame(data) The following code uses the pandas pivot_table() function to reshape the DataFrame from long to wide. &gt; df_wide = pd.pivot_table(df_long, + index=&#39;sex&#39;, + columns=&#39;week&#39;, + values=&#39;observations&#39;) + + df_wide week 0 1 2 sex f 2.333333 NaN NaN i 7.500000 9.0 6.0 m NaN NaN 2.0 R Say we begin with a long data frame, df_long, that looks like this: &gt; df_long id sex week val 1.m.1 1 m 1 16 2.m.1 2 m 1 12 3.f.1 3 f 1 8 1.m.2 1 m 2 7 2.m.2 2 m 2 19 3.f.2 3 f 2 15 1.m.3 1 m 3 15 2.m.3 2 m 3 10 3.f.3 3 f 3 7 To take data from long to wide with base Rs reshape(), a user would write: &gt; df_wide &lt;- reshape(df_long, + direction = &#39;wide&#39;, + idvar = c(&#39;id&#39;, &#39;sex&#39;), # column(s) that determine which rows should be grouped together in the wide data + v.names = &#39;val&#39;, # column containing values to widen + timevar = &#39;week&#39;, # column from which resulting wide column names are pulled + sep = &#39;_&#39;) # the `sep` argument allows a user to specify how the contents of `timevar` should be joined with the name of the `v.names` variable to form wide column names &gt; df_wide id sex val_1 val_2 val_3 1.m.1 1 m 16 7 15 2.m.1 2 m 12 19 10 3.f.1 3 f 8 15 7 The tidyverse function for taking data from long to wide is pivot_wider(). To widen df_long using pivot_longer(), a user would write: &gt; library(tidyverse) &gt; df_wide_PW &lt;- pivot_wider(df_long, + id_cols = c(&#39;id&#39;, &#39;sex&#39;), + values_from = &#39;val&#39;, + names_from = &#39;week&#39;, + names_prefix = &#39;week_&#39;) # `names_prefix` specifies a string to paste in front of the contents of &#39;week&#39; in the resulting wide column names &gt; df_wide_PW # A tibble: 3 x 5 id sex week_1 week_2 week_3 &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; 1 1 m 16 7 15 2 2 m 12 19 10 3 3 f 8 15 7 pivot_wider() offers a lot of usability when widening relatively complicated long data structures. For example, say we want to widen both of the long versions of the animals data frame created above. To widen the version of the long data that has a column for each of the measures (playfulness and excitement): &gt; animals_long_1 # A tibble: 6 x 5 animal lives_in_water month playfulness excitement &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 dolphin TRUE jan 6 7 2 dolphin TRUE feb 5.5 7 3 porcupine FALSE jan 3.5 3.5 4 porcupine FALSE feb 4.5 3.5 5 capybara FALSE jan 4 4 6 capybara FALSE feb 5 4 &gt; animals_wide &lt;- pivot_wider(animals_long_1, + id_cols = c(&#39;animal&#39;, + &#39;lives_in_water&#39;), + values_from = c(&#39;playfulness&#39;, + &#39;excitement&#39;), + names_from = &#39;month&#39;, + names_glue = &#39;{month}_{.value}&#39;) &gt; # `names_glue` allows for customization &gt; # of column names using &quot;glue&quot;; &gt; # see https://glue.tidyverse.org/ &gt; animals_wide # A tibble: 3 x 6 animal lives_in_water jan_playfulness feb_playfulness jan_excitement &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 dolphin TRUE 6 5.5 7 2 porcupine FALSE 3.5 4.5 3.5 3 capybara FALSE 4 5 4 # ... with 1 more variable: feb_excitement &lt;dbl&gt; To widen the version of the long data that has one column containing all the values of playfulness and excitement together: &gt; animals_long_2 # A tibble: 12 x 5 animal lives_in_water month measure val &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 dolphin TRUE jan playfulness 6 2 dolphin TRUE feb playfulness 5.5 3 dolphin TRUE jan excitement 7 4 dolphin TRUE feb excitement 7 5 porcupine FALSE jan playfulness 3.5 6 porcupine FALSE feb playfulness 4.5 7 porcupine FALSE jan excitement 3.5 8 porcupine FALSE feb excitement 3.5 9 capybara FALSE jan playfulness 4 10 capybara FALSE feb playfulness 5 11 capybara FALSE jan excitement 4 12 capybara FALSE feb excitement 4 &gt; animals_wide &lt;- pivot_wider(animals_long_2, + id_cols = c(&#39;animal&#39;, &#39;lives_in_water&#39;), + values_from = &#39;val&#39;, + names_from = c(&#39;month&#39;, &#39;measure&#39;), + names_sep = &#39;_&#39;) &gt; animals_wide # A tibble: 3 x 6 animal lives_in_water jan_playfulness feb_playfulness jan_excitement &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 dolphin TRUE 6 5.5 7 2 porcupine FALSE 3.5 4.5 3.5 3 capybara FALSE 4 5 4 # ... with 1 more variable: feb_excitement &lt;dbl&gt; 5.4 Merge/Join The merge/join examples below all make use of the following sample data frames: &gt; x merge_var val_x 1 a 12 2 b 94 3 c 92 &gt; y merge_var val_y 1 c 78 2 d 32 3 e 30 5.4.1 Left Join A left join of x and y keeps all rows of x and merges rows of y into x where possible based on the merge criterion: Python &gt; import pandas as pd + pd.merge(x, y, how = &#39;left&#39;) merge_var val_x val_y 0 a 12.0 NaN 1 b 94.0 NaN 2 c 92.0 78.0 R &gt; # all.x = T results in a left join &gt; merge(x, y, by = &#39;merge_var&#39;, all.x = T) merge_var val_x val_y 1 a 12 NA 2 b 94 NA 3 c 92 78 5.4.2 Right Join A right join of x and y keeps all rows of y and merges rows of x into y wherever possible based on the merge criterion: Python &gt; import pandas as pd + pd.merge(x, y, how = &#39;right&#39;) merge_var val_x val_y 0 c 92.0 78.0 1 d NaN 32.0 2 e NaN 30.0 R &gt; # all.y = T results in a right join &gt; merge(x, y, by = &#39;merge_var&#39;, all.y = T) merge_var val_x val_y 1 c 92 78 2 d NA 32 3 e NA 30 5.4.3 Inner Join An inner join of x and y returns merged rows for which a match can be found on the merge criterion in both tables: Python &gt; import pandas as pd + pd.merge(x, y, how = &#39;inner&#39;) merge_var val_x val_y 0 c 92.0 78.0 R &gt; # with its default arguments, merge() executes an inner join &gt; # (more specifically, a natural join, which is a kind of &gt; # inner join in which the merge-criterion column is not &gt; # repeated, despite being initially present in both tables) &gt; merge(x, y, by = &#39;merge_var&#39;) merge_var val_x val_y 1 c 92 78 5.4.4 Outer Join An outer join of x and y keeps all rows from both tables, merging rows wherever possible based on the merge criterion: Python &gt; import pandas as pd + pd.merge(x, y, how = &#39;outer&#39;) merge_var val_x val_y 0 a 12.0 NaN 1 b 94.0 NaN 2 c 92.0 78.0 3 d NaN 32.0 4 e NaN 30.0 R &gt; # all = T (or all.x = T AND all.y = T) results in an outer join &gt; merge(x, y, by = &#39;merge_var&#39;, all = T) merge_var val_x val_y 1 a 12 NA 2 b 94 NA 3 c 92 78 4 d NA 32 5 e NA 30 "],["aggregation-and-group-operations.html", "Chapter 6 Aggregation and Group Operations 6.1 Cross tabulation 6.2 Group summaries 6.3 Centering and Scaling", " Chapter 6 Aggregation and Group Operations This chapter looks at manipulating and summarizing data by groups. 6.1 Cross tabulation Cross tabulation is the process of determining frequencies per group (or determining values based on frequencies, like proportions), with groups delineated by one or more variables (e.g., nationality and sex). The Python and R examples of cross tabulation below both make use of the following dataset, dat: &gt; dat nationality sex 1 Canadian m 2 French f 3 French f 4 Egyptian m 5 Canadian f Python The pandas package contains a crosstab() function for cross tabulation with two or more variables. Alternatively, the groupby() function, also in pandas, facilitates cross tabulation by one or more variables when used in combination with count(). &gt; import pandas as pd + pd.crosstab(dat.nationality, dat.sex) sex f m nationality Canadian 1 1 Egyptian 0 1 French 2 0 &gt; dat.groupby(by = &#39;nationality&#39;).nationality.count() nationality Canadian 2 Egyptian 1 French 2 Name: nationality, dtype: int64 &gt; dat.groupby(by = [&#39;nationality&#39;, &#39;sex&#39;]).nationality.count() + # Or: dat.groupby(by = [&#39;nationality&#39;, &#39;sex&#39;]).sex.count() nationality sex Canadian f 1 m 1 Egyptian m 1 French f 2 Name: nationality, dtype: int64 R The table() function performs cross tabulation in R. A user can enter a single grouping variable or enter multiple grouping variables separated by a comma(s). The xtabs() function also computes cross-tabs; a user enters the variables to be used for grouping in formula notation. &gt; table(dat$nationality) Canadian Egyptian French 2 1 2 &gt; table(dat$nationality, dat$sex) f m Canadian 1 1 Egyptian 0 1 French 2 0 &gt; xtabs(formula = ~nationality + sex, data = dat) sex nationality f m Canadian 1 1 Egyptian 0 1 French 2 0 6.2 Group summaries Computing statistical summaries per group. Python The groupby() function from Pandas splits up a data set based on one or more grouping variables. Summarizing functionslike mean(), sum(), and so oncan then be applied to those groups. In the first example below, we use groupby() to group rows of the mtcars dataset by the number of cylinders each car has; from there, we select just the mpg column and call mean(), thus producing the average miles per gallon within each cylinder group. In the second example, we again group observations by cyl, but instead of then selecting just the mpg column, we directly call mean(); this gives the mean for each variable in the data set within each cylinder group. Finally, in the third example, we group by two variablescyl and vsand then use the describe() function to generate a set of descriptive statistics for mpg within each cylinder*vs group (e.g., mean, SD, minimum, etc.). &gt; import pandas as pd + + mean_mpg_by_cyl = mtcars.groupby(by = &#39;cyl&#39;)[&#39;mpg&#39;].mean() + print(mean_mpg_by_cyl) cyl 4.0 26.663636 6.0 19.742857 8.0 15.100000 Name: mpg, dtype: float64 &gt; means_all_vars = mtcars.groupby(by = &#39;cyl&#39;).mean() + print(means_all_vars) mpg disp hp ... am gear carb cyl ... 4.0 26.663636 105.136364 82.636364 ... 0.727273 4.090909 1.545455 6.0 19.742857 183.314286 122.285714 ... 0.428571 3.857143 3.428571 8.0 15.100000 353.100000 209.214286 ... 0.142857 3.285714 3.500000 [3 rows x 10 columns] &gt; mpg_by_cyl_vs = mtcars.groupby(by = [&#39;cyl&#39;, &#39;vs&#39;])[&#39;mpg&#39;].describe() + print(mpg_by_cyl_vs) count mean std min 25% 50% 75% max cyl vs 4.0 0.0 1.0 26.000000 NaN 26.0 26.000 26.00 26.00 26.0 1.0 10.0 26.730000 4.748111 21.4 22.800 25.85 30.40 33.9 6.0 0.0 3.0 20.566667 0.750555 19.7 20.350 21.00 21.00 21.0 1.0 4.0 19.125000 1.631717 17.8 18.025 18.65 19.75 21.4 8.0 0.0 14.0 15.100000 2.560048 10.4 14.400 15.20 16.25 19.2 R The aggregate() function can be used to generate by-group statistical summaries based on one or more grouping variables. Grouping variables can be declared as a list in the functions by argument. Alternatively, the grouping variable(s) and the variable to be summarized can be passed to aggregate() in formula notation: var_to_be_aggregated ~ grouping_var_1 + ... + grouping_var_N. The summarizing function (e.g., mean(); median(); etc.) is declared in the FUN argument. &gt; # One grouping variable &gt; # Calculating mean of `mpg` in each `cyl` group &gt; aggregate(x = mtcars$mpg, + by = list(cyl = mtcars$cyl), + FUN = &quot;mean&quot;) cyl x 1 4 26.66364 2 6 19.74286 3 8 15.10000 Adding drop=FALSE ensures all combinations of levels are returned even if no data exist at that combination. Below the final row is NA since there are no 8-cylinder cars with a straight engine (vs = 1). &gt; # Two or more grouping variables &gt; # Calculating max of `mpg` in each `cyl`*`vs` group &gt; aggregate(x = mtcars$mpg, + by = list(cyl = mtcars$cyl, vs = mtcars$vs), + FUN = &quot;max&quot;, drop = FALSE) cyl vs x 1 4 0 26.0 2 6 0 21.0 3 8 0 19.2 4 4 1 33.9 5 6 1 21.4 6 8 1 NA &gt; # Or, specify the variable to summarize and the grouping variables in formula notation &gt; aggregate(mpg ~ cyl + vs, data = mtcars, FUN = max) The tidyverse also offers a summarizing function, summarize() (or summarise(), for the Britons), which is in the dplyr package. After grouping a data frame/tibble (with, e.g., dplyrs group_by() function), a user passes it to summarize(), specifying in the function call how the summary statistic should be calculated. &gt; library(dplyr) &gt; mtcars %&gt;% + group_by(cyl, vs) %&gt;% + summarize(avg_mpg = mean(mpg)) `summarise()` has grouped output by &#39;cyl&#39;. You can override using the `.groups` argument. # A tibble: 5 x 3 # Groups: cyl [3] cyl vs avg_mpg &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 0 26 2 4 1 26.7 3 6 0 20.6 4 6 1 19.1 5 8 0 15.1 summarize() makes it easy to specify relatively complicated summary calculations without needing to write an external function. &gt; mtcars %&gt;% + group_by(cyl, vs) %&gt;% + summarize(avg_mpg = mean(mpg), + complicated_summary_calculation = + min(mpg)^0.5 * + mean(wt)^0.5 + + mean(disp)^(1/mean(hp))) `summarise()` has grouped output by &#39;cyl&#39;. You can override using the `.groups` argument. # A tibble: 5 x 4 # Groups: cyl [3] cyl vs avg_mpg complicated_summary_calculation &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 4 0 26 8.51 2 4 1 26.7 8.07 3 6 0 20.6 8.41 4 6 1 19.1 8.81 5 8 0 15.1 7.48 6.3 Centering and Scaling Centering refers to subtracting a constant, such as the mean, from every one of set of values. This is sometimes performed to aid interpretation of linear model coefficients. Scaling refers to rescaling a column or vector of values such that their mean is zero and their standard deviation is one. This is sometimes performed to put multiple variables on the same scale and is often recommended for procedures such as principal components analysis (PCA). Python The scale() function from the preprocessing module of the scikit-learn package provides one-step centering and scaling. To center a variable at zero without scaling it, use scale() with with_mean = True and with_std = False (both are True by default). &gt; from sklearn import preprocessing + + centered_mpg = preprocessing.scale(mtcars.mpg, with_mean = True, with_std = False) + centered_mpg.mean() -3.1086244689504383e-15 To scale a variable after centering it (so that its mean is zero and its standard deviation is one), use scale() with with_mean = True and with_std = True. &gt; from sklearn import preprocessing + + scaled_mpg = preprocessing.scale(mtcars.mpg, with_mean = True, with_std = True) + scaled_mpg.mean() -4.996003610813204e-16 &gt; scaled_mpg.std() 1.0 R The scale() function can both center and scale variables. To center a variable without scaling it, call scale() with the center argument set to TRUE and the scale argument set to FALSE. The variables mean will be subtracted off of each of the variable values. (Note: If desired, the center argument can be set to a numeric value instead of TRUE/FALSE; in that case, each variable value will have the argument value subtracted off of it.) &gt; centered_mpg &lt;- scale(mtcars$mpg, center = T, scale = F) &gt; mean(centered_mpg) [1] 4.440892e-16 To scale a variable (while also centering it), call scale() with the center and scale arguments set to TRUE (these are the default argument values). The variables mean will be subtracted off of each of the variable values, and each value will then be divided by the variables standard deviation. (Note: As with the center argument, the scale argument can also be set to a numeric value instead of TRUE/FALSE; in that case, the divisor will be the argument value instead of the standard deviation.) &gt; scaled_mpg &lt;- scale(mtcars$mpg, center = T, scale = T) &gt; mean(scaled_mpg) [1] 7.112366e-17 &gt; sd(scaled_mpg) [1] 1 "],["basic-plotting-and-visualization.html", "Chapter 7 Basic Plotting and Visualization 7.1 Histograms 7.2 Barplots 7.3 Scatterplot 7.4 Stripcharts 7.5 Boxplots 7.6 Facet plots", " Chapter 7 Basic Plotting and Visualization This chapter looks at creating basic plots to explore and understand data. Visualization in Python and R is a gigantic and evolving topic. We dont pretend to present a comprehensive comparison. The plots below make use of the palmerpenguins data set, which contains various measurements for 344 penguins across three islands in the Antarctic Palmer Archipelago. The data were collected by Kristen Gorman and colleagues, and they were made available under a CC0 public domain license by Allison Horst, Alison Hill, and Kristen Gorman. For the R sections below, we discuss how to generate plots using base R and using ggplot2. Heres a glimpse at the data set: &gt; head(penguins) # A tibble: 6 x 8 species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; 1 Adelie Torge~ 39.1 18.7 181 3750 male 2 Adelie Torge~ 39.5 17.4 186 3800 fema~ 3 Adelie Torge~ 40.3 18 195 3250 fema~ 4 Adelie Torge~ NA NA NA NA &lt;NA&gt; 5 Adelie Torge~ 36.7 19.3 193 3450 fema~ 6 Adelie Torge~ 39.3 20.6 190 3650 male # ... with 1 more variable: year &lt;int&gt; 7.1 Histograms Visualizing the distribution of numeric data. Python The Python plotting library Matplotlibs hist() function computes and plots a histogram. There are many parameters that can be specified within the hist() function so that you can customize the output histogram plot to best fit your needs. Some parameters include the number of bins, the upper and lower bounds on each bin, weights, colors, and more. Below we show a histogram of the bill length from the dataset. We specified 30 bins each of which is light blue with a black outline of linewidth 1. The hist() defaults to no outline which can make it difficult to distinguish bins clearly, so we add in the bin outlines here. One thing to note is that the bins are left inclusive and right exclusive. For example, if a particular bin spans the range of 1 to 3, the bin will include the value 1 but will exclude the value 2 (and will include all values between 1 and 3). In short, bin ranges are as follows [x1,x2) where x1 is the starting point of the bin and x2 is the ending point of the bin. Notice the semicolon at the end of the plt.hist() function. This suppresses the printing of the array generated to create the histogram. &gt; import matplotlib.pyplot as plt + + plt.clf() + plt.hist(penguins.bill_length_mm, bins=30, range=(30,60), + color=&#39;lightblue&#39;, edgecolor=&#39;k&#39;, linewidth=1); + plt.title(&quot;Penguin Bill Lengths&quot;) + plt.xlabel(&quot;Bill Length (mm)&quot;) + plt.ylabel(&quot;Count&quot;) + plt.show() R Base Rs hist() function generates histograms, and features of the histogramlike the bar color, number of bins/breaks, and so oncan be easily customized as below. &gt; hist(penguins$bill_length_mm, breaks = 25, col = &#39;lightblue&#39;, xlim = c(30, 60), + main = &#39;Penguin Bill Lengths&#39;, xlab = &#39;Bill Length (mm)&#39;, ylab = &#39;Count&#39;) The ggplot2 method for generating histograms follows the standard ggplot2 syntax: Initialize a plot with ggplot(), and then add layers thereto, specifying aesthetic properties along the way. Here, the layer to add is geom_histogram(). &gt; ggplot(penguins, aes(x = bill_length_mm)) + + geom_histogram(fill = &#39;lightblue&#39;, color = &#39;black&#39;, bins = 25) + + xlim(30, 60) + labs(title = &#39;Penguin Bill Lengths&#39;, x = &#39;Bill Length (mm)&#39;, y = &#39;Count&#39;) 7.2 Barplots Visualizing the distribution of categorical data. Python For this example, we will generate a bar plot showing how many of each speciesAdelie, Chinstrap, Gentoowe have in our dataset. We go through two ways of doing this here. First, we use the Matplotlib plotting library to create the bar plot using the function bar(). To start, we determine the number of each species, then use that data to create the bar plot. &gt; import matplotlib.pyplot as plt + + # Determine the number of each species + adelie_counts = len(penguins.loc[penguins[&quot;species&quot;]==&quot;Adelie&quot;]) + chinstrap_counts = len(penguins.loc[penguins[&quot;species&quot;]==&quot;Chinstrap&quot;]) + gentoo_counts = len(penguins.loc[penguins[&quot;species&quot;]==&quot;Gentoo&quot;]) + + # Save the counts information into arrays to be inputted into the bar() function + spec = [&quot;Adelie&quot;,&quot;Chinstrap&quot;,&quot;Gentoo&quot;] + counts = [adelie_counts,chinstrap_counts,gentoo_counts] + + plt.clf() # clears the figure to ensure that multiple plots are not overlaid + plt.bar(spec,counts) &lt;BarContainer object of 3 artists&gt; &gt; plt.show() Our data is stored in a pandas Dataframe, which has its own built-in plotting module, plot. Here we create the same bar plot by using the pandas bar() function. &gt; plt.clf() + penguins[&quot;species&quot;].value_counts().plot.bar() + plt.show() One thing to note here is that we generated the same bar plot containing the same information with way less effort. Using the built-in pandas plotting routine proved to be the more efficient method here. R To form barplots, well first take the penguins data set and create a summary data frame containing the statistics were looking to plot. Here, thats simply the sample size of each species in the data set. &gt; species_counts &lt;- as.data.frame(xtabs(~ species, data = penguins)) &gt; species_counts species Freq 1 Adelie 152 2 Chinstrap 68 3 Gentoo 124 We can plot those values using the barplot() function in base R, specifying arguments along the way to customize the title/axis labeling, bar colors, and range of the y axis. To add values above the bars, we can follow barplot() with a text() call as below. &gt; penguin_plot &lt;- barplot(Freq ~ species, data = species_counts, col = c(&#39;lightblue&#39;, &#39;cornflowerblue&#39;, &#39;darkslateblue&#39;), + main = &#39;Species Sample Size&#39;, + xlab = &#39;Species&#39;, ylab = &#39;Count&#39;, ylim = c(0, 200)) &gt; text(x = penguin_plot, y = species_counts$Freq + 10, + labels = species_counts$Freq) To recreate the barplot above with ggplot2, one can add a geom_bar() layer to a plot initialized with ggplot(). &gt; ggplot(species_counts, aes(x = species, y = Freq)) + + geom_bar(aes(fill = species), stat = &#39;identity&#39;) + + scale_fill_manual(values = c(&#39;lightblue&#39;, &#39;cornflowerblue&#39;, &#39;darkslateblue&#39;)) + # scale_fill_manual() is used here for bar-color customization + labs(title = &#39;Species Sample Size&#39;, x = &#39;Species&#39;, y = &#39;Count&#39;) + + theme(legend.position = &#39;none&#39;) + ylim(0, 200) + # For simplicity, we omit the legend here using the code at left + geom_text(aes(label = Freq, vjust = -0.5)) # geom_text() is used here to add counts above the bars 7.3 Scatterplot Visualizing the relationship between two numeric variables. Python The scatter() function, part of matplotlib, can produce scatterplots in Python. The x and y arguments specify the points to plot. In the example below, we also use the c and marker arguments to customize the point color and point shape, respectively. The xlabel(), ylabel(), and title() functions customize plot labels. &gt; import matplotlib.pyplot as plt + + penguins_no_na = penguins.dropna() # remove NA rows, as we only want to plot present data + plt.clf() # clear the plotting space to prevent plot overlap + plt.scatter(x = penguins_no_na[&#39;body_mass_g&#39;], y = penguins_no_na[&#39;flipper_length_mm&#39;], c = &#39;lightblue&#39;, marker = &#39;d&#39;) # &#39;d&#39; generates diamond markers; learn more about available marker shapes here: https://matplotlib.org/stable/api/markers_api.html#module-matplotlib.markers + plt.xlabel(&#39;Body Mass (g)&#39;) + plt.ylabel(&#39;Flipper Length (mm)&#39;) + plt.title(&#39;Scatterplot of Body Mass and Flipper Length&#39;) + + plt.show() R Scatterplots can be generated in base R with the plot() function. The pch argument below modifies the point shape (e.g., 20 = solid circle; 24 = unfilled triangle; etc.) &gt; plot(x = penguins$body_mass_g, y = penguins$flipper_length_mm, + col = &#39;navy&#39;, pch = 20, + main = &#39;Scatterplot of Body Mass and Flipper Length&#39;, + xlab = &#39;Body Mass (g)&#39;, ylab = &#39;Flipper Length (mm)&#39;) To generate a scatterplot with ggplot2, initialize a plot with ggplot(), then add a layer of points with geom_point(). &gt; ggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) + + geom_point(color = &#39;navy&#39;) + + labs(title = &#39;Scatterplot of Body Mass and Flipper Length&#39;, + x = &#39;Body Mass (g)&#39;, y = &#39;Flipper Length (mm)&#39;) 7.4 Stripcharts Stripcharts, or strip plots, are one-dimensional scatterplots. Like boxplots, they reveal the distribution of a numeric variable within levels of a categorical variable. Python The seaborn package provides the stripplot() function. Specify which variables you want on the x and y axes. Below we specify island on the y axis to see the distribution of bill_depth_mm horizontally. Specify your Pandas data frame using the data argument. Finally create the plot using plt.show() from matplotlib. &gt; import seaborn as sns + import matplotlib.pyplot as plt + sns.stripplot(x=&quot;bill_depth_mm&quot;, y=&quot;island&quot;, data=penguins) + plt.show() The stripplot help page provides more examples. R Base R offers the stripchart() function. To indicate the numeric variable and the grouping variable, you can use formula notation: numeric_var ~ grouping_var. Adding methhod = 'jitter' to the set of arguments spreads the points out slightly within each level of the grouping variable, making it easier to see points that might otherwise be obscured by overlap. &gt; stripchart(bill_depth_mm ~ island, data = penguins, + method = &#39;jitter&#39;, + ylab = &#39;Island&#39;, xlab = &#39;Bill Depth (mm)&#39;, + main = &#39;Stripchart of Bill Depth by Island&#39;) Stripcharts can also be made with ggplots2s geom_jitter() function, as shown below. You can control the amount of jitter with a position argument in geom_jitter(). &gt; ggplot(penguins, aes(x = island, y = bill_depth_mm)) + + geom_jitter(aes(color = island), position = position_jitter(0.1)) + + scale_color_manual(values = c(&#39;lightblue&#39;, &#39;cornflowerblue&#39;, &#39;darkslateblue&#39;)) + # scale_fill_manual() is used to manually specify group colors once aes(color = island) is specified in `geom_jitter()` + labs(title = &#39;Stripchart of Bill Depth by Island&#39;, + x = &#39;Island&#39;, y = &#39;Bill Depth (mm)&#39;) + + theme(legend.position = &#39;none&#39;) Warning: Removed 2 rows containing missing values (geom_point). 7.5 Boxplots Visualizing the relationship between a numeric variable and a categorical variable via five-number summaries. Python The boxplot() function in seaborn generates boxplots, and matplotlib can be used for the aesthetics of the plot. &gt; import seaborn as sns + import matplotlib.pyplot as plt + plt.figure() + sns.boxplot(x=&quot;island&quot;, y=&quot;bill_depth_mm&quot;, data=penguins) + plt.xlabel(&quot;Island&quot;) + plt.ylabel(&quot;Bill Depth (mm)&quot;) + plt.title(&quot;Bill Depth by Island&quot;) + plt.show() R The boxplot() function in base R generates boxplots, and a user specifies the grouping variable and the numeric variable to be plotted in formula notation: y ~ grouping_var. &gt; boxplot(bill_depth_mm ~ island, data = penguins, col = c(&#39;lightblue&#39;, &#39;cornflowerblue&#39;, &#39;darkslateblue&#39;), + main = &#39;Boxplot of Bill Depth by Island&#39;, xlab = &#39;Island&#39;, ylab = &#39;Bill Depth (mm)&#39;) To generate a boxplot with ggplot2, add a geom_boxplot() layer to a plot initialized with ggplot(). &gt; ggplot(penguins, aes(x = island, y = bill_depth_mm)) + + geom_boxplot(aes(fill = island)) + + scale_fill_manual(values = c(&#39;lightblue&#39;, &#39;cornflowerblue&#39;, &#39;darkslateblue&#39;)) + + labs(title = &#39;Boxplot of Bill Depth by Island&#39;, + x = &#39;Island&#39;, y = &#39;Bill Depth (mm)&#39;) + + theme(legend.position = &#39;none&#39;) 7.6 Facet plots Facet plots (also called trellis plots, lattice plots, and conditional plots) are comprised of multiple smaller plots, where each subplot contains a subset of the overall data, with subsets defined by one or more faceting variables. Python The seaborn package provides several functions for creating facet plots, including relplot(), displot(), catplot(), and lmplot(). Below we demonstrate the lmplot() function which allows you to create scatter plots at certain levels of categorical variable. Specify your x and y variables as character strings using the x and y arguments, respectively. Specing the grouping variable using either the col or row arguments. By default a linear-squares lines is added to the plot. Setting ci = None suppresses the confidence interval ribbon. &gt; plt.clf() + sns.lmplot(x = &quot;bill_length_mm&quot;, y = &quot;bill_depth_mm&quot;, col = &quot;species&quot;, + data = penguins, ci = None) To facet by two variables, provide variables to both the col and row arguments. &gt; plt.clf() + sns.lmplot(x = &quot;bill_length_mm&quot;, y = &quot;bill_depth_mm&quot;, + col = &quot;species&quot;, row = &quot;sex&quot;, + data = penguins, ci = None) Set lowess = True for smooth trend lines. Notice also that color can be mapped to the same variable used for faceting. &gt; plt.clf() + sns.lmplot(x = &quot;bill_length_mm&quot;, y = &quot;bill_depth_mm&quot;, + col = &quot;species&quot;, hue = &quot;species&quot;, + data = penguins, ci = None, lowess = True) The lmplot help page showcases other examples. R The coplot() function in base R produces conditioning plots using formula notation: y ~ x | grouping_var. The rows and columns arguments control layout. Below we specify one row of plots. The panel argument controls what action is carried out in each plot. The default is a scatterplot. Below we use the base R panel.smooth function to create scatter plots with a smooth trend line. &gt; coplot(bill_depth_mm ~ bill_length_mm | species, + data = penguins, + panel = panel.smooth, + rows = 1, col = &quot;navy&quot;) To condition on two variables, use formula notation with syntax: y ~ x | grp_var1 * grp_var2. &gt; coplot(bill_depth_mm ~ bill_length_mm | species * sex, + data = penguins, + panel = panel.smooth, + col = &quot;navy&quot;) The labels of the conditioning variables unfortunately use a lot of real estate in the margins, and there is no easy way to modify that. However, this design works quite well when we condition on a numeric variable. The coplot() function automatically creates overlapping group intervals to condition on, and the stacked layout of the labels helps us visualize how the relationship between y and x changes between the groups. To manually set the number of groups, use the number argument. Below we specify 4 groups to be generated for bill_length_mm. &gt; coplot(flipper_length_mm ~ body_mass_g | bill_length_mm, + data = penguins, + panel = panel.smooth, + number = 4, + rows = 1, col = &quot;navy&quot;) Alternatively, ggplot2 provides a intuitive and easy-to-use method for generating facet plots: A user specifies the aesthetics of the plot using standard ggplot2 syntax (i.e., as a series of added layers) and then adds an additional call, facet_wrap() (or facet_grid(); differences are discussed below), specifying the faceting variable(s) to split up the plots by. &gt; ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + + geom_point(color = &#39;navy&#39;) + + geom_smooth(method = &#39;lm&#39;, se = F, color = &#39;salmon&#39;) + # Add least-squares line + facet_wrap(~species) `geom_smooth()` using formula &#39;y ~ x&#39; &gt; # Use formula notation, a character vector, or vars() to specify faceting &gt; # variables; e.g., ~species, c(&#39;species&#39;), or vars(species) The number of rows and columns can be manually specified with nrow and ncol arguments in facet_wrap(). By default, the x and y axes of all facet plots will be on the same scale. The axis ranges can be set to vary freely by adding scales = 'free' as an argument (or, alternatively, scales = 'free_x' or scales = 'free_y' to free just the x or y axis). Both facet_wrap() and facet_grid() can be used to make facet plots. When faceting based on multiple variables (e.g., species and sex), facet_wrap() will drop group combinations for which there are no data points, whereas facet_grid() will generate a plot for all possible group combinations: &gt; ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + + geom_point(color = &#39;navy&#39;) + + geom_smooth(method = &#39;lm&#39;, se = F, color = &#39;salmon&#39;) + + facet_wrap(vars(species, sex)) `geom_smooth()` using formula &#39;y ~ x&#39; &gt; &gt; ggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + + geom_point(color = &#39;navy&#39;) + + geom_smooth(method = &#39;lm&#39;, se = F, color = &#39;salmon&#39;) + + facet_grid(rows = vars(sex), cols = vars(species)) `geom_smooth()` using formula &#39;y ~ x&#39; &gt; # Note that facet_grid() has separate `rows` and `cols` arguments for specifying &gt; # faceting variables "],["selected-topics-in-statistical-inference.html", "Chapter 8 Selected Topics in Statistical Inference 8.1 Comparing group means 8.2 Comparing group proportions 8.3 Linear modeling 8.4 Logistic regression", " Chapter 8 Selected Topics in Statistical Inference This chapter looks at performing selected statistical analyses. It is not comprehensive. The focus is on implementation using Python and R. Good statistical practice is more than knowing which function to use. At a minimum we recommend reading the article, Ten Simple Rules for Effective Statistical Practice (Kass et al. 2016). 8.1 Comparing group means Many research studies compare mean values of some quantity of interest between two or more groups. A t test analyzes two group means. An Analysis of Variance, or ANOVA, analyzes three or more group means. Both the t test and ANOVA are special cases of a linear model. To demonstrate the t test, we examine fictitious data on 15 scores between two groups of subjects. The control group was tested as-is while the treated group experienced a particular intervention. Of interest is (1) whether or not the mean scores differ meaningfully between the treated and control groups, and (2) if they do differ, how are they different? To demonstrate the ANOVA test, we use data from The Analysis of Biological Data (3rd ed)(Whitlock and Schluter 2020) on the mass of pine cones (in grams) from three different environments in North America. Of interest is (1) whether or not the mean mass of pine cones differ meaningfully between the three locations, and (2) if they do differ, how are they different? We usually assess the first question in each scenario with a hypothesis test and p-value. The null hypothesis is no difference between the means. The p-value is the probability of the observed differences between the groups (or more extreme differences) assuming the null hypothesis is true. A small p-value, traditionally less then 0.05, provides evidence against the null. For example, a p-value of 0.01 says theres a 1% chance of sampling data as different as this (or more different) if there really was no difference between the groups. Note that p-values dont tell you how two or more statistics differ. See the ASA Statement on p-values. We assess the second question in each scenario by calculating confidence intervals on the difference in means. This is more informative than a p-value. A confidence interval gives us information on the uncertainty, direction and magnitude of a difference in means. For example, a 95% confidence interval of [2, 15] tells us the data is consistent with a difference anywhere between 2 and 15 and that the mean of one group appears to be at least 2 units larger than the mean of the other group. Note that a 95% confidence interval does not mean there is a 95% probability that the true value is in the interval. The confidence interval either captured the true value or it did not. We dont know. However the process of calculating the confidence interval works roughly 95% of the time. Python t-test Our data is available as a Pandas dataframe. Its small enough to view in its entirety. &gt; ch8_d1 score group 0 77.0 control 1 81.0 control 2 77.0 control 3 86.0 control 4 81.0 control 5 77.0 control 6 82.0 control 7 83.0 control 8 82.0 control 9 79.0 control 10 86.0 control 11 82.0 control 12 78.0 control 13 71.0 control 14 84.0 control 15 85.0 treated 16 85.0 treated 17 89.0 treated 18 88.0 treated 19 87.0 treated 20 89.0 treated 21 88.0 treated 22 85.0 treated 23 77.0 treated 24 87.0 treated 25 85.0 treated 26 84.0 treated 27 79.0 treated 28 83.0 treated 29 87.0 treated A stripchart is one of many ways to visualize numeric data between two groups. Here we use the seaborn function stripplot(). It appears the treated group had higher scores. &gt; import seaborn as sns + import matplotlib.pyplot as plt + plt.clf() + sns.stripplot(x=&quot;score&quot;, y=&quot;group&quot;, data=ch8_d1) + plt.show() One way to perform a t test in Python is via the CompareMeans() function and its associated methods available in the statsmodels package. Below we import statsmodels.stats.api as sms. &gt; import statsmodels.stats.api as sms We first extract the data we want to compare as pandas Series. &gt; d_control = ch8_d1.query(&#39;group == &quot;control&quot;&#39;)[&#39;score&#39;] + d_treated = ch8_d1.query(&#39;group == &quot;treated&quot;&#39;)[&#39;score&#39;] Next we create Descriptive statistics objects using the DescrStatsW() function. &gt; control = sms.DescrStatsW(d_control) + treated = sms.DescrStatsW(d_treated) Descriptive statistics objects have attributes such as mean and std (standard deviation). Below we print the mean and standard deviation of each group. We also round the standard deviation to three decimal places and place a line break before printing the standard deviation. &gt; print(&quot;control mean:&quot;, control.mean, &quot;\\ncontrol std:&quot;, round(control.std, 3)) control mean: 80.4 control std: 3.844 &gt; print(&quot;treated mean:&quot;, treated.mean, &quot;\\ntreated std:&quot;, round(treated.std, 3)) treated mean: 85.2 treated std: 3.331 Next we create a CompareMeans means object using the CompareMeans() function. The required inputs are Descriptive statistics objects. We save the result as ttest. &gt; ttest = sms.CompareMeans(control, treated) Now we can use various methods with the ttest object. To see the result of a two sample t test assuming unequal variances, along with a confidence interval on the differences, use the summary method with usevar='unequal'. &gt; print(ttest.summary(usevar=&#39;unequal&#39;)) Test for equality of means ============================================================================== coef std err t P&gt;|t| [0.025 0.975] ------------------------------------------------------------------------------ subset #1 -4.8000 1.359 -3.531 0.001 -7.587 -2.013 ============================================================================== The p-value of 0.001 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group. ANOVA Our data is available as a Pandas dataframe. Its small enough to view in its entirety. &gt; ch8_d2 mass location 0 9.6 1 1 9.4 1 2 8.9 1 3 8.8 1 4 8.5 1 5 8.2 1 6 6.8 2 7 6.6 2 8 6.0 2 9 5.7 2 10 5.3 2 11 6.7 3 12 6.4 3 13 6.2 3 14 5.7 3 15 5.6 3 Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass. &gt; plt.clf() + sns.stripplot(x=&quot;mass&quot;, y=&quot;location&quot;, data=ch8_d2) + plt.show() We can calculate means using the groupby and mean methods. &gt; ch8_d2[&#39;mass&#39;].groupby(ch8_d2[&#39;location&#39;]).mean() location 1 8.90 2 6.08 3 6.12 Name: mass, dtype: float64 One way to perform an ANOVA test in Python is via the anova_oneway() function, also available in the statsmodels package. The anova_oneway() function can perform an ANOVA on a pandas Dataframe with the first argument specifying the numeric data and the second argument the grouping variable. We also set use_var='equal' to replicate the R output below. &gt; sms.anova_oneway(ch8_d2.mass, ch8_d2.location, use_var=&#39;equal&#39;) &lt;class &#39;statsmodels.stats.base.HolderTuple&#39;&gt; statistic = 50.085429769392036 pvalue = 7.786760128813737e-07 df = (2.0, 13.0) df_num = 2.0 df_denom = 13.0 nobs_t = 16.0 n_groups = 3 means = array([8.9 , 6.08, 6.12]) nobs = array([6., 5., 5.]) vars_ = array([0.28 , 0.387, 0.217]) use_var = &#39;equal&#39; welch_correction = True tuple = (50.085429769392036, 7.786760128813737e-07) The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population. A common follow-up to an ANOVA is Tukeys Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. To carry this out in the statsmodels package, we need to first create a MultiComparison object using the multicomp.MultiComparison() function. Then we use the tukeyhsd() method to compare the means with corrected p-values. &gt; mc = sms.multicomp.MultiComparison(ch8_d2.mass, ch8_d2.location) + print(mc.tukeyhsd()) Multiple Comparison of Means - Tukey HSD, FWER=0.05 =================================================== group1 group2 meandiff p-adj lower upper reject --------------------------------------------------- 1 2 -2.82 0.001 -3.6858 -1.9542 True 1 3 -2.78 0.001 -3.6458 -1.9142 True 2 3 0.04 0.9 -0.8643 0.9443 False --------------------------------------------------- The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but were not sure if the difference is positive or negative. R t-test The str() function allows to take a quick look at the data frame ch8_d1. One column contains the scores, the other column indicates which group the subject was in (control vs treated). &gt; str(ch8_d1) &#39;data.frame&#39;: 30 obs. of 2 variables: $ score: num 77 81 77 86 81 77 82 83 82 79 ... $ group: chr &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ... A stripchart is one of many ways to visualize numeric data between two groups. Here we use the base R function stripchart(). The formula score ~ group says to plot score by group. The las = 1 argument says to rotate the y-axis labels. The method = \"jitter\" arguments says to randomly scatter the points vertically so they dont overplot. It appears the treated group had higher scores. &gt; stripchart(score ~ group, data = ch8_d1, las = 1, method = &quot;jitter&quot;) To calculate the means between the two groups we can use the aggregate() function. Again the formula score ~ group says to aggregate score by group. We specify mean so that we calculate the mean between the two groups. Some other functions we could specify include median, sd, or sum. The sample mean of the treated group is about 5 points higher than the control group. &gt; aggregate(score ~ group, data = ch8_d1, mean) group score 1 control 80.4 2 treated 85.2 Is this difference meaningful? What if we took more samples? Would each sample result in similar differences in the means? A t test attempts to answer this. The t.test() function accommodates formula notation allowing us to specify that we want to calculate mean score by group. &gt; t.test(score ~ group, data = ch8_d1) Welch Two Sample t-test data: score by group t = -3.5313, df = 27.445, p-value = 0.001482 alternative hypothesis: true difference in means between group control and group treated is not equal to 0 95 percent confidence interval: -7.586883 -2.013117 sample estimates: mean in group control mean in group treated 80.4 85.2 The p-value of 0.0015 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group. ANOVA The str() function allows to take a quick look at the data frame ch8_d2. One column contains the mass of the pine cones, the other column indicates which location the pine cone was found. &gt; str(ch8_d2) &#39;data.frame&#39;: 16 obs. of 2 variables: $ mass : num 9.6 9.4 8.9 8.8 8.5 8.2 6.8 6.6 6 5.7 ... $ location: chr &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass. &gt; stripchart(mass ~ location, data = ch8_d2, las = 1, method = &quot;jitter&quot;) To calculate the means between the three groups we can use the aggregate() function. Again the formula mass ~ location says to aggregate mass by location. We specify mean so that we calculate the mean between the three groups. &gt; aggregate(mass ~ location, data = ch8_d2, mean) location mass 1 1 8.90 2 2 6.08 3 3 6.12 Is this difference meaningful? ANOVA attempts to answer this. The aov() function carries out the ANOVA test and also accommodates formula notation. Its usually preferable to save the ANOVA result into an object and call summary() on the object. &gt; aov1 &lt;- aov(mass ~ location, data = ch8_d2) &gt; summary(aov1) Df Sum Sq Mean Sq F value Pr(&gt;F) location 2 29.404 14.702 50.09 7.79e-07 *** Residuals 13 3.816 0.294 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population. Unlike the t.test() output, the aov() summary does not provide confidence intervals on differences in means. Thats because there are many kinds of differences we might want to assess. A common and easy procedure is Tukeys Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. Base R provides the TukeyHSD() function for this task. Call it on the ANOVA object. &gt; TukeyHSD(aov1) Tukey multiple comparisons of means 95% family-wise confidence level Fit: aov(formula = mass ~ location, data = ch8_d2) $location diff lwr upr p adj 2-1 -2.82 -3.6862516 -1.9537484 0.0000028 3-1 -2.78 -3.6462516 -1.9137484 0.0000033 3-2 0.04 -0.8647703 0.9447703 0.9925198 The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but were not sure if the difference is positive or negative. 8.2 Comparing group proportions It is often of interest to compare proportions between two groups. Sometimes this is referred to as a two-sample proportion test. To demonstrate we use an exercise from the text Introductory Statistics with R (Dalgaard 2008) (p.154). We are told that 210 out of 747 patients died of Rocky Mountain spotted fever in the western United States. Thats a proportion of 0.281. In the eastern United States, 122 out 661 patients died. Thats a proportion of 0.185. Is the difference in proportions statistically significant? In other words, assuming there is no difference in the fatality rate between the two regions, is this difference in proportions surprising? Python A two-sample proportion test can be carried out in Python using the test_proportions_2indep() function from the statsmodels package. The two proportions being compared must be independent. The first argument is the number of successes or occurrences for the first proportion. The second argument is the number of total trials for the first group. The third and fourth arguments are the occurrences and total number of trials for the second group, respectively. &gt; import statsmodels.stats.api as sms + ptest = sms.test_proportions_2indep(210, 747, 122, 661) We can extract the p-value of the test and the difference in proportions using the pvalue and diff attributes, respectively. &gt; ptest.pvalue 1.632346798072468e-05 &gt; # rounded to 4 decimal places + round(ptest.diff, 4) 0.0966 To calculate a 95% confidence interval for the difference in proportions we need to use the confint_proportions_2indep() function. &gt; pdiff = sms.confint_proportions_2indep(210, 747, 122, 661) + pdiff (0.05241555145475882, 0.13988087590630482) The result is returned as a tuple with an extreme amount of precision. We recommend rounding these values to few decimal places. Heres one way using f strings. Notice we extract each element of the pdiff tuple and round to 5 decimal places. &gt; print(f&quot;({round(pdiff[0],5)}, {round(pdiff[1],5)})&quot;) (0.05242, 0.13988) This results are slightly different from the R example below. Thats because the test_proportions_2indep() and confint_proportions_2indep() functions use different methods. See their respective help pages to learn more about the methods available and other function arguments. test_proportions_2indep help page confint_proportions_2indep help page R A two-sample proportion test in R can be carried out with the prop.test() function. The first argument, x, is the number of successes or occurrences of some event for each group. The second argument, n, is the number of total trials for each group. &gt; prop.test(x = c(210, 122), n = c(747, 661)) 2-sample test for equality of proportions with continuity correction data: c(210, 122) out of c(747, 661) X-squared = 17.612, df = 1, p-value = 2.709e-05 alternative hypothesis: two.sided 95 percent confidence interval: 0.05138139 0.14172994 sample estimates: prop 1 prop 2 0.2811245 0.1845688 The proportion of patients who died in the western US is about 0.28. The proportion who died in the eastern US is about 0.18. The small p-value says there is a very small chance of seeing a difference as large as this (or larger) if there really was no difference in the proportions. The confidence interval on the difference of proportions ranges from 0.05 to 0.14, indicating that this fever seems to kill at least 5% more patients in the western US. Sometimes data is presented in a 2-way table with successes and failures. We can present the preceding data in a table as follows using the matrix() function. &gt; fever &lt;- matrix(c(210, 122, + 747-210, 661-122), ncol = 2) &gt; rownames(fever) &lt;- c(&quot;western US&quot;, &quot;eastern US&quot;) &gt; colnames(fever) &lt;- c(&quot;died&quot;, &quot;lived&quot;) &gt; fever died lived western US 210 537 eastern US 122 539 When the table is constructed in this fashion with successes in the first column and failures in the second column, we can feed the table directly to the prop.test() function. (Obviously success here means experienced the event of interest.) &gt; prop.test(fever) 2-sample test for equality of proportions with continuity correction data: fever X-squared = 17.612, df = 1, p-value = 2.709e-05 alternative hypothesis: two.sided 95 percent confidence interval: 0.05138139 0.14172994 sample estimates: prop 1 prop 2 0.2811245 0.1845688 The chi-squared test statistic is reported as X-squared = 17.612. This is the same statistic reported if we ran a chi-squared test of association using the chisq.test() function. &gt; chisq.test(fever) Pearson&#39;s Chi-squared test with Yates&#39; continuity correction data: fever X-squared = 17.612, df = 1, p-value = 2.709e-05 This tests the null hypothesis of no association between location in the US and fatality of the fever. The result is identical to prop.test() output, however there is no indication of the nature of association. 8.3 Linear modeling Linear modeling attempts to assess if or how the variability a numeric variable depends on one or more predictor variables. This is often referred to as regression modeling or multiple regression. While it is relatively easy to fit a model and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise. We walk through a somewhat simple example using data on weekly gas consumption. The data is courtesy of the R package MASS (Venables and Ripley 2002a). The documentation describes the data as follows: Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption. The whiteside data frame has 56 rows and 3 columns: Insul: A factor, before or after insulation. Temp: average outside temperature in degrees Celsius. Gas: weekly gas consumption in 1000s of cubic feet. Below we demonstrate modeling Gas as a function of Insul, Temp, and their interaction. Obviously this is not a comprehensive treatment of linear modeling. Python R The lm() function fits a linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (~), and then your predictor variables, or terms, separated by plus operators (+). Listing two variables separated by a colon (:) indicates we wish to fit an interaction for those variables. See ?formula for further details on formula syntax. Its considered best practice to reference variables in a data frame and indicate the data frame using the data argument. Though not required, youll almost always want to save the result to an object for further inquiry. &gt; m &lt;- lm(Gas ~ Insul + Temp + Insul:Temp, data = whiteside) Once you fit your model, you can extract information about it using several functions. The most commonly used include: summary(): summary of model coefficients with standard errors and test statistics coef(): model coefficients confint(): 95% confidence interval of model coefficients plot(): a set of four diagnostic plots The summary() function produces the standard regression summary one typically finds described in a statistics textbook. &gt; summary(m) Call: lm(formula = Gas ~ Insul + Temp + Insul:Temp, data = whiteside) Residuals: Min 1Q Median 3Q Max -0.97802 -0.18011 0.03757 0.20930 0.63803 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 6.85383 0.13596 50.409 &lt; 2e-16 *** InsulAfter -2.12998 0.18009 -11.827 2.32e-16 *** Temp -0.39324 0.02249 -17.487 &lt; 2e-16 *** InsulAfter:Temp 0.11530 0.03211 3.591 0.000731 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.323 on 52 degrees of freedom Multiple R-squared: 0.9277, Adjusted R-squared: 0.9235 F-statistic: 222.3 on 3 and 52 DF, p-value: &lt; 2.2e-16 Calling plot() on a model object produces four different diagnostic plots by default. Using the which argument we can specify which of six possible plots to create. The first one checks the constant variance assumption (ie, that our model is not dramatically over- or under-predicting values.) We hope to see residuals evenly scattered around 0. (See ?plot.lm for more details on the diagnostic plots.) &gt; plot(m, which = 1) Once we fit a model and were reasonably confident that its a good model, we may want to visualize it. Three packages in R that help with this are emmeans, effects, and ggeffects. We briefly demonstrate the ggeffects package. You need to first install the ggeffects package as it does not come with the base R installation. Once installed, load using the library() function. Once loaded, we can get a basic visualization of our model by using the plot() and ggpredict() functions. This is particularly useful for models with interactions. Use the terms argument to specify which variables to plot. Below we list Temp first, which will plot Temp on the x axis. Then we list Insul, the grouping variable, to indicate we want a separate fit for each level of Insul. &gt; # install.pacakges(&quot;ggeffects&quot;) &gt; library(ggeffects) Warning: package &#39;ggeffects&#39; was built under R version 4.1.2 &gt; plot(ggpredict(m, terms = c(&quot;Temp&quot;, &quot;Insul&quot;))) We see that after installing insulation, gas consumption fell considerably, and that the effect of temperature on gas consumption is less pronounced. 8.4 Logistic regression Logistic regression attempts to assess if or how the variability a binary variable depends on one or more predictor variables. It is a type of Generalized Linear Model and is commonly used to model the probability of an event occurring. While it is relatively easy to fit a model and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise. We walk through a basic example using data on low infant birth weight. The data is courtesy of the R package MASS (Venables and Ripley 2002a). According to the documentation, the data were collected at Baystate Medical Center, Springfield, Mass during 1986. We use the data as prepared in the example code found at ?birthwt. The birthwt data frame has 189 rows and 9 columns: low: 1 if birth weight less than 2.5 kg, 0 otherwise age: mothers age in years lwt: mothers weight in pounds at last menstrual period race: mothers race (white, black, other) smoke: smoking status during pregnancy (1 = yes, 0 = no) ptd: previous premature labors (1 = yes, 0 = no) ht: history of hypertension (1 = yes, 0 = no) ui: presence of uterine irritability (1 = yes, 0 = no) ftv: number of physician visits during the first trimester (0, 1, 2+) Below we demonstrate modeling low as a function of all other predictors. Obviously this is not a comprehensive treatment of logistic regression. Python R The glm() function fits a generalized linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (~), and then your predictor variables, or terms, separated by plus operators (+). Listing two variables separated by a colon (:) indicates we wish to fit an interaction for those variables. See ?formula for further details on formula syntax. In addition, glm() requires we specify a family argument to specify the error distribution for the dependent variable. The default is gaussian. For a logistic regression model, we need to specify binomial since our dependent variable is binary. Its considered best practice to reference variables in a data frame and indicate the data frame using the data argument. Though not required, youll almost always want to save the result to an object for further inquiry. &gt; mod &lt;- glm(low ~ age + lwt + race + + smoke + ptd + ht + + ui + ftv, + data = birthwt, family = binomial) Since were modeling low as a function of all other variables in the data frame, we could have used the following syntax, where the period symbolizes all other remaining variables: &gt; mod &lt;- glm(low ~ ., data = birthwt, family = binomial) Once you fit your logistic regression model, you can extract information about it using several functions. The most commonly used include: summary(): summary of model coefficients with standard errors and test statistics coef(): model coefficients confint(): 95% confidence interval of model coefficients The summary() function produces the standard regression summary one typically finds described in a statistics textbook. &gt; summary(mod) Call: glm(formula = low ~ age + lwt + race + smoke + ptd + ht + ui + ftv, family = binomial, data = birthwt) Deviance Residuals: Min 1Q Median 3Q Max -1.7038 -0.8068 -0.5008 0.8835 2.2152 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.82302 1.24471 0.661 0.50848 age -0.03723 0.03870 -0.962 0.33602 lwt -0.01565 0.00708 -2.211 0.02705 * raceblack 1.19241 0.53597 2.225 0.02609 * raceother 0.74069 0.46174 1.604 0.10869 smoke1 0.75553 0.42502 1.778 0.07546 . ptd1 1.34376 0.48062 2.796 0.00518 ** ht1 1.91317 0.72074 2.654 0.00794 ** ui1 0.68019 0.46434 1.465 0.14296 ftv1 -0.43638 0.47939 -0.910 0.36268 ftv2+ 0.17901 0.45638 0.392 0.69488 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 234.67 on 188 degrees of freedom Residual deviance: 195.48 on 178 degrees of freedom AIC: 217.48 Number of Fisher Scoring iterations: 4 Exponentiating coefficients in a logistic regression model produces odds ratios. To get the odds ratio for the previous premature labors variable, ptd, we do the following: &gt; exp(coef(mod)[&quot;ptd1&quot;]) ptd1 3.833443 This says the odds of having an infant with low birth weight are about 3.8 times higher for women who experienced previous premature labors versus women who did not, assuming all other variables equal. The 3.8 value is just an estimate. We can use the confint() function to get a 95% confidence interval on the odds ratio. &gt; exp(confint(mod)[&quot;ptd1&quot;,]) 2.5 % 97.5 % 1.516837 10.128974 It appears the odds ratio is at least 1.5, possibly as high as 10.1 (assuming we believe this model). Once we fit a model and were reasonably confident that its a good model, we may want to visualize it. Three packages in R that help with this are emmeans, effects, and ggeffects. We briefly demonstrate the ggeffects package. You need to first install the ggeffects package as it does not come with the base R installation. Once installed, load using the library() function. Once loaded, we can get a basic visualization of our model by using the plot() and ggpredict() functions. This is particularly useful for logistic regression models because it produces model predictions on a probability scale. Use the terms argument to specify which variables to plot. Below we create two plots: one for ptd (previous premature labors) and one for lwt (mothers weight at last menstrual period). &gt; # install.packages(&quot;ggeffects&quot;) &gt; library(ggeffects) &gt; plot(ggpredict(mod, terms = &quot;ptd&quot;)) It looks like the probability of low infant birth weight jumps from about 12% to over 35% for mothers who previously experienced premature labors, though the error bars on the expected values are quite large. &gt; plot(ggpredict(mod, terms = &quot;lwt&quot;)) It appears the probability of low infant birth weight drops from about 15% when a mother weighs 100 lbs to about 5% when a mother weighs around 200 lbs. The regions with the larger confidence ribbon indicate regions of higher uncertainty. There are clearly not many mothers in our data who weigh less than 100 lbs. References "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
