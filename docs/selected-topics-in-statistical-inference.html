<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Selected Topics in Statistical Inference | Python and R</title>
  <meta name="description" content="This book provides parallel examples in Python and R to help users of one platform more easily transition to the other." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Selected Topics in Statistical Inference | Python and R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="This book provides parallel examples in Python and R to help users of one platform more easily transition to the other." />
  <meta name="github-repo" content="uvastatlab/Python_and_R" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Selected Topics in Statistical Inference | Python and R" />
  
  <meta name="twitter:description" content="This book provides parallel examples in Python and R to help users of one platform more easily transition to the other." />
  

<meta name="author" content="Clay Ford, Jacob Goldstein-Greenwood, Oyinkansola Adenekan, Samantha Lomuscio" />


<meta name="date" content="2022-05-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="basic-plotting-and-visualization.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Python and R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="basics.html"><a href="basics.html"><i class="fa fa-check"></i><b>1</b> Basics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="basics.html"><a href="basics.html#math"><i class="fa fa-check"></i><b>1.1</b> Math</a></li>
<li class="chapter" data-level="1.2" data-path="basics.html"><a href="basics.html#missing-values"><i class="fa fa-check"></i><b>1.2</b> Missing values</a></li>
<li class="chapter" data-level="1.3" data-path="basics.html"><a href="basics.html#assignment"><i class="fa fa-check"></i><b>1.3</b> Assignment</a></li>
<li class="chapter" data-level="1.4" data-path="basics.html"><a href="basics.html#printing-a-value"><i class="fa fa-check"></i><b>1.4</b> Printing a value</a></li>
<li class="chapter" data-level="1.5" data-path="basics.html"><a href="basics.html#packages"><i class="fa fa-check"></i><b>1.5</b> Packages</a></li>
<li class="chapter" data-level="1.6" data-path="basics.html"><a href="basics.html#logic"><i class="fa fa-check"></i><b>1.6</b> Logic</a></li>
<li class="chapter" data-level="1.7" data-path="basics.html"><a href="basics.html#generating-a-sequence-of-values"><i class="fa fa-check"></i><b>1.7</b> Generating a sequence of values</a></li>
<li class="chapter" data-level="1.8" data-path="basics.html"><a href="basics.html#calculating-means-and-medians"><i class="fa fa-check"></i><b>1.8</b> Calculating means and medians</a></li>
<li class="chapter" data-level="1.9" data-path="basics.html"><a href="basics.html#writing-your-own-functions"><i class="fa fa-check"></i><b>1.9</b> Writing your own functions</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-structures.html"><a href="data-structures.html"><i class="fa fa-check"></i><b>2</b> Data Structures</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-structures.html"><a href="data-structures.html#one-dimensional-data"><i class="fa fa-check"></i><b>2.1</b> One-dimensional data</a></li>
<li class="chapter" data-level="2.2" data-path="data-structures.html"><a href="data-structures.html#two-dimensional-data"><i class="fa fa-check"></i><b>2.2</b> Two-dimensional data</a></li>
<li class="chapter" data-level="2.3" data-path="data-structures.html"><a href="data-structures.html#three-dimensional-and-higher-data"><i class="fa fa-check"></i><b>2.3</b> Three-dimensional and higher data</a></li>
<li class="chapter" data-level="2.4" data-path="data-structures.html"><a href="data-structures.html#general-data-structures"><i class="fa fa-check"></i><b>2.4</b> General data structures</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html"><i class="fa fa-check"></i><b>3</b> Import, Export, and Save Data</a>
<ul>
<li class="chapter" data-level="3.1" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html#csv"><i class="fa fa-check"></i><b>3.1</b> CSV</a></li>
<li class="chapter" data-level="3.2" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html#xlsxlsx-excel"><i class="fa fa-check"></i><b>3.2</b> XLS/XLSX (Excel)</a></li>
<li class="chapter" data-level="3.3" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html#json"><i class="fa fa-check"></i><b>3.3</b> JSON</a></li>
<li class="chapter" data-level="3.4" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html#xml"><i class="fa fa-check"></i><b>3.4</b> XML</a></li>
<li class="chapter" data-level="3.5" data-path="import-export-and-save-data.html"><a href="import-export-and-save-data.html#exportingwritingsaving-data-and-variables"><i class="fa fa-check"></i><b>3.5</b> Exporting/Writing/Saving data and variables</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data-manipulation.html"><a href="data-manipulation.html"><i class="fa fa-check"></i><b>4</b> Data Manipulation</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data-manipulation.html"><a href="data-manipulation.html#view-variable-names-and-types"><i class="fa fa-check"></i><b>4.1</b> View variable names and types</a></li>
<li class="chapter" data-level="4.2" data-path="data-manipulation.html"><a href="data-manipulation.html#select-variables"><i class="fa fa-check"></i><b>4.2</b> Select variables</a></li>
<li class="chapter" data-level="4.3" data-path="data-manipulation.html"><a href="data-manipulation.html#filtersubset-variables"><i class="fa fa-check"></i><b>4.3</b> Filter/Subset variables</a></li>
<li class="chapter" data-level="4.4" data-path="data-manipulation.html"><a href="data-manipulation.html#rename-variables"><i class="fa fa-check"></i><b>4.4</b> Rename variables</a></li>
<li class="chapter" data-level="4.5" data-path="data-manipulation.html"><a href="data-manipulation.html#create-replace-and-remove-variables"><i class="fa fa-check"></i><b>4.5</b> Create, replace and remove variables</a></li>
<li class="chapter" data-level="4.6" data-path="data-manipulation.html"><a href="data-manipulation.html#create-strings-from-numbers"><i class="fa fa-check"></i><b>4.6</b> Create strings from numbers</a></li>
<li class="chapter" data-level="4.7" data-path="data-manipulation.html"><a href="data-manipulation.html#create-numbers-from-strings"><i class="fa fa-check"></i><b>4.7</b> Create numbers from strings</a></li>
<li class="chapter" data-level="4.8" data-path="data-manipulation.html"><a href="data-manipulation.html#combine-strings"><i class="fa fa-check"></i><b>4.8</b> Combine strings</a></li>
<li class="chapter" data-level="4.9" data-path="data-manipulation.html"><a href="data-manipulation.html#finding-and-replacing-patterns-within-strings"><i class="fa fa-check"></i><b>4.9</b> Finding and replacing patterns within strings</a></li>
<li class="chapter" data-level="4.10" data-path="data-manipulation.html"><a href="data-manipulation.html#change-case"><i class="fa fa-check"></i><b>4.10</b> Change case</a></li>
<li class="chapter" data-level="4.11" data-path="data-manipulation.html"><a href="data-manipulation.html#drop-duplicate-rows"><i class="fa fa-check"></i><b>4.11</b> Drop duplicate rows</a></li>
<li class="chapter" data-level="4.12" data-path="data-manipulation.html"><a href="data-manipulation.html#format-dates"><i class="fa fa-check"></i><b>4.12</b> Format dates</a></li>
<li class="chapter" data-level="4.13" data-path="data-manipulation.html"><a href="data-manipulation.html#randomly-sample-rows"><i class="fa fa-check"></i><b>4.13</b> Randomly sample rows</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html"><i class="fa fa-check"></i><b>5</b> Combine, Reshape and Merge</a>
<ul>
<li class="chapter" data-level="5.1" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#combine-rows"><i class="fa fa-check"></i><b>5.1</b> Combine rows</a></li>
<li class="chapter" data-level="5.2" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#combine-columns"><i class="fa fa-check"></i><b>5.2</b> Combine columns</a></li>
<li class="chapter" data-level="5.3" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#reshaping-data"><i class="fa fa-check"></i><b>5.3</b> Reshaping data</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#wide-to-long"><i class="fa fa-check"></i><b>5.3.1</b> Wide to long</a></li>
<li class="chapter" data-level="5.3.2" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#long-to-wide"><i class="fa fa-check"></i><b>5.3.2</b> Long to wide</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#mergejoin"><i class="fa fa-check"></i><b>5.4</b> Merge/Join</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#left-join"><i class="fa fa-check"></i><b>5.4.1</b> Left Join</a></li>
<li class="chapter" data-level="5.4.2" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#right-join"><i class="fa fa-check"></i><b>5.4.2</b> Right Join</a></li>
<li class="chapter" data-level="5.4.3" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#inner-join"><i class="fa fa-check"></i><b>5.4.3</b> Inner Join</a></li>
<li class="chapter" data-level="5.4.4" data-path="combine-reshape-and-merge.html"><a href="combine-reshape-and-merge.html#outer-join"><i class="fa fa-check"></i><b>5.4.4</b> Outer Join</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="aggregation-and-group-operations.html"><a href="aggregation-and-group-operations.html"><i class="fa fa-check"></i><b>6</b> Aggregation and Group Operations</a>
<ul>
<li class="chapter" data-level="6.1" data-path="aggregation-and-group-operations.html"><a href="aggregation-and-group-operations.html#cross-tabulation"><i class="fa fa-check"></i><b>6.1</b> Cross tabulation</a></li>
<li class="chapter" data-level="6.2" data-path="aggregation-and-group-operations.html"><a href="aggregation-and-group-operations.html#group-summaries"><i class="fa fa-check"></i><b>6.2</b> Group summaries</a></li>
<li class="chapter" data-level="6.3" data-path="aggregation-and-group-operations.html"><a href="aggregation-and-group-operations.html#centering-and-scaling"><i class="fa fa-check"></i><b>6.3</b> Centering and Scaling</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html"><i class="fa fa-check"></i><b>7</b> Basic Plotting and Visualization</a>
<ul>
<li class="chapter" data-level="7.1" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#histograms"><i class="fa fa-check"></i><b>7.1</b> Histograms</a></li>
<li class="chapter" data-level="7.2" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#barplots"><i class="fa fa-check"></i><b>7.2</b> Barplots</a></li>
<li class="chapter" data-level="7.3" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#scatterplot"><i class="fa fa-check"></i><b>7.3</b> Scatterplot</a></li>
<li class="chapter" data-level="7.4" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#stripcharts"><i class="fa fa-check"></i><b>7.4</b> Stripcharts</a></li>
<li class="chapter" data-level="7.5" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#boxplots"><i class="fa fa-check"></i><b>7.5</b> Boxplots</a></li>
<li class="chapter" data-level="7.6" data-path="basic-plotting-and-visualization.html"><a href="basic-plotting-and-visualization.html#facet-plots"><i class="fa fa-check"></i><b>7.6</b> Facet plots</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="selected-topics-in-statistical-inference.html"><a href="selected-topics-in-statistical-inference.html"><i class="fa fa-check"></i><b>8</b> Selected Topics in Statistical Inference</a>
<ul>
<li class="chapter" data-level="8.1" data-path="selected-topics-in-statistical-inference.html"><a href="selected-topics-in-statistical-inference.html#comparing-group-means"><i class="fa fa-check"></i><b>8.1</b> Comparing group means</a></li>
<li class="chapter" data-level="8.2" data-path="selected-topics-in-statistical-inference.html"><a href="selected-topics-in-statistical-inference.html#comparing-group-proportions"><i class="fa fa-check"></i><b>8.2</b> Comparing group proportions</a></li>
<li class="chapter" data-level="8.3" data-path="selected-topics-in-statistical-inference.html"><a href="selected-topics-in-statistical-inference.html#linear-modeling"><i class="fa fa-check"></i><b>8.3</b> Linear modeling</a></li>
<li class="chapter" data-level="8.4" data-path="selected-topics-in-statistical-inference.html"><a href="selected-topics-in-statistical-inference.html#logistic-regression"><i class="fa fa-check"></i><b>8.4</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Python and R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="selected-topics-in-statistical-inference" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Selected Topics in Statistical Inference<a href="selected-topics-in-statistical-inference.html#selected-topics-in-statistical-inference" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>This chapter looks at performing selected statistical analyses. It is not comprehensive. The focus is on implementation using Python and R. Good statistical practice is more than knowing which function to use. At a minimum we recommend reading the article, <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004961">Ten Simple Rules for Effective Statistical Practice</a> <span class="citation">(<a href="#ref-kass_caffo_davidian_meng_yu_reid_2016" role="doc-biblioref">Kass et al. 2016</a>)</span>.</p>
<div id="comparing-group-means" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Comparing group means<a href="selected-topics-in-statistical-inference.html#comparing-group-means" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Many research studies compare mean values of some quantity of interest between two or more groups. A t test analyzes two group means. An Analysis of Variance, or ANOVA, analyzes three or more group means. Both the t test and ANOVA are special cases of a linear model.</p>
<p>To demonstrate the t test, we examine fictitious data on 15 scores between two groups of subjects. The “control” group was tested as-is while the “treated” group experienced a particular intervention. Of interest is (1) whether or not the mean scores differ meaningfully between the treated and control groups, and (2) if they do differ, how are they different?</p>
<p>To demonstrate the ANOVA test, we use data from <em>The Analysis of Biological Data (3rd ed)</em><span class="citation">(<a href="#ref-whitlock_schluter_2020" role="doc-biblioref">Whitlock and Schluter 2020</a>)</span> on the mass of pine cones (in grams) from three different environments in North America. Of interest is (1) whether or not the mean mass of pine cones differ meaningfully between the three locations, and (2) if they do differ, how are they different?</p>
<p>We usually assess the first question in each scenario with a hypothesis test and p-value. The null hypothesis is no difference between the means. The p-value is the probability of the observed differences between the groups (or more extreme differences) assuming the null hypothesis is true. A small p-value, traditionally less then 0.05, provides evidence against the null. For example, a p-value of 0.01 says there’s a 1% chance of sampling data as different as this (or more different) if there really was no difference between the groups. Note that p-values don’t tell you how two or more statistics differ. See <a href="https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#_i28">the ASA Statement on p-values</a>.</p>
<p>We assess the second question in each scenario by calculating confidence intervals on the difference in means. This is more informative than a p-value. A confidence interval gives us information on the uncertainty, direction and magnitude of a difference in means. For example, a 95% confidence interval of [2, 15] tells us the data is consistent with a difference anywhere between 2 and 15 and that the mean of one group appears to be at least 2 units larger than the mean of the other group. Note that a 95% confidence interval does not mean there is a 95% probability that the true value is in the interval. The confidence interval either captured the true value or it did not. We don’t know. However the <em>process</em> of calculating the confidence interval works roughly 95% of the time.</p>
<div id="python-47" class="section level4 unnumbered hasAnchor">
<h4>Python<a href="selected-topics-in-statistical-inference.html#python-47" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>t-test</strong></p>
<p>Our data is available as a <strong>Pandas</strong> dataframe. It’s small enough to view in its entirety.</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb665-1"><a href="selected-topics-in-statistical-inference.html#cb665-1" aria-hidden="true" tabindex="-1"></a>ch8_d1</span></code></pre></div>
<pre><code>##     score    group
## 0    77.0  control
## 1    81.0  control
## 2    77.0  control
## 3    86.0  control
## 4    81.0  control
## 5    77.0  control
## 6    82.0  control
## 7    83.0  control
## 8    82.0  control
## 9    79.0  control
## 10   86.0  control
## 11   82.0  control
## 12   78.0  control
## 13   71.0  control
## 14   84.0  control
## 15   85.0  treated
## 16   85.0  treated
## 17   89.0  treated
## 18   88.0  treated
## 19   87.0  treated
## 20   89.0  treated
## 21   88.0  treated
## 22   85.0  treated
## 23   77.0  treated
## 24   87.0  treated
## 25   85.0  treated
## 26   84.0  treated
## 27   79.0  treated
## 28   83.0  treated
## 29   87.0  treated</code></pre>
<p>A stripchart is one of many ways to visualize numeric data between two groups. Here we use the seaborn function <code>stripplot()</code>. It appears the treated group had higher scores.</p>
<div class="sourceCode" id="cb667"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb667-1"><a href="selected-topics-in-statistical-inference.html#cb667-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb667-2"><a href="selected-topics-in-statistical-inference.html#cb667-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb667-3"><a href="selected-topics-in-statistical-inference.html#cb667-3" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb667-4"><a href="selected-topics-in-statistical-inference.html#cb667-4" aria-hidden="true" tabindex="-1"></a>sns.stripplot(x<span class="op">=</span><span class="st">&quot;score&quot;</span>, y<span class="op">=</span><span class="st">&quot;group&quot;</span>, data<span class="op">=</span>ch8_d1)</span>
<span id="cb667-5"><a href="selected-topics-in-statistical-inference.html#cb667-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-322-1.png" width="1440" /></p>
<p>One way to perform a t test in Python is via the <code>CompareMeans()</code> function and its associated methods available in the <strong>statsmodels</strong> package. Below we import <strong>statsmodels.stats.api</strong> as “sms”.</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb668-1"><a href="selected-topics-in-statistical-inference.html#cb668-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.api <span class="im">as</span> sms</span></code></pre></div>
<p>We first extract the data we want to compare as pandas Series.</p>
<div class="sourceCode" id="cb669"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb669-1"><a href="selected-topics-in-statistical-inference.html#cb669-1" aria-hidden="true" tabindex="-1"></a>d_control <span class="op">=</span> ch8_d1.query(<span class="st">&#39;group == &quot;control&quot;&#39;</span>)[<span class="st">&#39;score&#39;</span>]</span>
<span id="cb669-2"><a href="selected-topics-in-statistical-inference.html#cb669-2" aria-hidden="true" tabindex="-1"></a>d_treated <span class="op">=</span> ch8_d1.query(<span class="st">&#39;group == &quot;treated&quot;&#39;</span>)[<span class="st">&#39;score&#39;</span>]</span></code></pre></div>
<p>Next we create Descriptive statistics objects using the <code>DescrStatsW()</code> function.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb670-1"><a href="selected-topics-in-statistical-inference.html#cb670-1" aria-hidden="true" tabindex="-1"></a>control <span class="op">=</span> sms.DescrStatsW(d_control)</span>
<span id="cb670-2"><a href="selected-topics-in-statistical-inference.html#cb670-2" aria-hidden="true" tabindex="-1"></a>treated <span class="op">=</span> sms.DescrStatsW(d_treated)</span></code></pre></div>
<p>Descriptive statistics objects have attributes such as <code>mean</code> and <code>std</code> (standard deviation). Below we print the mean and standard deviation of each group. We also round the standard deviation to three decimal places and place a line break before printing the standard deviation.</p>
<div class="sourceCode" id="cb671"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb671-1"><a href="selected-topics-in-statistical-inference.html#cb671-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;control mean:&quot;</span>, control.mean, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">control std:&quot;</span>, <span class="bu">round</span>(control.std, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## control mean: 80.4 
## control std: 3.844</code></pre>
<div class="sourceCode" id="cb673"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb673-1"><a href="selected-topics-in-statistical-inference.html#cb673-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;treated mean:&quot;</span>, treated.mean, <span class="st">&quot;</span><span class="ch">\n</span><span class="st">treated std:&quot;</span>, <span class="bu">round</span>(treated.std, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>## treated mean: 85.2 
## treated std: 3.331</code></pre>
<p>Next we create a CompareMeans means object using the <code>CompareMeans()</code> function. The required inputs are Descriptive statistics objects. We save the result as “ttest”.</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb675-1"><a href="selected-topics-in-statistical-inference.html#cb675-1" aria-hidden="true" tabindex="-1"></a>ttest <span class="op">=</span> sms.CompareMeans(control, treated)</span></code></pre></div>
<p>Now we can use various methods with the “ttest” object. To see the result of a two sample t test assuming unequal variances, along with a confidence interval on the differences, use the <code>summary</code> method with <code>usevar='unequal'</code>.</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb676-1"><a href="selected-topics-in-statistical-inference.html#cb676-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ttest.summary(usevar<span class="op">=</span><span class="st">&#39;unequal&#39;</span>))</span></code></pre></div>
<pre><code>##                           Test for equality of means                          
## ==============================================================================
##                  coef    std err          t      P&gt;|t|      [0.025      0.975]
## ------------------------------------------------------------------------------
## subset #1     -4.8000      1.359     -3.531      0.001      -7.587      -2.013
## ==============================================================================</code></pre>
<p>The p-value of 0.001 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group.</p>
<p><strong>ANOVA</strong></p>
<p>Our data is available as a <strong>Pandas</strong> dataframe. It’s small enough to view in its entirety.</p>
<div class="sourceCode" id="cb678"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb678-1"><a href="selected-topics-in-statistical-inference.html#cb678-1" aria-hidden="true" tabindex="-1"></a>ch8_d2</span></code></pre></div>
<pre><code>##     mass location
## 0    9.6        1
## 1    9.4        1
## 2    8.9        1
## 3    8.8        1
## 4    8.5        1
## 5    8.2        1
## 6    6.8        2
## 7    6.6        2
## 8    6.0        2
## 9    5.7        2
## 10   5.3        2
## 11   6.7        3
## 12   6.4        3
## 13   6.2        3
## 14   5.7        3
## 15   5.6        3</code></pre>
<p>Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass.</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb680-1"><a href="selected-topics-in-statistical-inference.html#cb680-1" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb680-2"><a href="selected-topics-in-statistical-inference.html#cb680-2" aria-hidden="true" tabindex="-1"></a>sns.stripplot(x<span class="op">=</span><span class="st">&quot;mass&quot;</span>, y<span class="op">=</span><span class="st">&quot;location&quot;</span>, data<span class="op">=</span>ch8_d2)</span>
<span id="cb680-3"><a href="selected-topics-in-statistical-inference.html#cb680-3" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-330-3.png" width="1440" /></p>
<p>We can calculate means using the <code>groupby</code> and <code>mean</code> methods.</p>
<div class="sourceCode" id="cb681"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb681-1"><a href="selected-topics-in-statistical-inference.html#cb681-1" aria-hidden="true" tabindex="-1"></a>ch8_d2[<span class="st">&#39;mass&#39;</span>].groupby(ch8_d2[<span class="st">&#39;location&#39;</span>]).mean()</span></code></pre></div>
<pre><code>## location
## 1    8.90
## 2    6.08
## 3    6.12
## Name: mass, dtype: float64</code></pre>
<p>One way to perform an ANOVA test in Python is via the <code>anova_oneway()</code> function, also available in the <strong>statsmodels</strong> package.</p>
<p>The <code>anova_oneway()</code> function can perform an ANOVA on a pandas Dataframe with the first argument specifying the numeric data and the second argument the grouping variable. We also set <code>use_var='equal'</code> to replicate the R output below.</p>
<div class="sourceCode" id="cb683"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb683-1"><a href="selected-topics-in-statistical-inference.html#cb683-1" aria-hidden="true" tabindex="-1"></a>sms.anova_oneway(ch8_d2.mass, ch8_d2.location, use_var<span class="op">=</span><span class="st">&#39;equal&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;class &#39;statsmodels.stats.base.HolderTuple&#39;&gt;
## statistic = 50.085429769392036
## pvalue = 7.786760128813737e-07
## df = (2.0, 13.0)
## df_num = 2.0
## df_denom = 13.0
## nobs_t = 16.0
## n_groups = 3
## means = array([8.9 , 6.08, 6.12])
## nobs = array([6., 5., 5.])
## vars_ = array([0.28 , 0.387, 0.217])
## use_var = &#39;equal&#39;
## welch_correction = True
## tuple = (50.085429769392036, 7.786760128813737e-07)</code></pre>
<p>The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population.</p>
<p>A common follow-up to an ANOVA is Tukey’s Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. To carry this out in the <strong>statsmodels</strong> package, we need to first create a MultiComparison object using the <code>multicomp.MultiComparison()</code> function. Then we use the <code>tukeyhsd()</code> method to compare the means with corrected p-values.</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb685-1"><a href="selected-topics-in-statistical-inference.html#cb685-1" aria-hidden="true" tabindex="-1"></a>mc <span class="op">=</span> sms.multicomp.MultiComparison(ch8_d2.mass, ch8_d2.location)</span>
<span id="cb685-2"><a href="selected-topics-in-statistical-inference.html#cb685-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mc.tukeyhsd())</span></code></pre></div>
<pre><code>## Multiple Comparison of Means - Tukey HSD, FWER=0.05 
## ====================================================
## group1 group2 meandiff p-adj   lower   upper  reject
## ----------------------------------------------------
##      1      2    -2.82    0.0 -3.6863 -1.9537   True
##      1      3    -2.78    0.0 -3.6463 -1.9137   True
##      2      3     0.04 0.9925 -0.8648  0.9448  False
## ----------------------------------------------------</code></pre>
<p>The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but we’re not sure if the difference is positive or negative.</p>
</div>
<div id="r-47" class="section level4 unnumbered hasAnchor">
<h4>R<a href="selected-topics-in-statistical-inference.html#r-47" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><strong>t-test</strong></p>
<p>The <code>str()</code> function allows to take a quick look at the data frame <code>ch8_d1</code>. One column contains the scores, the other column indicates which group the subject was in (control vs treated).</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="selected-topics-in-statistical-inference.html#cb687-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(ch8_d1)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    30 obs. of  2 variables:
##  $ score: num  77 81 77 86 81 77 82 83 82 79 ...
##  $ group: chr  &quot;control&quot; &quot;control&quot; &quot;control&quot; &quot;control&quot; ...</code></pre>
<p>A stripchart is one of many ways to visualize numeric data between two groups. Here we use the base R function <code>stripchart()</code>. The formula <code>score ~ group</code> says to plot score by group. The <code>las = 1</code> argument says to rotate the y-axis labels. The <code>method = "jitter"</code> arguments says to randomly scatter the points vertically so they don’t overplot. It appears the treated group had higher scores.</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="selected-topics-in-statistical-inference.html#cb689-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stripchart</span>(score <span class="sc">~</span> group, <span class="at">data =</span> ch8_d1, <span class="at">las =</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">&quot;jitter&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-335-1.png" width="672" /></p>
<p>To calculate the means between the two groups we can use the <code>aggregate()</code> function. Again the formula <code>score ~ group</code> says to aggregate score by group. We specify <code>mean</code> so that we calculate the mean between the two groups. Some other functions we could specify include <code>median</code>, <code>sd</code>, or <code>sum</code>. The sample mean of the treated group is about 5 points higher than the control group.</p>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="selected-topics-in-statistical-inference.html#cb690-1" aria-hidden="true" tabindex="-1"></a><span class="fu">aggregate</span>(score <span class="sc">~</span> group, <span class="at">data =</span> ch8_d1, mean)</span></code></pre></div>
<pre><code>##     group score
## 1 control  80.4
## 2 treated  85.2</code></pre>
<p>Is this difference meaningful? What if we took more samples? Would each sample result in similar differences in the means? A t test attempts to answer this.</p>
<p>The <code>t.test()</code> function accommodates formula notation allowing us to specify that we want to calculate mean score by group.</p>
<div class="sourceCode" id="cb692"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb692-1"><a href="selected-topics-in-statistical-inference.html#cb692-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(score <span class="sc">~</span> group, <span class="at">data =</span> ch8_d1)</span></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  score by group
## t = -3.5313, df = 27.445, p-value = 0.001482
## alternative hypothesis: true difference in means between group control and group treated is not equal to 0
## 95 percent confidence interval:
##  -7.586883 -2.013117
## sample estimates:
## mean in group control mean in group treated 
##                  80.4                  85.2</code></pre>
<p>The p-value of 0.0015 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group.</p>
<p><strong>ANOVA</strong></p>
<p>The <code>str()</code> function allows to take a quick look at the data frame <code>ch8_d2</code>. One column contains the mass of the pine cones, the other column indicates which location the pine cone was found.</p>
<div class="sourceCode" id="cb694"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb694-1"><a href="selected-topics-in-statistical-inference.html#cb694-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(ch8_d2)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    16 obs. of  2 variables:
##  $ mass    : num  9.6 9.4 8.9 8.8 8.5 8.2 6.8 6.6 6 5.7 ...
##  $ location: chr  &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ...</code></pre>
<p>Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass.</p>
<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb696-1"><a href="selected-topics-in-statistical-inference.html#cb696-1" aria-hidden="true" tabindex="-1"></a><span class="fu">stripchart</span>(mass <span class="sc">~</span> location, <span class="at">data =</span> ch8_d2, <span class="at">las =</span> <span class="dv">1</span>, <span class="at">method =</span> <span class="st">&quot;jitter&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-339-1.png" width="672" /></p>
<p>To calculate the means between the three groups we can use the <code>aggregate()</code> function. Again the formula <code>mass ~ location</code> says to aggregate mass by location. We specify <code>mean</code> so that we calculate the mean between the three groups.</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="selected-topics-in-statistical-inference.html#cb697-1" aria-hidden="true" tabindex="-1"></a><span class="fu">aggregate</span>(mass <span class="sc">~</span> location, <span class="at">data =</span> ch8_d2, mean)</span></code></pre></div>
<pre><code>##   location mass
## 1        1 8.90
## 2        2 6.08
## 3        3 6.12</code></pre>
<p>Is this difference meaningful? ANOVA attempts to answer this.</p>
<p>The <code>aov()</code> function carries out the ANOVA test and also accommodates formula notation. It’s usually preferable to save the ANOVA result into an object and call <code>summary()</code> on the object.</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="selected-topics-in-statistical-inference.html#cb699-1" aria-hidden="true" tabindex="-1"></a>aov1 <span class="ot">&lt;-</span> <span class="fu">aov</span>(mass <span class="sc">~</span> location, <span class="at">data =</span> ch8_d2)</span>
<span id="cb699-2"><a href="selected-topics-in-statistical-inference.html#cb699-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(aov1)</span></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## location     2 29.404  14.702   50.09 7.79e-07 ***
## Residuals   13  3.816   0.294                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population.</p>
<p>Unlike the <code>t.test()</code> output, the <code>aov()</code> summary does not provide confidence intervals on differences in means. That’s because there are many kinds of differences we might want to assess. A common and easy procedure is Tukey’s Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. Base R provides the <code>TukeyHSD()</code> function for this task. Call it on the ANOVA object.</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="selected-topics-in-statistical-inference.html#cb701-1" aria-hidden="true" tabindex="-1"></a><span class="fu">TukeyHSD</span>(aov1)</span></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = mass ~ location, data = ch8_d2)
## 
## $location
##      diff        lwr        upr     p adj
## 2-1 -2.82 -3.6862516 -1.9537484 0.0000028
## 3-1 -2.78 -3.6462516 -1.9137484 0.0000033
## 3-2  0.04 -0.8647703  0.9447703 0.9925198</code></pre>
<p>The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but we’re not sure if the difference is positive or negative.</p>
</div>
</div>
<div id="comparing-group-proportions" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Comparing group proportions<a href="selected-topics-in-statistical-inference.html#comparing-group-proportions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is often of interest to compare proportions between two groups. Sometimes this is referred to as a two-sample proportion test. To demonstrate we use an exercise from the text <em>Introductory Statistics with R</em> <span class="citation">(<a href="#ref-Dalgaard_2020" role="doc-biblioref">Dalgaard 2008</a>)</span> (p.154). We are told that 210 out of 747 patients died of Rocky Mountain spotted fever in the western United States. That’s a proportion of 0.281. In the eastern United States, 122 out 661 patients died. That’s a proportion of 0.185. Is the difference in proportions statistically significant? In other words, assuming there is no difference in the fatality rate between the two regions, is this difference in proportions surprising?</p>
<div id="python-48" class="section level4 unnumbered hasAnchor">
<h4>Python<a href="selected-topics-in-statistical-inference.html#python-48" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A two-sample proportion test can be carried out in Python using the <code>test_proportions_2indep()</code> function from the <strong>statsmodels</strong> package. The two proportions being compared must be independent.</p>
<p>The first argument is the number of successes or occurrences for the first proportion. The second argument is the number of total trials for the first group. The third and fourth arguments are the occurrences and total number of trials for the second group, respectively.</p>
<div class="sourceCode" id="cb703"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb703-1"><a href="selected-topics-in-statistical-inference.html#cb703-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.stats.api <span class="im">as</span> sms</span>
<span id="cb703-2"><a href="selected-topics-in-statistical-inference.html#cb703-2" aria-hidden="true" tabindex="-1"></a>ptest <span class="op">=</span> sms.test_proportions_2indep(<span class="dv">210</span>, <span class="dv">747</span>, <span class="dv">122</span>, <span class="dv">661</span>)</span></code></pre></div>
<p>We can extract the p-value of the test and the difference in proportions using the <code>pvalue</code> and <code>diff</code> attributes, respectively.</p>
<div class="sourceCode" id="cb704"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb704-1"><a href="selected-topics-in-statistical-inference.html#cb704-1" aria-hidden="true" tabindex="-1"></a>ptest.pvalue</span></code></pre></div>
<pre><code>## 1.632346798072468e-05</code></pre>
<div class="sourceCode" id="cb706"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb706-1"><a href="selected-topics-in-statistical-inference.html#cb706-1" aria-hidden="true" tabindex="-1"></a><span class="co"># rounded to 4 decimal places</span></span>
<span id="cb706-2"><a href="selected-topics-in-statistical-inference.html#cb706-2" aria-hidden="true" tabindex="-1"></a><span class="bu">round</span>(ptest.diff, <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## 0.0966</code></pre>
<p>To calculate a 95% confidence interval for the difference in proportions we need to use the <code>confint_proportions_2indep()</code> function.</p>
<div class="sourceCode" id="cb708"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb708-1"><a href="selected-topics-in-statistical-inference.html#cb708-1" aria-hidden="true" tabindex="-1"></a>pdiff <span class="op">=</span> sms.confint_proportions_2indep(<span class="dv">210</span>, <span class="dv">747</span>, <span class="dv">122</span>, <span class="dv">661</span>)</span>
<span id="cb708-2"><a href="selected-topics-in-statistical-inference.html#cb708-2" aria-hidden="true" tabindex="-1"></a>pdiff</span></code></pre></div>
<pre><code>## (0.05241555145475882, 0.13988087590630482)</code></pre>
<p>The result is returned as a tuple with an extreme amount of precision. We recommend rounding these values to few decimal places. Here’s one way using f strings. Notice we extract each element of the “pdiff” tuple and round to 5 decimal places.</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb710-1"><a href="selected-topics-in-statistical-inference.html#cb710-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;(</span><span class="sc">{</span><span class="bu">round</span>(pdiff[<span class="dv">0</span>],<span class="dv">5</span>)<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span><span class="bu">round</span>(pdiff[<span class="dv">1</span>],<span class="dv">5</span>)<span class="sc">}</span><span class="ss">)&quot;</span>)</span></code></pre></div>
<pre><code>## (0.05242, 0.13988)</code></pre>
<p>This results are slightly different from the R example below. That’s because the <code>test_proportions_2indep()</code> and <code>confint_proportions_2indep()</code> functions use different methods. See their respective help pages to learn more about the methods available and other function arguments.</p>
<p><a href="https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.test_proportions_2indep.html#statsmodels.stats.proportion.test_proportions_2indep">test_proportions_2indep</a> help page<br />
<a href="https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.confint_proportions_2indep.html#statsmodels.stats.proportion.confint_proportions_2indep">confint_proportions_2indep</a> help page</p>
</div>
<div id="r-48" class="section level4 unnumbered hasAnchor">
<h4>R<a href="selected-topics-in-statistical-inference.html#r-48" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A two-sample proportion test in R can be carried out with the <code>prop.test()</code> function. The first argument, <code>x</code>, is the number of “successes” or “occurrences” of some event for each group. The second argument, <code>n</code>, is the number of total trials for each group.</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="selected-topics-in-statistical-inference.html#cb712-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(<span class="at">x =</span> <span class="fu">c</span>(<span class="dv">210</span>, <span class="dv">122</span>), <span class="at">n =</span> <span class="fu">c</span>(<span class="dv">747</span>, <span class="dv">661</span>))</span></code></pre></div>
<pre><code>## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(210, 122) out of c(747, 661)
## X-squared = 17.612, df = 1, p-value = 2.709e-05
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  0.05138139 0.14172994
## sample estimates:
##    prop 1    prop 2 
## 0.2811245 0.1845688</code></pre>
<p>The proportion of patients who died in the western US is about 0.28. The proportion who died in the eastern US is about 0.18. The small p-value says there is a very small chance of seeing a difference as large as this (or larger) if there really was no difference in the proportions. The confidence interval on the difference of proportions ranges from 0.05 to 0.14, indicating that this fever seems to kill at least 5% more patients in the western US.</p>
<p>Sometimes data is presented in a 2-way table with successes and failures. We can present the preceding data in a table as follows using the <code>matrix()</code> function.</p>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb714-1"><a href="selected-topics-in-statistical-inference.html#cb714-1" aria-hidden="true" tabindex="-1"></a>fever <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">210</span>, <span class="dv">122</span>,</span>
<span id="cb714-2"><a href="selected-topics-in-statistical-inference.html#cb714-2" aria-hidden="true" tabindex="-1"></a>                  <span class="dv">747-210</span>, <span class="dv">661-122</span>), <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb714-3"><a href="selected-topics-in-statistical-inference.html#cb714-3" aria-hidden="true" tabindex="-1"></a><span class="fu">rownames</span>(fever) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;western US&quot;</span>, <span class="st">&quot;eastern US&quot;</span>)</span>
<span id="cb714-4"><a href="selected-topics-in-statistical-inference.html#cb714-4" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(fever) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;died&quot;</span>, <span class="st">&quot;lived&quot;</span>)</span>
<span id="cb714-5"><a href="selected-topics-in-statistical-inference.html#cb714-5" aria-hidden="true" tabindex="-1"></a>fever</span></code></pre></div>
<pre><code>##            died lived
## western US  210   537
## eastern US  122   539</code></pre>
<p>When the table is constructed in this fashion with “successes” in the first column and “failures” in the second column, we can feed the table directly to the <code>prop.test()</code> function. (Obviously “success” here means “experienced the event of interest”.)</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="selected-topics-in-statistical-inference.html#cb716-1" aria-hidden="true" tabindex="-1"></a><span class="fu">prop.test</span>(fever)</span></code></pre></div>
<pre><code>## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  fever
## X-squared = 17.612, df = 1, p-value = 2.709e-05
## alternative hypothesis: two.sided
## 95 percent confidence interval:
##  0.05138139 0.14172994
## sample estimates:
##    prop 1    prop 2 
## 0.2811245 0.1845688</code></pre>
<p>The chi-squared test statistic is reported as <code>X-squared = 17.612</code>. This is the same statistic reported if we ran a chi-squared test of association using the <code>chisq.test()</code> function.</p>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="selected-topics-in-statistical-inference.html#cb718-1" aria-hidden="true" tabindex="-1"></a><span class="fu">chisq.test</span>(fever)</span></code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test with Yates&#39; continuity correction
## 
## data:  fever
## X-squared = 17.612, df = 1, p-value = 2.709e-05</code></pre>
<p>This tests the null hypothesis of no association between location in the US and fatality of the fever. The result is identical to <code>prop.test()</code> output, however there is no indication of the nature of association.</p>
</div>
</div>
<div id="linear-modeling" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> Linear modeling<a href="selected-topics-in-statistical-inference.html#linear-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Linear modeling attempts to assess if or how the variability a numeric variable depends on one or more predictor variables. This is often referred to as <em>regression modeling</em> or <em>multiple regression</em>. While it is relatively easy to “fit a model” and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise.</p>
<p>We walk through a somewhat simple example using data on weekly gas consumption. The data is courtesy of the R package <strong>MASS</strong> <span class="citation">(<a href="#ref-MASS" role="doc-biblioref">Venables and Ripley 2002a</a>)</span>. The documentation describes the data as follows:</p>
<p>“Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption.”</p>
<p>The <code>whiteside</code> data frame has 56 rows and 3 columns:</p>
<ul>
<li><code>Insul</code>: A factor, before or after insulation.</li>
<li><code>Temp</code>: average outside temperature in degrees Celsius.</li>
<li><code>Gas</code>: weekly gas consumption in 1000s of cubic feet.</li>
</ul>
<p>Below we demonstrate modeling <code>Gas</code> as a function of <code>Insul</code>, <code>Temp</code>, and their interaction.</p>
<p>Obviously this is not a comprehensive treatment of linear modeling.</p>
<div id="python-49" class="section level4 unnumbered hasAnchor">
<h4>Python<a href="selected-topics-in-statistical-inference.html#python-49" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Python, the <code>OLS()</code> function in the statsmodels package fits a linear model.</p>
<p>In this example, we fit a linear model to predict gas prices from insulation, temperature, and the interaction between insulation and temperature.</p>
<p>The basic construction is to first list your dependent or response variable, then a tilde (<code>~</code>), and then your predictor variables, or terms, separated by plus operators (<code>+</code>). Listing two variables separated by a colon (<code>:</code>) indicates we wish to fit an interaction for those variables. The variables in the formula correspond to columns in a pandas DataFrame. Users specify the pandas DataFrame using the <code>data</code> argument.</p>
<div class="sourceCode" id="cb720"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb720-1"><a href="selected-topics-in-statistical-inference.html#cb720-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb720-2"><a href="selected-topics-in-statistical-inference.html#cb720-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb720-3"><a href="selected-topics-in-statistical-inference.html#cb720-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb720-4"><a href="selected-topics-in-statistical-inference.html#cb720-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> smf.ols(<span class="st">&#39;Gas ~ Insul + Temp + Insul:Temp&#39;</span>, data<span class="op">=</span>whiteside)</span>
<span id="cb720-5"><a href="selected-topics-in-statistical-inference.html#cb720-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> model.fit() <span class="co"># fit the linear model</span></span></code></pre></div>
<p>Once you fit your model, you can extract information about it using several functions. The most commonly used include:</p>
<ul>
<li><code>summary()</code>: summary of model coefficients with standard errors and test statistics</li>
<li><code>params</code>: model coefficients</li>
<li><code>conf_int()</code>: 95% confidence interval of model coefficients</li>
<li>statsmodels provides functions for diagnostic plots. A few examples are shown below.</li>
</ul>
<p>The <code>summary()</code> function produces the standard regression summary one typically finds in a statistics textbook.</p>
<div class="sourceCode" id="cb721"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb721-1"><a href="selected-topics-in-statistical-inference.html#cb721-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(results.summary())</span></code></pre></div>
<pre><code>##                             OLS Regression Results                            
## ==============================================================================
## Dep. Variable:                    Gas   R-squared:                       0.928
## Model:                            OLS   Adj. R-squared:                  0.924
## Method:                 Least Squares   F-statistic:                     222.3
## Date:                Thu, 12 May 2022   Prob (F-statistic):           1.23e-29
## Time:                        10:26:25   Log-Likelihood:                -14.100
## No. Observations:                  56   AIC:                             36.20
## Df Residuals:                      52   BIC:                             44.30
## Df Model:                           3                                         
## Covariance Type:            nonrobust                                         
## =======================================================================================
##                           coef    std err          t      P&gt;|t|      [0.025      0.975]
## ---------------------------------------------------------------------------------------
## Intercept               6.8538      0.136     50.409      0.000       6.581       7.127
## Insul[T.After]         -2.1300      0.180    -11.827      0.000      -2.491      -1.769
## Temp                   -0.3932      0.022    -17.487      0.000      -0.438      -0.348
## Insul[T.After]:Temp     0.1153      0.032      3.591      0.001       0.051       0.180
## ==============================================================================
## Omnibus:                        6.016   Durbin-Watson:                   1.854
## Prob(Omnibus):                  0.049   Jarque-Bera (JB):                4.998
## Skew:                          -0.626   Prob(JB):                       0.0822
## Kurtosis:                       3.757   Cond. No.                         31.6
## ==============================================================================
## 
## Notes:
## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</code></pre>
<p>The following code produces a fitted vs residual plot for the model.</p>
<div class="sourceCode" id="cb723"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb723-1"><a href="selected-topics-in-statistical-inference.html#cb723-1" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb723-2"><a href="selected-topics-in-statistical-inference.html#cb723-2" aria-hidden="true" tabindex="-1"></a>smoothed_line <span class="op">=</span> sm.nonparametric.lowess(results.resid, results.fittedvalues)</span>
<span id="cb723-3"><a href="selected-topics-in-statistical-inference.html#cb723-3" aria-hidden="true" tabindex="-1"></a>plt.plot(results.fittedvalues, results.resid, <span class="st">&quot;.&quot;</span>)</span>
<span id="cb723-4"><a href="selected-topics-in-statistical-inference.html#cb723-4" aria-hidden="true" tabindex="-1"></a>plt.plot(smoothed_line[:,<span class="dv">0</span>], smoothed_line[:,<span class="dv">1</span>],color <span class="op">=</span> <span class="st">&#39;r&#39;</span>)</span>
<span id="cb723-5"><a href="selected-topics-in-statistical-inference.html#cb723-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;fitted values&quot;</span>)</span>
<span id="cb723-6"><a href="selected-topics-in-statistical-inference.html#cb723-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;residuals&quot;</span>)</span>
<span id="cb723-7"><a href="selected-topics-in-statistical-inference.html#cb723-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;residuals vs fitted&quot;</span>)</span>
<span id="cb723-8"><a href="selected-topics-in-statistical-inference.html#cb723-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-356-1.png" width="672" /></p>
<p>The following code plots model predictions. Each line of the plot represents a level of the variable “Insul”.</p>
<div class="sourceCode" id="cb724"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb724-1"><a href="selected-topics-in-statistical-inference.html#cb724-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb724-2"><a href="selected-topics-in-statistical-inference.html#cb724-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb724-3"><a href="selected-topics-in-statistical-inference.html#cb724-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> statsmodels.sandbox.predict_functional <span class="im">import</span> predict_functional</span>
<span id="cb724-4"><a href="selected-topics-in-statistical-inference.html#cb724-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb724-5"><a href="selected-topics-in-statistical-inference.html#cb724-5" aria-hidden="true" tabindex="-1"></a><span class="co"># create DataFrame (wrt Inusl == &quot;Before&quot;) to pass into predict function</span></span>
<span id="cb724-6"><a href="selected-topics-in-statistical-inference.html#cb724-6" aria-hidden="true" tabindex="-1"></a>temp <span class="op">=</span> np.linspace(whiteside[<span class="st">&quot;Temp&quot;</span>].<span class="bu">min</span>(), whiteside[<span class="st">&quot;Temp&quot;</span>].<span class="bu">max</span>())</span>
<span id="cb724-7"><a href="selected-topics-in-statistical-inference.html#cb724-7" aria-hidden="true" tabindex="-1"></a>insul_before <span class="op">=</span> [<span class="st">&quot;Before&quot;</span>]<span class="op">*</span>temp.shape[<span class="dv">0</span>]</span>
<span id="cb724-8"><a href="selected-topics-in-statistical-inference.html#cb724-8" aria-hidden="true" tabindex="-1"></a><span class="co"># whiteside_before = pd.DataFrame({&quot;Temp&quot;: temp, &quot;Insul&quot;: insul_before})</span></span>
<span id="cb724-9"><a href="selected-topics-in-statistical-inference.html#cb724-9" aria-hidden="true" tabindex="-1"></a>whiteside_before <span class="op">=</span> {<span class="st">&quot;Temp&quot;</span>: temp, <span class="st">&quot;Insul&quot;</span>: insul_before, <span class="st">&quot;Insul:Temp&quot;</span>:[<span class="dv">0</span>]<span class="op">*</span>temp.shape[<span class="dv">0</span>]}</span>
<span id="cb724-10"><a href="selected-topics-in-statistical-inference.html#cb724-10" aria-hidden="true" tabindex="-1"></a><span class="co"># pr, cb, fv = predict_functional(results, &quot;Temp&quot;, values=whiteside_before, ci_method=&#39;scheffe&#39;)</span></span>
<span id="cb724-11"><a href="selected-topics-in-statistical-inference.html#cb724-11" aria-hidden="true" tabindex="-1"></a>before_predict_object <span class="op">=</span> results.get_prediction(whiteside_before)</span>
<span id="cb724-12"><a href="selected-topics-in-statistical-inference.html#cb724-12" aria-hidden="true" tabindex="-1"></a>before_predictions <span class="op">=</span> before_predict_object.predicted_mean</span>
<span id="cb724-13"><a href="selected-topics-in-statistical-inference.html#cb724-13" aria-hidden="true" tabindex="-1"></a>before_ci <span class="op">=</span> before_predict_object.conf_int()</span>
<span id="cb724-14"><a href="selected-topics-in-statistical-inference.html#cb724-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb724-15"><a href="selected-topics-in-statistical-inference.html#cb724-15" aria-hidden="true" tabindex="-1"></a><span class="co"># create DataFrame (wrt Inusl == &quot;After&quot;) to pass into predict function</span></span>
<span id="cb724-16"><a href="selected-topics-in-statistical-inference.html#cb724-16" aria-hidden="true" tabindex="-1"></a>insul_after <span class="op">=</span> [<span class="st">&quot;After&quot;</span>]<span class="op">*</span>temp.shape[<span class="dv">0</span>]</span>
<span id="cb724-17"><a href="selected-topics-in-statistical-inference.html#cb724-17" aria-hidden="true" tabindex="-1"></a>whiteside_after <span class="op">=</span> pd.DataFrame({<span class="st">&quot;Temp&quot;</span>: temp, <span class="st">&quot;Insul&quot;</span>: insul_after, <span class="st">&quot;Insul:Temp&quot;</span>:temp})</span>
<span id="cb724-18"><a href="selected-topics-in-statistical-inference.html#cb724-18" aria-hidden="true" tabindex="-1"></a>after_predictions_object <span class="op">=</span> results.get_prediction(whiteside_after)</span>
<span id="cb724-19"><a href="selected-topics-in-statistical-inference.html#cb724-19" aria-hidden="true" tabindex="-1"></a>after_predictions <span class="op">=</span> after_predictions_object.predicted_mean</span>
<span id="cb724-20"><a href="selected-topics-in-statistical-inference.html#cb724-20" aria-hidden="true" tabindex="-1"></a>after_ci <span class="op">=</span> after_predictions_object.conf_int()</span>
<span id="cb724-21"><a href="selected-topics-in-statistical-inference.html#cb724-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb724-22"><a href="selected-topics-in-statistical-inference.html#cb724-22" aria-hidden="true" tabindex="-1"></a><span class="co"># plot results</span></span>
<span id="cb724-23"><a href="selected-topics-in-statistical-inference.html#cb724-23" aria-hidden="true" tabindex="-1"></a>plt.figure()</span>
<span id="cb724-24"><a href="selected-topics-in-statistical-inference.html#cb724-24" aria-hidden="true" tabindex="-1"></a>plt.plot(temp, before_predictions, color <span class="op">=</span> <span class="st">&quot;red&quot;</span>, label<span class="op">=</span><span class="st">&quot;Before&quot;</span>)</span>
<span id="cb724-25"><a href="selected-topics-in-statistical-inference.html#cb724-25" aria-hidden="true" tabindex="-1"></a>plt.fill_between(temp, before_ci[:,<span class="dv">0</span>], before_ci[:,<span class="dv">1</span>], color <span class="op">=</span> <span class="st">&quot;red&quot;</span>, alpha <span class="op">=</span> <span class="fl">0.1</span>)</span>
<span id="cb724-26"><a href="selected-topics-in-statistical-inference.html#cb724-26" aria-hidden="true" tabindex="-1"></a>plt.plot(temp, after_predictions, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;After&quot;</span>)</span>
<span id="cb724-27"><a href="selected-topics-in-statistical-inference.html#cb724-27" aria-hidden="true" tabindex="-1"></a>plt.fill_between(temp, after_ci[:,<span class="dv">0</span>], after_ci[:,<span class="dv">1</span>], color <span class="op">=</span> <span class="st">&quot;blue&quot;</span>, alpha <span class="op">=</span> <span class="fl">0.1</span>)</span>
<span id="cb724-28"><a href="selected-topics-in-statistical-inference.html#cb724-28" aria-hidden="true" tabindex="-1"></a>plt.legend(title<span class="op">=</span><span class="st">&quot;Insul&quot;</span>)</span>
<span id="cb724-29"><a href="selected-topics-in-statistical-inference.html#cb724-29" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Predicted values of Gas&quot;</span>)</span>
<span id="cb724-30"><a href="selected-topics-in-statistical-inference.html#cb724-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Temp&quot;</span>)</span>
<span id="cb724-31"><a href="selected-topics-in-statistical-inference.html#cb724-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Gas&quot;</span>)</span>
<span id="cb724-32"><a href="selected-topics-in-statistical-inference.html#cb724-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-357-3.png" width="672" /></p>
<p>We see that after installing insulation, gas consumption fell considerably, and that the effect of temperature on gas consumption is less pronounced.</p>
</div>
<div id="r-49" class="section level4 unnumbered hasAnchor">
<h4>R<a href="selected-topics-in-statistical-inference.html#r-49" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <code>lm()</code> function fits a linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (<code>~</code>), and then your predictor variables, or terms, separated by plus operators (<code>+</code>). Listing two variables separated by a colon (<code>:</code>) indicates we wish to fit an interaction for those variables. See <code>?formula</code> for further details on formula syntax.</p>
<p>It’s considered best practice to reference variables in a data frame and indicate the data frame using the <code>data</code> argument. Though not required, you’ll almost always want to save the result to an object for further inquiry.</p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="selected-topics-in-statistical-inference.html#cb725-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(Gas <span class="sc">~</span> Insul <span class="sc">+</span> Temp <span class="sc">+</span> Insul<span class="sc">:</span>Temp, <span class="at">data =</span> whiteside)</span></code></pre></div>
<p>Once you fit your model, you can extract information about it using several functions. The most commonly used include:</p>
<ul>
<li><code>summary()</code>: summary of model coefficients with standard errors and test statistics</li>
<li><code>coef()</code>: model coefficients</li>
<li><code>confint()</code>: 95% confidence interval of model coefficients</li>
<li><code>plot()</code>: a set of four diagnostic plots</li>
</ul>
<p>The <code>summary()</code> function produces the standard regression summary one typically finds described in a statistics textbook.</p>
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="selected-topics-in-statistical-inference.html#cb726-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Gas ~ Insul + Temp + Insul:Temp, data = whiteside)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.97802 -0.18011  0.03757  0.20930  0.63803 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      6.85383    0.13596  50.409  &lt; 2e-16 ***
## InsulAfter      -2.12998    0.18009 -11.827 2.32e-16 ***
## Temp            -0.39324    0.02249 -17.487  &lt; 2e-16 ***
## InsulAfter:Temp  0.11530    0.03211   3.591 0.000731 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.323 on 52 degrees of freedom
## Multiple R-squared:  0.9277, Adjusted R-squared:  0.9235 
## F-statistic: 222.3 on 3 and 52 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Calling <code>plot()</code> on a model object produces four different diagnostic plots by default. Using the <code>which</code> argument we can specify which of six possible plots to create. The first one checks the constant variance assumption (ie, that our model is not dramatically over- or under-predicting values.) We hope to see residuals evenly scattered around 0. (See <code>?plot.lm</code> for more details on the diagnostic plots.)</p>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="selected-topics-in-statistical-inference.html#cb728-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">which =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-360-1.png" width="672" /></p>
<p>Once we fit a model and we’re reasonably confident that it’s a good model, we may want to visualize it. Three packages in R that help with this are <strong>emmeans</strong>, <strong>effects</strong>, and <strong>ggeffects</strong>. We briefly demonstrate the <strong>ggeffects</strong> package.</p>
<p>You need to first install the <strong>ggeffects</strong> package as it does not come with the base R installation. Once installed, load using the <code>library()</code> function.</p>
<p>Once loaded, we can get a basic visualization of our model by using the <code>plot()</code> and <code>ggpredict()</code> functions. This is particularly useful for models with interactions. Use the <code>terms</code> argument to specify which variables to plot. Below we list “Temp” first, which will plot “Temp” on the x axis. Then we list “Insul”, the grouping variable, to indicate we want a separate fit for each level of “Insul”.</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="selected-topics-in-statistical-inference.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.pacakges(&quot;ggeffects&quot;)</span></span>
<span id="cb729-2"><a href="selected-topics-in-statistical-inference.html#cb729-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggeffects)</span>
<span id="cb729-3"><a href="selected-topics-in-statistical-inference.html#cb729-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ggpredict</span>(m, <span class="at">terms =</span> <span class="fu">c</span>(<span class="st">&quot;Temp&quot;</span>, <span class="st">&quot;Insul&quot;</span>)))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-361-1.png" width="672" /></p>
<p>We see that after installing insulation, gas consumption fell considerably, and that the effect of temperature on gas consumption is less pronounced.</p>
</div>
</div>
<div id="logistic-regression" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Logistic regression<a href="selected-topics-in-statistical-inference.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Logistic regression attempts to assess if or how the variability a binary variable depends on one or more predictor variables. It is a type of Generalized Linear Model and is commonly used to model the <em>probability</em> of an event occurring. While it is relatively easy to “fit a model” and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise.</p>
<p>We walk through a basic example using data on low infant birth weight. The data is courtesy of the R package <strong>MASS</strong> <span class="citation">(<a href="#ref-MASS" role="doc-biblioref">Venables and Ripley 2002a</a>)</span>. According to the documentation, “the data were collected at Baystate Medical Center, Springfield, Mass during 1986.”</p>
<p>We use the data as prepared in the example code found at <code>?birthwt</code>.</p>
<p>The <code>birthwt</code> data frame has 189 rows and 9 columns:</p>
<ul>
<li><code>low</code>: 1 if birth weight less than 2.5 kg, 0 otherwise</li>
<li><code>age</code>: mother’s age in years</li>
<li><code>lwt</code>: mother’s weight in pounds at last menstrual period</li>
<li><code>race</code>: mother’s race (white, black, other)</li>
<li><code>smoke</code>: smoking status during pregnancy (1 = yes, 0 = no)</li>
<li><code>ptd</code>: previous premature labors (1 = yes, 0 = no)</li>
<li><code>ht</code>: history of hypertension (1 = yes, 0 = no)</li>
<li><code>ui</code>: presence of uterine irritability (1 = yes, 0 = no)</li>
<li><code>ftv</code>: number of physician visits during the first trimester (0, 1, 2+)</li>
</ul>
<p>Below we demonstrate modeling <code>low</code> as a function of all other predictors.</p>
<p>Obviously this is not a comprehensive treatment of logistic regression.</p>
<div id="python-50" class="section level4 unnumbered hasAnchor">
<h4>Python<a href="selected-topics-in-statistical-inference.html#python-50" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Python statistics library <code>statsmodels.formula.api</code> provides the <code>glm</code> function for fitting generalized linear models. We specify how we want to model our data as a formula string in the first argument of the <code>glm</code> function. The format is as follows: dependent/response variable followed by a tilde (<code>~</code>), then the predictor variables separated by a plus sign (<code>+</code>). To indicate we want to fit a logistic regression model we specify <code>family=sm.families.Binomial()</code> since our dependent variable is binary. We also specify our data set in the argument as well.</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb730-1"><a href="selected-topics-in-statistical-inference.html#cb730-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb730-2"><a href="selected-topics-in-statistical-inference.html#cb730-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.formula.api <span class="im">as</span> smf</span>
<span id="cb730-3"><a href="selected-topics-in-statistical-inference.html#cb730-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb730-4"><a href="selected-topics-in-statistical-inference.html#cb730-4" aria-hidden="true" tabindex="-1"></a>formula <span class="op">=</span> <span class="st">&quot;low ~ age + lwt + race + smoke + ptd + ht + ui + ftv&quot;</span></span>
<span id="cb730-5"><a href="selected-topics-in-statistical-inference.html#cb730-5" aria-hidden="true" tabindex="-1"></a>mod1 <span class="op">=</span> smf.glm(formula<span class="op">=</span>formula, data<span class="op">=</span>birthwt,</span>
<span id="cb730-6"><a href="selected-topics-in-statistical-inference.html#cb730-6" aria-hidden="true" tabindex="-1"></a>               family<span class="op">=</span>sm.families.Binomial()).fit()</span>
<span id="cb730-7"><a href="selected-topics-in-statistical-inference.html#cb730-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mod1.summary())</span></code></pre></div>
<pre><code>##                  Generalized Linear Model Regression Results                  
## ==============================================================================
## Dep. Variable:                    low   No. Observations:                  189
## Model:                            GLM   Df Residuals:                      178
## Model Family:                Binomial   Df Model:                           10
## Link Function:                  Logit   Scale:                          1.0000
## Method:                          IRLS   Log-Likelihood:                -97.738
## Date:                Thu, 12 May 2022   Deviance:                       195.48
## Time:                        10:26:27   Pearson chi2:                     179.
## No. Iterations:                     5   Pseudo R-squ. (CS):             0.1873
## Covariance Type:            nonrobust                                         
## =================================================================================
##                     coef    std err          z      P&gt;|z|      [0.025      0.975]
## ---------------------------------------------------------------------------------
## Intercept         0.8230      1.245      0.661      0.508      -1.617       3.263
## race[T.black]     1.1924      0.536      2.225      0.026       0.142       2.243
## race[T.other]     0.7407      0.462      1.604      0.109      -0.164       1.646
## smoke[T.1]        0.7555      0.425      1.778      0.075      -0.078       1.589
## ptd[T.1]          1.3438      0.481      2.796      0.005       0.402       2.286
## ht[T.1]           1.9132      0.721      2.654      0.008       0.501       3.326
## ui[T.1]           0.6802      0.464      1.465      0.143      -0.230       1.590
## ftv[T.1]         -0.4364      0.479     -0.910      0.363      -1.376       0.503
## ftv[T.2+]         0.1790      0.456      0.392      0.695      -0.715       1.074
## age              -0.0372      0.039     -0.962      0.336      -0.113       0.039
## lwt              -0.0157      0.007     -2.211      0.027      -0.030      -0.002
## =================================================================================</code></pre>
<p>Once we fit a model and we’re reasonably confident that it’s a good model, we may want to visualize it. For example, how does <code>ptd</code> affect the probability of low infant birth weight?</p>
<p>To begin, we create a Pandas DataFrame for our predictors. This needs to be a Pandas DataFrame since we used the formula option when fitting the model. Notice the only variable that changes is <code>ptd</code>. All other variables are held constant. What values you choose to hold them at is up to you, but typical choices are means or medians for numeric predictors, and most populous group for categorical predictors.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb732-1"><a href="selected-topics-in-statistical-inference.html#cb732-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create dictionary of values</span></span>
<span id="cb732-2"><a href="selected-topics-in-statistical-inference.html#cb732-2" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> {<span class="st">&#39;age&#39;</span>: <span class="dv">23</span>, <span class="st">&#39;lwt&#39;</span>: <span class="dv">123</span>, <span class="st">&#39;race&#39;</span>: <span class="st">&quot;white&quot;</span>,</span>
<span id="cb732-3"><a href="selected-topics-in-statistical-inference.html#cb732-3" aria-hidden="true" tabindex="-1"></a>     <span class="st">&#39;smoke&#39;</span> : <span class="st">&quot;0&quot;</span>, <span class="st">&#39;ptd&#39;</span> : [<span class="st">&quot;0&quot;</span>,<span class="st">&quot;1&quot;</span>], <span class="st">&#39;ht&#39;</span>: <span class="st">&quot;0&quot;</span>, </span>
<span id="cb732-4"><a href="selected-topics-in-statistical-inference.html#cb732-4" aria-hidden="true" tabindex="-1"></a>     <span class="st">&#39;ui&#39;</span>: <span class="st">&quot;0&quot;</span>, <span class="st">&#39;ftv&#39;</span>: <span class="st">&quot;0&quot;</span>}</span>
<span id="cb732-5"><a href="selected-topics-in-statistical-inference.html#cb732-5" aria-hidden="true" tabindex="-1"></a><span class="co"># convert dictionary to Pandas DataFrame.</span></span>
<span id="cb732-6"><a href="selected-topics-in-statistical-inference.html#cb732-6" aria-hidden="true" tabindex="-1"></a>nd <span class="op">=</span> pd.DataFrame(data<span class="op">=</span>d)</span></code></pre></div>
<p>Now plug in our new data (nd) into our model using the <code>get_prediction</code> method. See <a href="https://www.statsmodels.org/devel/generated/statsmodels.genmod.generalized_linear_model.GLMResults.html#statsmodels.genmod.generalized_linear_model.GLMResults">this page</a>. This returns the predictions as a <code>PredictionResults</code> class. We can access the predicted probabilities using the <code>predicted_mean</code> property. See <a href="https://www.statsmodels.org/devel/generated/statsmodels.tsa.base.prediction.PredictionResults.html">this page</a>.</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb733-1"><a href="selected-topics-in-statistical-inference.html#cb733-1" aria-hidden="true" tabindex="-1"></a>pred <span class="op">=</span> mod1.get_prediction(exog<span class="op">=</span>nd)</span>
<span id="cb733-2"><a href="selected-topics-in-statistical-inference.html#cb733-2" aria-hidden="true" tabindex="-1"></a>prob <span class="op">=</span> pred.predicted_mean</span>
<span id="cb733-3"><a href="selected-topics-in-statistical-inference.html#cb733-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prob)</span></code></pre></div>
<pre><code>## [0.12360891 0.35093623]</code></pre>
<p>We can use the <code>conf_int</code> method to extract the confidence intervals for the predicted probabilities.</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb735-1"><a href="selected-topics-in-statistical-inference.html#cb735-1" aria-hidden="true" tabindex="-1"></a>ci <span class="op">=</span> pred.conf_int()</span>
<span id="cb735-2"><a href="selected-topics-in-statistical-inference.html#cb735-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(ci)</span></code></pre></div>
<pre><code>## [[0.05166761 0.26746827]
##  [0.12724595 0.66722908]]</code></pre>
<p>Now we use the matplotlib <code>errorbar</code> function to create the plot. The <code>errorbar</code> function requires <em>margin of error</em>, not the lower and upper limits. So we have to do some subtraction to get the lower and upper margin of errors.</p>
<div class="sourceCode" id="cb737"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb737-1"><a href="selected-topics-in-statistical-inference.html#cb737-1" aria-hidden="true" tabindex="-1"></a>lower <span class="op">=</span> [prob[<span class="dv">0</span>] <span class="op">-</span> ci[<span class="dv">0</span>,<span class="dv">0</span>], prob[<span class="dv">1</span>] <span class="op">-</span> ci[<span class="dv">1</span>,<span class="dv">0</span>]]</span>
<span id="cb737-2"><a href="selected-topics-in-statistical-inference.html#cb737-2" aria-hidden="true" tabindex="-1"></a>upper <span class="op">=</span> [ci[<span class="dv">0</span>,<span class="dv">1</span>] <span class="op">-</span> prob[<span class="dv">0</span>], ci[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">-</span> prob[<span class="dv">1</span>]]</span></code></pre></div>
<p>Finally we can make the plot.</p>
<div class="sourceCode" id="cb738"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb738-1"><a href="selected-topics-in-statistical-inference.html#cb738-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb738-2"><a href="selected-topics-in-statistical-inference.html#cb738-2" aria-hidden="true" tabindex="-1"></a>plt.clf()</span>
<span id="cb738-3"><a href="selected-topics-in-statistical-inference.html#cb738-3" aria-hidden="true" tabindex="-1"></a>plt.errorbar(x <span class="op">=</span> [<span class="st">&quot;0&quot;</span>,<span class="st">&quot;1&quot;</span>], y <span class="op">=</span> prob, </span>
<span id="cb738-4"><a href="selected-topics-in-statistical-inference.html#cb738-4" aria-hidden="true" tabindex="-1"></a>             yerr<span class="op">=</span>[lower, upper], fmt<span class="op">=</span><span class="st">&#39;ok&#39;</span>)</span></code></pre></div>
<pre><code>## &lt;ErrorbarContainer object of 3 artists&gt;</code></pre>
<div class="sourceCode" id="cb740"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb740-1"><a href="selected-topics-in-statistical-inference.html#cb740-1" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&#39;ptd&#39;</span>)</span>
<span id="cb740-2"><a href="selected-topics-in-statistical-inference.html#cb740-2" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&#39;low&#39;</span>)</span>
<span id="cb740-3"><a href="selected-topics-in-statistical-inference.html#cb740-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&#39;Predicted probabilities of low&#39;</span>)</span>
<span id="cb740-4"><a href="selected-topics-in-statistical-inference.html#cb740-4" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-369-1.png" width="672" /></p>
</div>
<div id="r-50" class="section level4 unnumbered hasAnchor">
<h4>R<a href="selected-topics-in-statistical-inference.html#r-50" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <code>glm()</code> function fits a generalized linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (<code>~</code>), and then your predictor variables, or terms, separated by plus operators (<code>+</code>). Listing two variables separated by a colon (<code>:</code>) indicates we wish to fit an interaction for those variables. See <code>?formula</code> for further details on formula syntax.</p>
<p>In addition, <code>glm()</code> requires we specify a family argument to specify the error distribution for the dependent variable. The default is <code>gaussian</code>. For a logistic regression model, we need to specify <code>binomial</code> since our dependent variable is binary.</p>
<p>It’s considered best practice to reference variables in a data frame and indicate the data frame using the <code>data</code> argument. Though not required, you’ll almost always want to save the result to an object for further inquiry.</p>
<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb741-1"><a href="selected-topics-in-statistical-inference.html#cb741-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">glm</span>(low <span class="sc">~</span> age <span class="sc">+</span> lwt <span class="sc">+</span> race <span class="sc">+</span> </span>
<span id="cb741-2"><a href="selected-topics-in-statistical-inference.html#cb741-2" aria-hidden="true" tabindex="-1"></a>             smoke <span class="sc">+</span> ptd <span class="sc">+</span> ht <span class="sc">+</span> </span>
<span id="cb741-3"><a href="selected-topics-in-statistical-inference.html#cb741-3" aria-hidden="true" tabindex="-1"></a>             ui <span class="sc">+</span> ftv, </span>
<span id="cb741-4"><a href="selected-topics-in-statistical-inference.html#cb741-4" aria-hidden="true" tabindex="-1"></a>           <span class="at">data =</span> birthwt, <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Since we’re modeling <code>low</code> as a function of all other variables in the data frame, we could have used the following syntax, where the period symbolizes all other remaining variables:</p>
<div class="sourceCode" id="cb742"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb742-1"><a href="selected-topics-in-statistical-inference.html#cb742-1" aria-hidden="true" tabindex="-1"></a>mod <span class="ot">&lt;-</span> <span class="fu">glm</span>(low <span class="sc">~</span> ., <span class="at">data =</span> birthwt, <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Once you fit your logistic regression model, you can extract information about it using several functions. The most commonly used include:</p>
<ul>
<li><code>summary()</code>: summary of model coefficients with standard errors and test statistics</li>
<li><code>coef()</code>: model coefficients</li>
<li><code>confint()</code>: 95% confidence interval of model coefficients</li>
</ul>
<p>The <code>summary()</code> function produces the standard regression summary one typically finds described in a statistics textbook.</p>
<div class="sourceCode" id="cb743"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb743-1"><a href="selected-topics-in-statistical-inference.html#cb743-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = low ~ age + lwt + race + smoke + ptd + ht + ui + 
##     ftv, family = binomial, data = birthwt)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.7038  -0.8068  -0.5008   0.8835   2.2152  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.82302    1.24471   0.661  0.50848   
## age         -0.03723    0.03870  -0.962  0.33602   
## lwt         -0.01565    0.00708  -2.211  0.02705 * 
## raceblack    1.19241    0.53597   2.225  0.02609 * 
## raceother    0.74069    0.46174   1.604  0.10869   
## smoke1       0.75553    0.42502   1.778  0.07546 . 
## ptd1         1.34376    0.48062   2.796  0.00518 **
## ht1          1.91317    0.72074   2.654  0.00794 **
## ui1          0.68019    0.46434   1.465  0.14296   
## ftv1        -0.43638    0.47939  -0.910  0.36268   
## ftv2+        0.17901    0.45638   0.392  0.69488   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 234.67  on 188  degrees of freedom
## Residual deviance: 195.48  on 178  degrees of freedom
## AIC: 217.48
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Exponentiating coefficients in a logistic regression model produces odds ratios. To get the odds ratio for the previous premature labors variable, <code>ptd</code>, we do the following:</p>
<div class="sourceCode" id="cb745"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb745-1"><a href="selected-topics-in-statistical-inference.html#cb745-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(mod)[<span class="st">&quot;ptd1&quot;</span>])</span></code></pre></div>
<pre><code>##     ptd1 
## 3.833443</code></pre>
<p>This says the odds of having an infant with low birth weight are about 3.8 times higher for women who experienced previous premature labors versus women who did not, assuming all other variables equal.</p>
<p>The 3.8 value is just an estimate. We can use the <code>confint()</code> function to get a 95% confidence interval on the odds ratio.</p>
<div class="sourceCode" id="cb747"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb747-1"><a href="selected-topics-in-statistical-inference.html#cb747-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(mod)[<span class="st">&quot;ptd1&quot;</span>,])</span></code></pre></div>
<pre><code>##     2.5 %    97.5 % 
##  1.516837 10.128974</code></pre>
<p>It appears the odds ratio is at least 1.5, possibly as high as 10.1 (assuming we believe this model).</p>
<p>Once we fit a model and we’re reasonably confident that it’s a good model, we may want to visualize it. Three packages in R that help with this are <strong>emmeans</strong>, <strong>effects</strong>, and <strong>ggeffects</strong>. We briefly demonstrate the <strong>ggeffects</strong> package.</p>
<p>You need to first install the <strong>ggeffects</strong> package as it does not come with the base R installation. Once installed, load using the <code>library()</code> function.</p>
<p>Once loaded, we can get a basic visualization of our model by using the <code>plot()</code> and <code>ggpredict()</code> functions. This is particularly useful for logistic regression models because it produces model predictions on a probability scale.</p>
<p>Use the <code>terms</code> argument to specify which variables to plot. Below we plot the probability of low infant birth weight versus whether or not the mother had previous premature labors (1 = yes, 0 = no).</p>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="selected-topics-in-statistical-inference.html#cb749-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;ggeffects&quot;)</span></span>
<span id="cb749-2"><a href="selected-topics-in-statistical-inference.html#cb749-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggeffects)</span>
<span id="cb749-3"><a href="selected-topics-in-statistical-inference.html#cb749-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">ggpredict</span>(mod, <span class="at">terms =</span> <span class="st">&quot;ptd&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-375-1.png" width="672" /></p>
<p>It looks like the probability of low infant birth weight jumps from about 12% to over 35% for mothers who previously experienced premature labors, though the error bars on the expected values are quite large.</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Dalgaard_2020" class="csl-entry">
Dalgaard, Peter. 2008. <em>Introductory Statistics with <span>R</span></em>. 2nd ed. Springer.
</div>
<div id="ref-kass_caffo_davidian_meng_yu_reid_2016" class="csl-entry">
Kass, Robert E., Brian S. Caffo, Marie Davidian, Xiao-Li Meng, Bin Yu, and Nancy Reid. 2016. <span>“Ten Simple Rules for Effective Statistical Practice.”</span> <em>PLOS Computational Biology</em> 12 (6). <a href="https://doi.org/10.1371/journal.pcbi.1004961">https://doi.org/10.1371/journal.pcbi.1004961</a>.
</div>
<div id="ref-MASS" class="csl-entry">
Venables, W. N., and B. D. Ripley. 2002a. <em>Modern Applied Statistics with s</em>. Fourth. New York: Springer. <a href="https://www.stats.ox.ac.uk/pub/MASS4/">https://www.stats.ox.ac.uk/pub/MASS4/</a>.
</div>
<div id="ref-whitlock_schluter_2020" class="csl-entry">
Whitlock, Michael, and Dolph Schluter. 2020. <em>The Analysis of Biological Data</em>. 3rd ed. Macmillan Learning.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="basic-plotting-and-visualization.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/uvastatlab/Python_and_R/edit/main/08-statistical_inference-and-modeling.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

</body>

</html>
