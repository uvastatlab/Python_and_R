# Selected Topics in Statistical Inference

```{r,echo=FALSE}
knitr::opts_chunk$set(comment = '##', prompt = FALSE, collapse = FALSE)
```

This chapter looks at performing selected statistical analyses. It is not comprehensive. The focus is on implementation using Python and R. Good statistical practice is more than knowing which function to use. At a minimum we recommend reading the article, [Ten Simple Rules for Effective Statistical Practice](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004961) [@kass_caffo_davidian_meng_yu_reid_2016].

## Comparing group means

Many research studies compare mean values of some quantity of interest between two or more groups. A t test analyzes two group means. An Analysis of Variance, or ANOVA, analyzes three or more group means. Both the t test and ANOVA are special cases of a linear model.

To demonstrate the t test, we examine fictitious data on 15 scores between two groups of subjects. The "control" group was tested as-is while the "treated" group experienced a particular intervention. Of interest is (1) whether or not the mean scores differ meaningfully between the treated and control groups, and (2) if they do differ, how are they different?

To demonstrate the ANOVA test, we use data from _The Analysis of Biological Data (3rd ed)_[@whitlock_schluter_2020] on the mass of pine cones (in grams) from three different environments in North America. Of interest is (1) whether or not the mean mass of pine cones differ meaningfully between the three locations, and (2) if they do differ, how are they different?

We usually assess the first question in each scenario with a hypothesis test and p-value. The null hypothesis is no difference between the means. The p-value is the probability of the observed differences between the groups (or more extreme differences) assuming the null hypothesis is true. A small p-value, traditionally less then 0.05, provides evidence against the null. For example, a p-value of 0.01 says there's a 1% chance of sampling data as different as this (or more different) if there really was no difference between the groups. Note that p-values don't tell you how two or more statistics differ. See [the ASA Statement on p-values](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#_i28).

We assess the second question in each scenario by calculating confidence intervals on the difference in means. This is more informative than a p-value. A confidence interval gives us information on the uncertainty, direction and magnitude of a difference in means. For example, a 95% confidence interval of [2, 15] tells us the data is consistent with a difference anywhere between 2 and 15 and that the mean of one group appears to be at least 2 units larger than the mean of the other group. Note that a 95% confidence interval does not mean there is a 95% probability that the true value is in the interval. The confidence interval either captured the true value or it did not. We don't know. However the _process_ of calculating the confidence interval works roughly 95% of the time.


```{r echo=FALSE}
# toy data for t-test
set.seed(1)
s1 <- round(rnorm(n = 15, mean = 80, sd = 4))
s2 <- round(rnorm(n = 15, mean = 85, sd = 4))
ch8_d1 <- data.frame(
  score = c(s1, s2),
  group = rep(c("control", "treated"), each = 15)
)
rm(s1,s2)

# data from The Analysis of Biological Data, 3 ed. 
# Ch 15 assignment problem, p. 498
ch8_d2 <- data.frame(mass = c(9.6, 9.4, 8.9, 8.8, 8.5, 8.2,
                              6.8, 6.6, 6.0, 5.7, 5.3,
                              6.7, 6.4, 6.2, 5.7, 5.6),
                     location = rep(c("1", "2", "3"),
                                    times = c(6, 5, 5)))

```



```{python echo = FALSE}
ch8_d1 = r.ch8_d1
ch8_d2 = r.ch8_d2
```



#### Python {-}

**t-test**

Our data is available as a **Pandas** dataframe. It's small enough to view in its entirety.

```{python}
ch8_d1
```


A stripchart is one of many ways to visualize numeric data between two groups. Here we use the seaborn function `stripplot()`. It appears the treated group had higher scores.

```{python }
import seaborn as sns
import matplotlib.pyplot as plt
plt.clf()
sns.stripplot(x="score", y="group", data=ch8_d1)
plt.show()
```


One way to perform a t test in Python is via the `CompareMeans()` function and its associated methods available in the **statsmodels** package. Below we import **statsmodels.stats.api** as "sms".

```{python}
import statsmodels.stats.api as sms
```

We first extract the data we want to compare as pandas Series.

```{python}
d_control = ch8_d1.query('group == "control"')['score']
d_treated = ch8_d1.query('group == "treated"')['score']
```

Next we create Descriptive statistics objects using the `DescrStatsW()` function.

```{python}
control = sms.DescrStatsW(d_control)
treated = sms.DescrStatsW(d_treated)
```

Descriptive statistics objects have attributes such as `mean` and `std` (standard deviation). Below we print the mean and standard deviation of each group. We also round the standard deviation to three decimal places and place a line break before printing the standard deviation.

```{python}
print("control mean:", control.mean, "\ncontrol std:", round(control.std, 3))
print("treated mean:", treated.mean, "\ntreated std:", round(treated.std, 3))
```

Next we create a CompareMeans means object using the `CompareMeans()` function. The required inputs are Descriptive statistics objects. We save the result as "ttest".

```{python}
ttest = sms.CompareMeans(control, treated)
```

Now we can use various methods with the "ttest" object. To see the result of a two sample t test assuming unequal variances, along with a confidence interval on the differences, use the `summary` method with `usevar='unequal'`. 

```{python}
print(ttest.summary(usevar='unequal'))
```

The p-value of 0.001 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group.


**ANOVA**

Our data is available as a **Pandas** dataframe. It's small enough to view in its entirety.

```{python}
ch8_d2
```

Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass.

```{python}
plt.clf()
sns.stripplot(x="mass", y="location", data=ch8_d2)
plt.show()
```



We can calculate means using the `groupby` and `mean` methods.

```{python}
ch8_d2['mass'].groupby(ch8_d2['location']).mean()
```

One way to perform an ANOVA test in Python is via the `anova_oneway()` function, also available in the **statsmodels** package.

The `anova_oneway()` function can perform an ANOVA on a pandas Dataframe with the first argument specifying the numeric data and the second argument the grouping variable. We also set `use_var='equal'` to replicate the R output below.

```{python}
sms.anova_oneway(ch8_d2.mass, ch8_d2.location, use_var='equal')
```

The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population.

A common follow-up to an ANOVA is Tukey's Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. To carry this out in the **statsmodels** package, we need to first create a MultiComparison object using the `multicomp.MultiComparison()` function. Then we use the `tukeyhsd()` method to compare the means with corrected p-values.

```{python}
mc = sms.multicomp.MultiComparison(ch8_d2.mass, ch8_d2.location)
print(mc.tukeyhsd())
```

The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but we're not sure if the difference is positive or negative.

#### R {-}



**t-test**

The `str()` function allows to take a quick look at the data frame `ch8_d1`. One column contains the scores, the other column indicates which group the subject was in (control vs treated).

```{r}
str(ch8_d1)
```

A stripchart is one of many ways to visualize numeric data between two groups. Here we use the base R function `stripchart()`. The formula `score ~ group` says to plot score by group. The `las = 1` argument says to rotate the y-axis labels. The `method = "jitter"` arguments says to randomly scatter the points vertically so they don't overplot. It appears the treated group had higher scores.

```{r}
stripchart(score ~ group, data = ch8_d1, las = 1, method = "jitter")
```

To calculate the means between the two groups we can use the `aggregate()` function. Again the formula `score ~ group` says to aggregate score by group. We specify `mean` so that we calculate the mean between the two groups. Some other functions we could specify include `median`, `sd`, or `sum`. The sample mean of the treated group is about 5 points higher than the control group.

```{r}
aggregate(score ~ group, data = ch8_d1, mean)
```

Is this difference meaningful? What if we took more samples? Would each sample result in similar differences in the means? A t test attempts to answer this. 

The `t.test()` function accommodates formula notation allowing us to specify that we want to calculate mean score by group.

```{r}
t.test(score ~ group, data = ch8_d1)
```

The p-value of 0.0015 is small, providing good evidence that the difference in means we witnessed reflects a real difference in the population. The confidence interval on the difference in means tells us the data is consistent with a difference between -7 and -2. It appears we can expect the control group to score at least 2 points lower than the treated group. 


**ANOVA**

The `str()` function allows to take a quick look at the data frame `ch8_d2`. One column contains the mass of the pine cones, the other column indicates which location the pine cone was found.


```{r}
str(ch8_d2)
```

Again we use a stripchart to visualize the three groups of data. It appears the pine cones in location 1 have a higher mass.

```{r}
stripchart(mass ~ location, data = ch8_d2, las = 1, method = "jitter")
```

To calculate the means between the three groups we can use the `aggregate()` function. Again the formula `mass ~ location` says to aggregate mass by location. We specify `mean` so that we calculate the mean between the three groups. 

```{r}
aggregate(mass ~ location, data = ch8_d2, mean)
```

Is this difference meaningful? ANOVA attempts to answer this. 

The `aov()` function carries out the ANOVA test and also accommodates formula notation. It's usually preferable to save the ANOVA result into an object and call `summary()` on the object.

```{r}
aov1 <- aov(mass ~ location, data = ch8_d2)
summary(aov1)
```

The small p-value of 0.0000007 provides strong evidence that the difference in means we witnessed reflects a real difference in the population.

Unlike the `t.test()` output, the `aov()` summary does not provide confidence intervals on differences in means. That's because there are many kinds of differences we might want to assess. A common and easy procedure is Tukey's Honestly Significant Differences (HSD), which computes differences between all possible pairs and returns adjusted p-values and confidence intervals to account for the multiple comparisons. Base R provides the `TukeyHSD()` function for this task. Call it on the ANOVA object.

```{r}
TukeyHSD(aov1)
```

The difference in means between locations 2 and 1 (2 - 1) and locations 3 and 1 (3 - 1) are about -2.8. The difference in means between locations 3 and 2 (3 - 2) is inconclusive. It seems to be small but we're not sure if the difference is positive or negative.


## Comparing group proportions

It is often of interest to compare proportions between two groups. Sometimes this is referred to as a two-sample proportion test. To demonstrate we use an exercise from the text _Introductory Statistics with R_ [@Dalgaard_2020] (p.154). We are told that 210 out of 747 patients died of Rocky Mountain spotted fever in the western United States. That's a proportion of `r round(210/747, 3)`. In the eastern United States, 122 out 661 patients died. That's a proportion of `r round(122/661, 3)`. Is the difference in proportions statistically significant? In other words, assuming there is no difference in the fatality rate between the two regions, is this difference in proportions surprising?


#### Python {-}

A two-sample proportion test can be carried out in Python using the `test_proportions_2indep()` function from the **statsmodels** package. The two proportions being compared must be independent.

The first argument is the number of successes or occurrences for the first proportion. The second argument is the number of total trials for the first group. The third and fourth arguments are the occurrences and total number of trials for the second group, respectively.

```{python}
import statsmodels.stats.api as sms
ptest = sms.test_proportions_2indep(210, 747, 122, 661)
```

We can extract the p-value of the test and the difference in proportions using the `pvalue` and `diff` attributes, respectively.

```{python}
ptest.pvalue
```

```{python}
# rounded to 4 decimal places
round(ptest.diff, 4)
```

To calculate a 95% confidence interval for the difference in proportions we need to use the `confint_proportions_2indep()` function.

```{python}
pdiff = sms.confint_proportions_2indep(210, 747, 122, 661)
pdiff
```

The result is returned as a tuple with an extreme amount of precision. We recommend rounding these values to few decimal places. Here's one way using f strings. Notice we extract each element of the "pdiff" tuple and round to 5 decimal places.

```{python}
print(f"({round(pdiff[0],5)}, {round(pdiff[1],5)})")
```

This results are slightly different from the R example below. That's because the `test_proportions_2indep()` and `confint_proportions_2indep()` functions use different methods. See their respective help pages to learn more about the methods available and other function arguments.

[test_proportions_2indep](https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.test_proportions_2indep.html#statsmodels.stats.proportion.test_proportions_2indep) help page   
[confint_proportions_2indep](https://www.statsmodels.org/dev/generated/statsmodels.stats.proportion.confint_proportions_2indep.html#statsmodels.stats.proportion.confint_proportions_2indep) help page   


#### R {-}

A two-sample proportion test in R can be carried out with the `prop.test()` function. The first argument, `x`, is the number of "successes" or "occurrences" of some event for each group. The second argument, `n`, is the number of total trials for each group.

```{r}
prop.test(x = c(210, 122), n = c(747, 661))
```

The proportion of patients who died in the western US is about 0.28. The proportion who died in the eastern US is about 0.18. The small p-value says there is a very small chance of seeing a difference as large as this (or larger) if there really was no difference in the proportions. The confidence interval on the difference of proportions ranges from 0.05 to 0.14, indicating that this fever seems to kill at least 5% more patients in the western US.

Sometimes data is presented in a 2-way table with successes and failures. We can present the preceding data in a table as follows using the `matrix()` function. 

```{r}
fever <- matrix(c(210, 122,
                  747-210, 661-122), ncol = 2)
rownames(fever) <- c("western US", "eastern US")
colnames(fever) <- c("died", "lived")
fever
```

When the table is constructed in this fashion with "successes" in the first column and "failures" in the second column, we can feed the table directly to the `prop.test()` function. (Obviously "success" here means "experienced the event of interest".)

```{r}
prop.test(fever)
```

The chi-squared test statistic is reported as `X-squared = 17.612`. This is the same statistic reported if we ran a chi-squared test of association using the `chisq.test()` function. 

```{r}
chisq.test(fever)
```

This tests the null hypothesis of no association between location in the US and fatality of the fever. The result is identical to `prop.test()` output, however there is no indication of the nature of association.


## Linear modeling

Linear modeling attempts to assess if or how the variability a numeric variable depends on one or more predictor variables. This is often referred to as _regression modeling_ or _multiple regression_. While it is relatively easy to "fit a model" and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise. 

We walk through a somewhat simple example using data on weekly gas consumption. The data is courtesy of the R package **MASS** [@MASS]. The documentation describes the data as follows:

"Mr Derek Whiteside of the UK Building Research Station recorded the weekly gas consumption and average external temperature at his own house in south-east England for two heating seasons, one of 26 weeks before, and one of 30 weeks after cavity-wall insulation was installed. The object of the exercise was to assess the effect of the insulation on gas consumption."

The `whiteside` data frame has 56 rows and 3 columns:

- `Insul`: A factor, before or after insulation.
- `Temp`: average outside temperature in degrees Celsius.
- `Gas`: weekly gas consumption in 1000s of cubic feet.

Below we demonstrate modeling `Gas` as a function of `Insul`, `Temp`, and their interaction.

Obviously this is not a comprehensive treatment of linear modeling. 

```{r echo=FALSE, message=FALSE}
library(MASS)
data("whiteside")
```


```{python echo = FALSE}
whiteside = r.whiteside
```


#### Python {-}

In Python, the `OLS()` function in the statsmodels package fits a linear model. 

First, we select a column for the independent variable y, the variable to be predicted, and then columns for the dependent variable X, the variable(s) used to predict.

```{python}
# load packages
import statsmodels.api as sm

# select dependent variables
X = whiteside[["Insul", "Temp"]]
X
```

Notice that X contains a column of categorical data. Categorical data cannot be an input to a linear regression model unless converted to an indicator, or dummy, variable. An indicator variable is a numerical encoding of a categorical variable. We can use the pandas `get_dummies()` function to convert categorical variables to indicator variables. 

```{python}
import pandas as pd

X = pd.get_dummies(X, columns=['Insul'], drop_first=True)
X
```

To add an intercept to the model, use the statsmodels `add_constant()` function to add a column of 1s to the X data. This column of 1s serves as an input to fit an intercept. Then we pass the y and X variables into the `OLS()` function.

```{python}
# select independent variables
y = whiteside[["Gas"]]

# add column of 1s for intercept
X = sm.add_constant(X)

model = sm.OLS(y, X)
results = model.fit()
```

The `summary()` function produces the standard regression summary one typically finds described in a statistics textbook.

```{python}
print(results.summary())
```

Because there are only 3 axes in the model (2 dependent variables and 1 independent variable) we can visu

```{python}
from matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
img = ax.grid(False)


```

#### R {-}

The `lm()` function fits a linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (`~`), and then your predictor variables, or terms, separated by plus operators (`+`). Listing two variables separated by a colon (`:`) indicates we wish to fit an interaction for those variables. See `?formula` for further details on formula syntax.

It's considered best practice to reference variables in a data frame and indicate the data frame using the `data` argument. Though not required, you'll almost always want to save the result to an object for further inquiry.

```{r}
m <- lm(Gas ~ Insul + Temp + Insul:Temp, data = whiteside)
```

Once you fit your model, you can extract information about it using several functions. The most commonly used include:

- `summary()`: summary of model coefficients with standard errors and test statistics
- `coef()`: model coefficients
- `confint()`: 95% confidence interval of model coefficients
- `plot()`: a set of four diagnostic plots

The `summary()` function produces the standard regression summary one typically finds described in a statistics textbook.

```{r}
summary(m)
```

Calling `plot()` on a model object produces four different diagnostic plots by default. Using the `which` argument we can specify which of six possible plots to create. The first one checks the constant variance assumption (ie, that our model is not dramatically over- or under-predicting values.) We hope to see residuals evenly scattered around 0. (See `?plot.lm` for more details on the diagnostic plots.)

```{r}
plot(m, which = 1)
```

Once we fit a model and we're reasonably confident that it's a good model, we may want to visualize it. Three packages in R that help with this are **emmeans**, **effects**, and **ggeffects**. We briefly demonstrate the **ggeffects** package.

You need to first install the **ggeffects** package as it does not come with the base R installation. Once installed, load using the `library()` function.

Once loaded, we can get a basic visualization of our model by using the `plot()` and `ggpredict()` functions. This is particularly useful for models with interactions. Use the `terms` argument to specify which variables to plot. Below we list "Temp" first, which will plot "Temp" on the x axis. Then we list "Insul", the grouping variable, to indicate we want a separate fit for each level of "Insul".

```{r message=FALSE}
# install.pacakges("ggeffects")
library(ggeffects)
plot(ggpredict(m, terms = c("Temp", "Insul")))
```

We see that after installing insulation, gas consumption fell considerably, and that the effect of temperature on gas consumption is less pronounced.

## Logistic regression

Logistic regression attempts to assess if or how the variability a binary variable depends on one or more predictor variables. It is a type of Generalized Linear Model and is commonly used to model the _probability_ of an event occurring. While it is relatively easy to "fit a model" and generate lots of output, the model we fit may not be very good. There are many decisions we have to make when proposing a model. Which predictors do we include? Will they interact? Do we allow for non-linear effects? Answering these kinds of questions require subject matter expertise.

We walk through a basic example using data on low infant birth weight. The data is courtesy of the R package **MASS** [@MASS]. According to the documentation, "the data were collected at Baystate Medical Center, Springfield, Mass during 1986." 

We use the data as prepared in the example code found at `?birthwt`. 

The `birthwt` data frame has 189 rows and 9 columns:

- `low`: 1 if birth weight less than 2.5 kg, 0 otherwise
- `age`: mother's age in years
- `lwt`: mother's weight in pounds at last menstrual period
- `race`: mother's race (white, black, other)
- `smoke`: smoking status during pregnancy (1 = yes, 0 = no)
- `ptd`: previous premature labors (1 = yes, 0 = no)
- `ht`: history of hypertension (1 = yes, 0 = no)
- `ui`: presence of uterine irritability (1 = yes, 0 = no)
- `ftv`: number of physician visits during the first trimester (0, 1, 2+)

Below we demonstrate modeling `low` as a function of all other predictors.

Obviously this is not a comprehensive treatment of logistic regression. 

```{r echo=FALSE, message=FALSE}
birthwt <- readRDS("data/bwt.Rds")
```

```{python echo = FALSE}
birthwt = r.birthwt
```


#### Python {-}


#### R {-}

The `glm()` function fits a generalized linear model in R using whatever model we propose. We specify models using a special syntax. The basic construction is to first list your dependent or response variable, then a tilde (`~`), and then your predictor variables, or terms, separated by plus operators (`+`). Listing two variables separated by a colon (`:`) indicates we wish to fit an interaction for those variables. See `?formula` for further details on formula syntax.

In addition, `glm()` requires we specify a family argument to specify the error distribution for the dependent variable. The default is `gaussian`. For a logistic regression model, we need to specify `binomial` since our dependent variable is binary. 

It's considered best practice to reference variables in a data frame and indicate the data frame using the `data` argument. Though not required, you'll almost always want to save the result to an object for further inquiry.

```{r}
mod <- glm(low ~ age + lwt + race + 
             smoke + ptd + ht + 
             ui + ftv, 
           data = birthwt, family = binomial)
```

Since we're modeling `low` as a function of all other variables in the data frame, we could have used the following syntax, where the period symbolizes all other remaining variables:

```{r eval=FALSE}
mod <- glm(low ~ ., data = birthwt, family = binomial)
```

Once you fit your logistic regression model, you can extract information about it using several functions. The most commonly used include:

- `summary()`: summary of model coefficients with standard errors and test statistics
- `coef()`: model coefficients
- `confint()`: 95% confidence interval of model coefficients

The `summary()` function produces the standard regression summary one typically finds described in a statistics textbook.

```{r}
summary(mod)
```

Exponentiating coefficients in a logistic regression model produces odds ratios. To get the odds ratio for the previous premature labors variable, `ptd`, we do the following:

```{r}
exp(coef(mod)["ptd1"])
```

This says the odds of having an infant with low birth weight are about 3.8 times higher for women who experienced previous premature labors versus women who did not, assuming all other variables equal.

The 3.8 value is just an estimate. We can use the `confint()` function to get a 95% confidence interval on the odds ratio.

```{r message=FALSE}
exp(confint(mod)["ptd1",])
```

It appears the odds ratio is at least 1.5, possibly as high as 10.1 (assuming we believe this model).

Once we fit a model and we're reasonably confident that it's a good model, we may want to visualize it. Three packages in R that help with this are **emmeans**, **effects**, and **ggeffects**. We briefly demonstrate the **ggeffects** package.

You need to first install the **ggeffects** package as it does not come with the base R installation. Once installed, load using the `library()` function.

Once loaded, we can get a basic visualization of our model by using the `plot()` and `ggpredict()` functions. This is particularly useful for logistic regression models because it produces model predictions on a probability scale. 

Use the `terms` argument to specify which variables to plot. Below we create two plots: one for `ptd` (previous premature labors) and one for `lwt` (mother's weight at last menstrual period).

```{r}
# install.packages("ggeffects")
library(ggeffects)
plot(ggpredict(mod, terms = "ptd"))
```

It looks like the probability of low infant birth weight jumps from about 12% to over 35% for mothers who previously experienced premature labors, though the error bars on the expected values are quite large. 

```{r message=FALSE}
plot(ggpredict(mod, terms = "lwt"))
```

It appears the probability of low infant birth weight drops from about 15% when a mother weighs 100 lbs to about 5% when a mother weighs around 200 lbs. The regions with the larger confidence ribbon indicate regions of higher uncertainty. There are clearly not many mothers in our data who weigh less than 100 lbs.


